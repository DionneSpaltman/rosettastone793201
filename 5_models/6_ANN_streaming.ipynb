{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c02236",
   "metadata": {},
   "source": [
    "# â€˜Streamingâ€™ ANN using embeddings from SentenceTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3defb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, concatenate, GlobalAveragePooling1D\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba409ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "# Load pickled sentence embeddings\n",
    "def load_embeddings(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return {k: np.array(v, dtype=np.float16) for k, v in data.items()}\n",
    "\n",
    "# Define data generator\n",
    "def data_generator(df, embedding_map):\n",
    "    for _, row in df.iterrows():\n",
    "        emb1 = embedding_map.get(row['sentence1_clean'])\n",
    "        emb2 = embedding_map.get(row['sentence2_clean'])\n",
    "        if emb1 is not None and emb2 is not None:\n",
    "            yield (emb1.astype(np.float16), emb2.astype(np.float16)), np.float16(row['score'])\n",
    "\n",
    "# Build ANN model\n",
    "def build_ann_model(input_shape):\n",
    "    input1 = Input(shape=input_shape)\n",
    "    input2 = Input(shape=input_shape)\n",
    "\n",
    "    pooled1 = GlobalAveragePooling1D()(input1)\n",
    "    pooled2 = GlobalAveragePooling1D()(input2)\n",
    "\n",
    "    abs_diff = Lambda(lambda x: tf.abs(x[0] - x[1]))([pooled1, pooled2])\n",
    "    mult = Lambda(lambda x: x[0] * x[1])([pooled1, pooled2])\n",
    "\n",
    "    merged = concatenate([pooled1, pooled2, abs_diff, mult])\n",
    "\n",
    "    dense = Dense(128, activation='relu')(merged)\n",
    "    drop = Dropout(0.3)(dense)\n",
    "    output = Dense(1, activation='linear')(drop)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Load resources\n",
    "embedding_path = \"drive/MyDrive/sentence_to_embedding.pkl\"\n",
    "sentence_to_embedding = load_embeddings(embedding_path)\n",
    "\n",
    "csv_path = \"drive/MyDrive/rs2_augmented.csv\"\n",
    "df_full = pd.read_csv(csv_path)[['sentence1_clean', 'sentence2_clean', 'score']]\n",
    "\n",
    "# Remove unknown embeddings\n",
    "df_full = df_full[\n",
    "    df_full['sentence1_clean'].isin(sentence_to_embedding) &\n",
    "    df_full['sentence2_clean'].isin(sentence_to_embedding)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Train-test split\n",
    "df_train, df_val = train_test_split(df_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Infer input shape\n",
    "sample_embedding = next(iter(sentence_to_embedding.values()))\n",
    "embedding_dim = sample_embedding.shape[1] if sample_embedding.ndim == 2 else sample_embedding.shape[0]\n",
    "max_len = 30\n",
    "input_shape = (max_len, embedding_dim)\n",
    "\n",
    "# Create tf.data datasets\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(df_train, sentence_to_embedding),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=input_shape, dtype=tf.float16),\n",
    "         tf.TensorSpec(shape=input_shape, dtype=tf.float16)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float16)\n",
    "    )\n",
    ").batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(df_val, sentence_to_embedding),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=input_shape, dtype=tf.float16),\n",
    "         tf.TensorSpec(shape=input_shape, dtype=tf.float16)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float16)\n",
    "    )\n",
    ").batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Free memory\n",
    "del df_full\n",
    "gc.collect()\n",
    "\n",
    "# Build and train model\n",
    "model = build_ann_model(input_shape)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Predictions (optional, be careful with RAM here)\n",
    "#y_pred = model.predict(val_dataset).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c80c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr, _ = pearsonr(y_val, y_pred)\n",
    "spearman_corr, _ = spearmanr(y_val, y_pred)\n",
    "\n",
    "print(f\"ðŸ“ˆ Pearson Correlation:  {pearson_corr:.4f}\")\n",
    "print(f\"ðŸ“Š Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "plt.title('MAE over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosettastone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
