{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOmPEiYDrOYH"
      },
      "source": [
        "# Outline\n",
        "- Building a spam detection model with GloVe and NLTK\n",
        "\n",
        "## Text Normalization in NLP: Lemmatization vs Stemming\n",
        "When processing natural language, it's important to reduce words to their base or root form. There are two main techniques for this:\n",
        "\n",
        "- Stemming: Cuts off prefixes/suffixes to find the stem of a word.\n",
        "\n",
        "- Lemmatization: Uses vocabulary and morphological analysis to return the base or dictionary form of a word (lemma).\n",
        "\n",
        "### ‚öñÔ∏è Comparison\n",
        "\n",
        "| Feature              | WordNetLemmatizer | PorterStemmer  | SnowballStemmer |\n",
        "|----------------------|-------------------|----------------|-----------------|\n",
        "| Approach             | Lemmatization     | Stemming       | Stemming        |\n",
        "| Vocabulary-aware     | ‚úÖ Yes            | ‚ùå No          | ‚ùå No           |\n",
        "| POS Support          | ‚úÖ Yes            | ‚ùå No          | ‚ùå No           |\n",
        "| Language Support     | English (via WordNet) | English | Multiple languages |\n",
        "| Output Accuracy      | ‚úÖ High           | ‚ö†Ô∏è Medium      | ‚úÖ Better than Porter |\n",
        "| Speed                | ‚ö†Ô∏è Slower         | ‚úÖ Fast        | ‚úÖ Fast         |\n",
        "\n",
        "## NLTK\n",
        "NLTK, which stands for Natural Language Toolkit, is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources.\n",
        "\n",
        "### Stemmers\n",
        "The key feature we are using today is the stemmer. It allow the reduction of words to their base or root form. For example, \"running\" to \"run\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## **What Are Word Embeddings?**\n",
        "\n",
        "Word embeddings are techniques to represent **words as vectors** in a continuous vector space, where **similar words have similar vectors**.\n",
        "\n",
        "Instead of one-hot encoding like this:\n",
        "\n",
        "| Word     | Vector      |\n",
        "|----------|-------------|\n",
        "| king     | [0, 0, 1, 0] |\n",
        "| queen    | [0, 1, 0, 0] |\n",
        "| man      | [1, 0, 0, 0] |\n",
        "\n",
        "Embeddings would look like this:\n",
        "\n",
        "| Word     | Vector (dim=4 example)     |\n",
        "|----------|----------------------------|\n",
        "| king     | [0.25, 0.89, -0.10, 0.34]  |\n",
        "| queen    | [0.30, 0.85, -0.08, 0.33]  |\n",
        "| man      | [0.45, 0.21, -0.34, 0.70]  |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "StEPbAhMpVzU"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem.porter import PorterStemmer # first iteration of stemmer by Porter\n",
        "from nltk.stem.snowball import SnowballStemmer # second iteration of stemmer by Porter, that deals with more complex word morphologies\n",
        "import spacy # OpenSource Python library for fast performance and easy-to-use interfaces for a wide range of NLP tasks\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import nltk\n",
        "# nltk.download('all') # averaged_perceptron_tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IxAtvbSUpbsD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-25 14:32:13.092780: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-25 14:32:13.107115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742909533.127402    3474 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742909533.133761    3474 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-25 14:32:13.160638: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def download_and_read(url, encoding='latin-1'):\n",
        "    # Download ZIP file (no auto-extract)\n",
        "    zip_path = tf.keras.utils.get_file(\n",
        "        fname=url.split('/')[-1],\n",
        "        origin=url,\n",
        "        extract=False,  # We'll extract manually\n",
        "        cache_dir='.',\n",
        "        cache_subdir='datasets'\n",
        "    )\n",
        "\n",
        "    # Extract manually\n",
        "    extract_dir = os.path.join(os.path.dirname(zip_path), 'smsspamcollection')\n",
        "    if not os.path.exists(extract_dir):\n",
        "        os.makedirs(extract_dir)\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    # Locate the extracted file\n",
        "    local_file = os.path.join(extract_dir, 'SMSSpamCollection')\n",
        "\n",
        "    # Read data\n",
        "    labels, texts = [], []\n",
        "    with open(local_file, \"r\", encoding=encoding) as fin:\n",
        "        for line in fin:\n",
        "            label, text = line.strip().split('\\t')\n",
        "            labels.append(1 if label == \"spam\" else 0)\n",
        "            texts.append(text)\n",
        "    return texts, labels\n",
        "\n",
        "# Dataset URL\n",
        "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "\n",
        "# Load the data\n",
        "texts, labels = download_and_read(DATASET_URL)\n",
        "\n",
        "# Quick check\n",
        "print(texts[0])\n",
        "print(labels[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40BVpxcW32iB"
      },
      "source": [
        "## Advanced words preprocessing methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcSdCLt_3uFJ"
      },
      "source": [
        "### SpaCy and NTLK API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0gAtq0u-pyer"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "\n",
        "import spacy\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# Load the small English model for spaCy\n",
        "# This model includes various NLP tools such as tokenizers, lemmatizers, etc.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Access the default set of stop words provided by spaCy for English\n",
        "# Stop words are common words that are usually filtered out in NLP tasks because they carry less meaningful information\n",
        "stop_words = nlp.Defaults.stop_words\n",
        "\n",
        "# Access the set of punctuation characters from the string module\n",
        "# Like stop words, punctuation is often removed in text preprocessing to focus on meaningful words\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Initialize the WordNet lemmatizer from NLTK\n",
        "# Lemmatization reduces words to their base or root form, but unlike stemming, it ensures the root word belongs to the language\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Initialize the Porter Stemmer from NLTK\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Initialize the Snowball Stemmer for English from NLTK\n",
        "snowBallStemmer = SnowballStemmer(\"english\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD4c2HjfyBTh"
      },
      "source": [
        "### Custom stop words\n",
        "Define a set of words that should not be treated as stop words despite being in the default stop words list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "edR5SZVYp3AE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'herein', 'becoming', 'wherever', 'other', 'have', 'least', 'during', \"'ve\", 'give', 'is', 'whom', 'does', 'name', 'throughout', 'regarding', 'therefore', 'that', 'himself', 'except', 'against', 'so', 'due', 'doing', 'since', 'were', 'hereafter', 'anything', 'here', 'three', 'make', 'where', 'serious', 'sometimes', 'many', 'four', 'how', 'we', 'why', 'always', 'show', 'whoever', 'our', 'what', 'along', 'own', 'can', 'her', 'side', 'everything', 'this', 're', 'seeming', 'my', '‚Äòve', 'all', 'now', 'seem', 'anywhere', 'n‚Äòt', 'much', 'same', 'them', 'may', \"'m\", '‚Äòd', 'unless', 'around', 'sometime', 'more', '‚Äôll', 'even', 'keep', 'get', 'namely', 'nothing', 'besides', 'between', 'it', 'twelve', 'onto', 'over', 'from', 'yourself', 'next', '‚Äòm', 'see', 'using', 'but', 'somehow', 'cannot', 'should', 'yours', 'must', 'neither', 'latterly', 'often', 'hundred', \"'d\", 'their', 'another', 'put', 'first', 'by', 'thereupon', 'on', 'whereas', 'again', 'upon', 'been', 'after', 'hers', 'sixty', 'thus', 'in', 'became', 'perhaps', 'thereby', 'every', 'an', 'n‚Äôt', 'those', 'yourselves', 'and', 'take', 'just', 'whence', 'either', 'anyone', '‚Äôm', 'eight', 'of', 'these', 'when', 'yet', 'at', 'herself', 'formerly', 'has', 'meanwhile', 'or', \"n't\", 'hence', 'others', 'out', 'everywhere', 'behind', 'did', 'various', 'front', 'once', 'several', 'ca', 'as', 'both', 'themselves', 'than', 'could', 'to', 'via', 'below', 'go', 'moreover', 'they', 'almost', 'latter', 'most', 'some', 'part', '‚Äôs', 'too', 'hereby', '‚Äòre', 'thereafter', 'him', 'together', '‚Äòs', 'i', 'two', 'the', 'former', 'its', 'move', 'seems', 'each', 'who', 'per', \"'s\", 'somewhere', 'six', 'being', 'really', 'still', 'something', 'he', '‚Äòll', 'ever', 'someone', 'only', 'am', 'myself', 'mostly', 'whenever', 'fifty', 'there', 'us', 'do', \"'re\", 'are', 'up', 'empty', 'your', 'such', 'five', 'about', 'whether', 'further', 'enough', 'within', 'which', 'top', 'before', 'whose', 'ours', 'done', 'mine', 'if', 'nevertheless', 'would', 'under', 'already', 'among', 'last', 'beside', 'afterwards', '‚Äôve', 'wherein', 'very', 'bottom', 'thence', 'be', 'beforehand', \"'ll\", 'also', 'thru', '‚Äôre', 'fifteen', 'say', 'alone', 'a', 'quite', 'into', 'seemed', 'she', 'beyond', 'whereby', 'everyone', 'whither', 'for', 'nine', 'then', 'although', 'with', 'therein', 'used', 'becomes', 'made', 'full', 'please', 'any', 'call', 'become', '‚Äôd', 'amongst', 'because', 'toward', 'forty', 'back', 'whereupon', 'had', 'while', 'amount', 'indeed', 'towards', 'ourselves', 'his', 'across', 'twenty', 'down', 'whole', 'you', 'whereafter', 'above', 'eleven', 'was', 'though', 'off', 'will', 'well', 'hereupon', 'one', 'me', 'third', 'itself', 'ten', 'might'}\n"
          ]
        }
      ],
      "source": [
        "not_stop = {'nor', 'no', 'through', 'elsewhere', 'anyway', 'until', 'without', 'noone', 'otherwise', 'not', 'none', 'else', 'nobody', 'anyhow', 'less', 'whatever', 'never', 'few', 'rather', 'however', 'nowhere'}\n",
        "\n",
        "# Initialize an empty set to hold the customized stop words\n",
        "my_stop = set([])\n",
        "\n",
        "# Iterate over the default stop words set\n",
        "for i in stop_words:\n",
        "    # If a default stop word is not in the not_stop set (i.e., it's not a word we want to keep), add it to the custom stop words set\n",
        "    if i not in not_stop:\n",
        "        my_stop.add(i)\n",
        "\n",
        "\n",
        "print(my_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gensim\n",
        "\n",
        "Please note that sometimes it is difficult to install\n",
        "\n",
        "**Gensim** is an open-source Python library for **unsupervised topic modeling** and **natural language processing**, especially known for working with **word embeddings** like **Word2Vec**, **FastText**, and **GloVe**.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Gensim?\n",
        "\n",
        "- **Gensim** stands for *\"Generate Similar\"*.\n",
        "- It is optimized for:\n",
        "  - **Handling large text corpora** efficiently.\n",
        "  - **Streaming data** (doesn‚Äôt require loading everything into memory).\n",
        "  - **Training** and **using** vector space models (e.g., Word2Vec, LDA).\n",
        "\n",
        "---\n",
        "\n",
        "## Key Features\n",
        "\n",
        "| Feature             | Description                                                                 |\n",
        "|---------------------|-----------------------------------------------------------------------------|\n",
        "| **Word Embeddings** | Word2Vec, FastText, and GloVe support                                       |\n",
        "| **Topic Modeling**  | Latent Dirichlet Allocation (LDA), LSI, and HDP                             |\n",
        "| **Similarity Queries** | Find similar documents or words                                           |\n",
        "| **Memory Efficient**| Processes data using iterators, not full in-memory loading                  |\n",
        "| **Pretrained Models** | Easy loading and usage of large-scale models like Word2Vec Google News     |\n",
        "\n",
        "---\n",
        "\n",
        "## Common Use Cases\n",
        "\n",
        "- Semantic similarity (e.g., *\"king\" - \"man\" + \"woman\" ‚âà \"queen\"*)\n",
        "- Text classification\n",
        "- Document clustering\n",
        "- Recommendation systems\n",
        "- Chatbot NLP pipelines\n",
        "\n",
        "---\n",
        "\n",
        "## Documentation\n",
        "\n",
        "Official site: [https://radimrehurek.com/gensim](https://radimrehurek.com/gensim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6SgcxXXp5I8",
        "outputId": "8b3b9f81-2cba-4603-f304-becb6e904302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Available Gensim Models: 13 total\n",
            "\n",
            "üîπ Model Name: fasttext-wiki-news-subwords-300\n",
            "   üìÑ Description : 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\n",
            "   üì¶ File Size   : 958.45 MB\n",
            "   üî¢ Num Vectors : 999999\n",
            "------------------------------------------------------------\n",
            "üîπ Model Name: conceptnet-numberbatch-17-06-300\n",
            "   üìÑ Description : ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.\n",
            "   üì¶ File Size   : 1168.73 MB\n",
            "   üî¢ Num Vectors : 1917247\n",
            "------------------------------------------------------------\n",
            "üîπ Model Name: word2vec-ruscorpora-300\n",
            "   üìÑ Description : Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.\n",
            "   üì¶ File Size   : 198.77 MB\n",
            "   üî¢ Num Vectors : 184973\n",
            "------------------------------------------------------------\n",
            "üîπ Model Name: word2vec-google-news-300\n",
            "   üìÑ Description : Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\n",
            "   üì¶ File Size   : 1662.79 MB\n",
            "   üî¢ Num Vectors : 3000000\n",
            "------------------------------------------------------------\n",
            "üîπ Model Name: glove-wiki-gigaword-50\n",
            "   üìÑ Description : Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "   üì¶ File Size   : 65.98 MB\n",
            "   üî¢ Num Vectors : 400000\n",
            "------------------------------------------------------------\n",
            "üîπ Model Name: glove-wiki-gigaword-100\n",
            "   üìÑ Description : Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "   üì¶ File Size   : 128.08 MB\n",
            "   üî¢ Num Vectors : 400000\n",
            "------------------------------------------------------------\n",
            "üîπ Model Name: glove-wiki-gigaword-200\n",
            "   üìÑ Description : Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "   üì¶ File Size   : 252.09 MB\n",
            "   üî¢ Num Vectors : 400000\n",
            "------------------------------------------------------------\n",
            "üîπ Model Name: glove-wiki-gigaword-300\n",
            "   üìÑ Description : Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "   üì¶ File Size   : 376.09 MB\n",
            "   üî¢ Num Vectors : 400000\n",
            "------------------------------------------------------------\n",
            "üîπ Model Name: glove-twitter-25\n",
            "   üìÑ Description : Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "   üì¶ File Size   : 104.79 MB\n",
            "   üî¢ Num Vectors : 1193514\n",
            "------------------------------------------------------------\n",
            "üîπ Model Name: glove-twitter-50\n",
            "   üìÑ Description : Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\n",
            "   üì¶ File Size   : 199.52 MB\n",
            "   üî¢ Num Vectors : 1193514\n",
            "------------------------------------------------------------\n",
            "üîπ Model Name: glove-twitter-100\n",
            "   üìÑ Description : Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\n",
            "   üì¶ File Size   : 387.13 MB\n",
            "   üî¢ Num Vectors : 1193514\n",
            "------------------------------------------------------------\n",
            "üîπ Model Name: glove-twitter-200\n",
            "   üìÑ Description : Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "   üì¶ File Size   : 758.53 MB\n",
            "   üî¢ Num Vectors : 1193514\n",
            "------------------------------------------------------------\n",
            "üîπ Model Name: __testing_word2vec-matrix-synopsis\n",
            "   üìÑ Description : [THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.\n",
            "   üì¶ File Size   : 0.00 MB\n",
            "   üî¢ Num Vectors : N/A\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "from pprint import pprint\n",
        "\n",
        "# Get full info dictionary\n",
        "info = api.info()\n",
        "models_info = info['models']\n",
        "\n",
        "print(f\"üì¶ Available Gensim Models: {len(models_info)} total\\n\")\n",
        "\n",
        "# Print detailed info for each model\n",
        "for name, meta in models_info.items():\n",
        "    print(f\"üîπ Model Name: {name}\")\n",
        "    print(f\"   üìÑ Description : {meta.get('description', 'N/A')}\")\n",
        "    print(f\"   üì¶ File Size   : {meta.get('file_size', 0) / 1024 / 1024:.2f} MB\")\n",
        "    print(f\"   üî¢ Num Vectors : {meta.get('num_records', 'N/A')}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî§ Word Embeddings: Word2Vec vs GloVe\n",
        "\n",
        "Word embeddings are techniques used to convert words into numerical vectors that capture **semantic meaning** ‚Äî similar words will have similar vector representations.\n",
        "\n",
        "Two major methods are:\n",
        "\n",
        "- **Word2Vec**: Predicts context from words or words from context.\n",
        "- **GloVe**: Uses word co-occurrence statistics across the entire corpus.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Word2Vec\n",
        "\n",
        "- **Developed by**: Google (2013)\n",
        "- **Architecture**:\n",
        "  - **CBOW (Continuous Bag of Words)**: Predicts the target word from surrounding context.\n",
        "  - **Skip-Gram**: Predicts the context words from the target word.\n",
        "- **Training Objective**: Maximize probability of context words given a word (or vice versa).\n",
        "- **Local Context**: Learns embeddings based on sliding window context.\n",
        "\n",
        "### Example Code\n",
        "```python\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [[\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
        "             [\"dogs\", \"run\", \"fast\"]]\n",
        "\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "print(model.wv[\"cat\"])  # vector representation of \"cat\"\n",
        "```\n",
        "\n",
        "### Pros\n",
        "- Learns efficient embeddings from context.\n",
        "- Captures semantic and syntactic relationships (e.g., *king - man + woman ‚âà queen*).\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ GloVe (Global Vectors for Word Representation)\n",
        "\n",
        "- **Developed by**: Stanford (2014)\n",
        "- **Training Objective**: Factorizes a word co-occurrence matrix to learn word vectors.\n",
        "- **Global Context**: Uses statistics from the entire corpus (not just local window).\n",
        "\n",
        "### Example Code (Using `gensim`)\n",
        "```python\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load pretrained GloVe converted to word2vec format\n",
        "model = KeyedVectors.load_word2vec_format(\"glove.6B.100d.word2vec.txt\", binary=False)\n",
        "\n",
        "print(model[\"cat\"])  # vector for \"cat\"\n",
        "```\n",
        "\n",
        "### Pros\n",
        "- Incorporates global word co-occurrence information.\n",
        "- Often yields better results on similarity tasks than Word2Vec.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è Comparison\n",
        "\n",
        "| Feature            | Word2Vec                  | GloVe                           |\n",
        "|--------------------|---------------------------|---------------------------------|\n",
        "| Developed by       | Google (2013)             | Stanford (2014)                 |\n",
        "| Approach           | Predictive (context-based)| Count-based (co-occurrence)     |\n",
        "| Context Type       | Local (window)            | Global (corpus-wide)            |\n",
        "| Architectures      | CBOW, Skip-Gram           | Matrix Factorization            |\n",
        "| Pre-trained Models | Google News, others       | Common Crawl, Wikipedia         |\n",
        "| Speed              | Fast                      | Fast                            |\n",
        "| Semantic Power     | ‚úÖ Strong                 | ‚úÖ Strong                        |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV9uJdlGWXyo"
      },
      "source": [
        "\n",
        "# Building a spam detection model with GloVe\n",
        "\n",
        "## GloVe: Global Vectors for Word Representation\n",
        "GloVe is an unsupervised learning algorithm designed to generate vector representations for words. Unlike traditional methods that primarily focus on local context, GloVe stands out by integrating global statistics of a corpus into its learning process.\n",
        "\n",
        "### How GloVe Works\n",
        "GloVe captures vector representations by examining the co-occurrence frequencies of words across the entire dataset. This approach allows it to consider both the local context of words (similar to Word2Vec) and the broader, global relationships between words based on their co-occurrence patterns. The result is a comprehensive embedding that provides a richer understanding of word meanings, effectively bridging the gap between local semantic relationships and global lexical patterns.\n",
        "\n",
        "### GloVe vs. Word2Vec\n",
        "While Word2Vec learns word vectors by either predicting a word given its context (Continuous Bag of Words, CBOW) or predicting the context given a word (Skip-Gram), GloVe adopts a different strategy. It focuses on the aggregate global statistics of word co-occurrences throughout the entire text corpus. This method not only considers the immediate context but also leverages the frequency with which words appear together, thereby capturing a wide spectrum of semantic relationships.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RQc4_VsKp6OU"
      },
      "outputs": [],
      "source": [
        "wv = gensim.downloader.load('glove-twitter-50')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVeAxli53X1a"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nIydIW33Z-5"
      },
      "source": [
        "### Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AJ2DkSS6qAk8"
      },
      "outputs": [],
      "source": [
        "def sent_vec(sent, wv):\n",
        "    \"\"\"\n",
        "    Evaluate the average vector for a sentence.\n",
        "\n",
        "    Args:\n",
        "    sent: Iterable of words (tokens) in a sentence.\n",
        "    wv: GloVe model or similar (like Word2Vec), providing vector representations for words.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: The average vector representing the sentence.\n",
        "    \"\"\"\n",
        "    # Extract the vectors for words found in the model's vocabulary\n",
        "    word_vectors = np.array([wv[w] for w in sent if w in wv])\n",
        "\n",
        "    if len(word_vectors) == 0:\n",
        "        # If no words in the sentence are found in the model, return a zero vector\n",
        "        return np.zeros(wv.vector_size)\n",
        "\n",
        "    # Compute the average vector across all extracted word vectors\n",
        "    avg_vector = np.mean(word_vectors, axis=0)\n",
        "\n",
        "    return avg_vector\n",
        "\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Tokenize, lemmatize, filter stopwords/punctuations, and check if alphabetical in one step\n",
        "    tokens = [\n",
        "        snowBallStemmer.stem(word.lemma_.lower().strip())\n",
        "        for word in nlp(sentence)\n",
        "        if word.lemma_.lower().strip() not in my_stop\n",
        "        and word.lemma_.lower().strip() not in punctuations\n",
        "        and word.lemma_.isalpha() # alphabetic character check\n",
        "    ]\n",
        "    return tokens\n",
        "\n",
        "# For batch processing (more efficient if spam_df['content'] is large)\n",
        "def tokenize_texts(texts):\n",
        "    return [spacy_tokenizer(doc) for doc in nlp.pipe(texts, batch_size=20)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJJyWA-P3b3i"
      },
      "source": [
        "### Ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DY8nXLXmqFRE"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import spacy\n",
        "\n",
        "# Load SpaCy model with disabled components for efficiency\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "# Assuming snowBallStemmer and my_stop are defined\n",
        "# Convert my_stop and punctuations to sets for faster lookup\n",
        "my_stop = set(my_stop)  # Assuming my_stop is defined\n",
        "punctuations = set(string.punctuation)  # Assuming punctuations are defined\n",
        "\n",
        "snowBallStemmer = SnowballStemmer(language=\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "d-CzSx58qhA1"
      },
      "outputs": [],
      "source": [
        "spam_df = pd.DataFrame(texts, columns=['content'], index=range(0, len(texts)))\n",
        "spam_df['label'] = labels\n",
        "\n",
        "data = pd.DataFrame()\n",
        "data[\"sent\"] = tokenize_texts(spam_df['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dsqR7foNs9Vr"
      },
      "outputs": [],
      "source": [
        "data['vec'] = data['sent'].apply(lambda x: sent_vec(x, wv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "P2QrOubPq0mO"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "data['v1'] = le.fit_transform(spam_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AAycQenFrAzO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = data['vec'].to_list()\n",
        "y = data['v1'].to_list()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify = y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting xgboost\n",
            "  Downloading xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy in /home/fabio/.local/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /home/fabio/.local/lib/python3.12/site-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /home/fabio/.local/lib/python3.12/site-packages (from xgboost) (1.13.1)\n",
            "Downloading xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xgboost\n",
            "Successfully installed xgboost-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnebpZfDttLd"
      },
      "outputs": [],
      "source": [
        "# Import various ensemble and individual classifiers from scikit-learn\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression       # Linear model\n",
        "from sklearn.svm import SVC                               # Support Vector Machine\n",
        "from sklearn.tree import DecisionTreeClassifier           # Decision Tree\n",
        "from sklearn.neighbors import KNeighborsClassifier        # K-Nearest Neighbors\n",
        "from sklearn.ensemble import (                            # Ensemble methods\n",
        "    RandomForestClassifier,\n",
        "    ExtraTreesClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    BaggingClassifier,\n",
        "    GradientBoostingClassifier\n",
        ")\n",
        "import xgboost as xgb                                     # XGBoost library\n",
        "from sklearn import metrics                               # Evaluation metrics\n",
        "\n",
        "# Initialize individual classifiers with specific hyperparameters\n",
        "\n",
        "# Logistic Regression with L1 regularization (sparse weights)\n",
        "lg = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "\n",
        "# Support Vector Machine with sigmoid kernel and gamma=1.0\n",
        "sv = SVC(kernel='sigmoid', gamma=1.0)\n",
        "\n",
        "# Decision Tree with max depth of 5 (to prevent overfitting)\n",
        "dtc = DecisionTreeClassifier(max_depth=5)\n",
        "\n",
        "# K-Nearest Neighbors (default settings)\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Random Forest with 50 trees and a fixed random seed\n",
        "rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
        "\n",
        "# Extra Trees (similar to Random Forest but more randomized)\n",
        "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
        "\n",
        "# AdaBoost (adaptive boosting) with 50 weak learners\n",
        "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
        "\n",
        "# Bagging (bootstrap aggregating) classifier with 50 estimators\n",
        "bg = BaggingClassifier(n_estimators=50, random_state=2)\n",
        "\n",
        "# Gradient Boosting with 50 boosting stages\n",
        "gbc = GradientBoostingClassifier(n_estimators=50, random_state=2)\n",
        "\n",
        "# XGBoost classifier with 50 trees and fixed seed\n",
        "xgb = xgb.XGBClassifier(n_estimators=50, random_state=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "alpcvhhYtu0N"
      },
      "outputs": [],
      "source": [
        "def score_prediction(model,X_train,X_test,y_train,y_test):\n",
        "    model.fit(X_train,y_train)\n",
        "    pr = model.predict(X_test)\n",
        "    acc_score = metrics.accuracy_score(y_test,pr)\n",
        "    pre_score = metrics.precision_score(y_test,pr)\n",
        "\n",
        "    return acc_score,pre_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sZKYv9IPtwGN"
      },
      "outputs": [],
      "source": [
        "acc_score = {}\n",
        "pre_score = {}\n",
        "clfs= {\n",
        "    'LR':lg,\n",
        "    'SVM':sv,\n",
        "    'DTC':dtc,\n",
        "    'KNN':knn,\n",
        "    'RFC':rfc,\n",
        "    'ETC':etc,\n",
        "    'ABC':abc,\n",
        "    'BG':bg,\n",
        "    'GBC':gbc,\n",
        "    'XGB':xgb\n",
        "}\n",
        "for name,clf in clfs.items():\n",
        "    current_score,current_precision = score_prediction(clf, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    acc_score[name]=current_score\n",
        "    pre_score[name]=current_precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "y341NG9BtxN9",
        "outputId": "e9e8afde-c1f7-4c99-de6a-71b75f535eb2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"‚ñ∏\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"‚ñæ\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;LR&#x27;,\n",
              "                              LogisticRegression(penalty=&#x27;l1&#x27;,\n",
              "                                                 solver=&#x27;liblinear&#x27;)),\n",
              "                             (&#x27;SVM&#x27;, SVC(gamma=1.0, kernel=&#x27;sigmoid&#x27;)),\n",
              "                             (&#x27;DTC&#x27;, DecisionTreeClassifier(max_depth=5)),\n",
              "                             (&#x27;KNN&#x27;, KNeighborsClassifier()),\n",
              "                             (&#x27;RFC&#x27;,\n",
              "                              RandomForestClassifier(n_estimators=50,\n",
              "                                                     random_state=2)),\n",
              "                             (&#x27;ETC&#x27;,\n",
              "                              ExtraTreesClassifier(n_estimators=50,\n",
              "                                                   random_state=2)),\n",
              "                             (&#x27;ABC&#x27;, AdaBoostClassifi...\n",
              "                                            feature_weights=None, gamma=None,\n",
              "                                            grow_policy=None,\n",
              "                                            importance_type=None,\n",
              "                                            interaction_constraints=None,\n",
              "                                            learning_rate=None, max_bin=None,\n",
              "                                            max_cat_threshold=None,\n",
              "                                            max_cat_to_onehot=None,\n",
              "                                            max_delta_step=None, max_depth=None,\n",
              "                                            max_leaves=None,\n",
              "                                            min_child_weight=None, missing=nan,\n",
              "                                            monotone_constraints=None,\n",
              "                                            multi_strategy=None,\n",
              "                                            n_estimators=50, n_jobs=None,\n",
              "                                            num_parallel_tree=None, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>VotingClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.VotingClassifier.html\">?<span>Documentation for VotingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>VotingClassifier(estimators=[(&#x27;LR&#x27;,\n",
              "                              LogisticRegression(penalty=&#x27;l1&#x27;,\n",
              "                                                 solver=&#x27;liblinear&#x27;)),\n",
              "                             (&#x27;SVM&#x27;, SVC(gamma=1.0, kernel=&#x27;sigmoid&#x27;)),\n",
              "                             (&#x27;DTC&#x27;, DecisionTreeClassifier(max_depth=5)),\n",
              "                             (&#x27;KNN&#x27;, KNeighborsClassifier()),\n",
              "                             (&#x27;RFC&#x27;,\n",
              "                              RandomForestClassifier(n_estimators=50,\n",
              "                                                     random_state=2)),\n",
              "                             (&#x27;ETC&#x27;,\n",
              "                              ExtraTreesClassifier(n_estimators=50,\n",
              "                                                   random_state=2)),\n",
              "                             (&#x27;ABC&#x27;, AdaBoostClassifi...\n",
              "                                            feature_weights=None, gamma=None,\n",
              "                                            grow_policy=None,\n",
              "                                            importance_type=None,\n",
              "                                            interaction_constraints=None,\n",
              "                                            learning_rate=None, max_bin=None,\n",
              "                                            max_cat_threshold=None,\n",
              "                                            max_cat_to_onehot=None,\n",
              "                                            max_delta_step=None, max_depth=None,\n",
              "                                            max_leaves=None,\n",
              "                                            min_child_weight=None, missing=nan,\n",
              "                                            monotone_constraints=None,\n",
              "                                            multi_strategy=None,\n",
              "                                            n_estimators=50, n_jobs=None,\n",
              "                                            num_parallel_tree=None, ...))])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>LR</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>SVM</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(gamma=1.0, kernel=&#x27;sigmoid&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>DTC</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=5)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>KNN</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KNeighborsClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">?<span>Documentation for KNeighborsClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>RFC</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_estimators=50, random_state=2)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>ETC</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>ExtraTreesClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\">?<span>Documentation for ExtraTreesClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>ExtraTreesClassifier(n_estimators=50, random_state=2)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>ABC</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>AdaBoostClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\">?<span>Documentation for AdaBoostClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>AdaBoostClassifier(random_state=2)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>BG</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>BaggingClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.BaggingClassifier.html\">?<span>Documentation for BaggingClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>BaggingClassifier(n_estimators=50, random_state=2)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>GBC</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GradientBoostingClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\">?<span>Documentation for GradientBoostingClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>GradientBoostingClassifier(n_estimators=50, random_state=2)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>XGB</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBClassifier\">?<span>Documentation for XGBClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              feature_weights=None, gamma=None, grow_policy=None,\n",
              "              importance_type=None, interaction_constraints=None,\n",
              "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
              "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
              "              max_leaves=None, min_child_weight=None, missing=nan,\n",
              "              monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
              "              n_jobs=None, num_parallel_tree=None, ...)</pre></div> </div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "VotingClassifier(estimators=[('LR',\n",
              "                              LogisticRegression(penalty='l1',\n",
              "                                                 solver='liblinear')),\n",
              "                             ('SVM', SVC(gamma=1.0, kernel='sigmoid')),\n",
              "                             ('DTC', DecisionTreeClassifier(max_depth=5)),\n",
              "                             ('KNN', KNeighborsClassifier()),\n",
              "                             ('RFC',\n",
              "                              RandomForestClassifier(n_estimators=50,\n",
              "                                                     random_state=2)),\n",
              "                             ('ETC',\n",
              "                              ExtraTreesClassifier(n_estimators=50,\n",
              "                                                   random_state=2)),\n",
              "                             ('ABC', AdaBoostClassifi...\n",
              "                                            feature_weights=None, gamma=None,\n",
              "                                            grow_policy=None,\n",
              "                                            importance_type=None,\n",
              "                                            interaction_constraints=None,\n",
              "                                            learning_rate=None, max_bin=None,\n",
              "                                            max_cat_threshold=None,\n",
              "                                            max_cat_to_onehot=None,\n",
              "                                            max_delta_step=None, max_depth=None,\n",
              "                                            max_leaves=None,\n",
              "                                            min_child_weight=None, missing=nan,\n",
              "                                            monotone_constraints=None,\n",
              "                                            multi_strategy=None,\n",
              "                                            n_estimators=50, n_jobs=None,\n",
              "                                            num_parallel_tree=None, ...))])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "estimators=[]\n",
        "for name,clf in clfs.items():\n",
        "    estimators.append((name,clf))\n",
        "\n",
        "voting = VotingClassifier(estimators=estimators,voting='hard')\n",
        "voting.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjlLU3Jyt6TH",
        "outputId": "066b1168-5f89-41a9-c5f3-3963f192d54f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Evaluation Metrics\n",
            "‚úÖ Accuracy : 0.9555\n",
            "üéØ Precision: 0.9371\n",
            "üì• Recall   : 0.7166\n",
            "üèÜ F1 Score : 0.8121\n",
            "\n",
            "üìä Confusion Matrix (Raw Counts):\n",
            "  True Negative: 1198\n",
            " False Positive: 9\n",
            " False Negative: 53\n",
            "  True Positive: 134\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìÑ Detailed Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.96      0.99      0.97      1207\n",
            "        Spam       0.94      0.72      0.81       187\n",
            "\n",
            "    accuracy                           0.96      1394\n",
            "   macro avg       0.95      0.85      0.89      1394\n",
            "weighted avg       0.95      0.96      0.95      1394\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Predict\n",
        "y_pred = voting.predict(X_test)\n",
        "\n",
        "# Basic metrics\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "precision = metrics.precision_score(y_test, y_pred)\n",
        "recall = metrics.recall_score(y_test, y_pred)\n",
        "f1 = metrics.f1_score(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(\"üîç Evaluation Metrics\")\n",
        "print(f\"‚úÖ Accuracy : {accuracy:.4f}\")\n",
        "print(f\"üéØ Precision: {precision:.4f}\")\n",
        "print(f\"üì• Recall   : {recall:.4f}\")\n",
        "print(f\"üèÜ F1 Score : {f1:.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = metrics.confusion_matrix(y_test, y_pred)\n",
        "cm_labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
        "cm_dict = dict(zip(cm_labels, cm.ravel()))\n",
        "\n",
        "print(\"\\nüìä Confusion Matrix (Raw Counts):\")\n",
        "for label, count in cm_dict.items():\n",
        "    print(f\"{label:>15}: {count}\")\n",
        "\n",
        "# Visualize confusion matrix as heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Ham', 'Predicted Spam'],\n",
        "            yticklabels=['Actual Ham', 'Actual Spam'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Prediction\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nüìÑ Detailed Classification Report:\\n\")\n",
        "report = metrics.classification_report(y_test, y_pred, target_names=['Ham', 'Spam'])\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìù Exercise: Exploring Semantic Relationships with GloVe\n",
        "\n",
        "#### üìö Description\n",
        "\n",
        "In this exercise, you'll use the **pretrained GloVe model (`glove-wiki-gigaword-100`)** to explore semantic relationships between words. Your tasks are:\n",
        "\n",
        "1. **Load the pretrained GloVe model** using Gensim's downloader.\n",
        "2. Use it to:\n",
        "   - Find the **most similar words** to a given word.\n",
        "   - Solve **word analogies** (e.g., _king - man + woman = ?_).\n",
        "   - Compute **similarity** between pairs of words.\n",
        "3. Reflect on how GloVe captures relationships between words and how it differs from Word2Vec.\n",
        "\n",
        "#### üîß Instructions\n",
        "\n",
        "- Use `gensim.downloader` to load and interact with the model.\n",
        "- Suggested words/analogies to try:\n",
        "  - Similar words: `\"king\"`, `\"computer\"`, `\"school\"`\n",
        "  - Analogy: `\"king\" - \"man\" + \"woman\"`\n",
        "  - Word pairs: `(\"cat\", \"dog\")`, `(\"book\", \"banana\")`\n",
        "\n",
        "> üí° **Note**: GloVe vectors are trained on large text corpora and provide a dense representation of meaning. This specific model (`glove-wiki-gigaword-100`) contains 100-dimensional embeddings trained on Wikipedia and Gigaword.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Solution (in Python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Most similar to 'king':\n",
            "[('prince', 0.7682328820228577), ('queen', 0.7507690787315369), ('son', 0.7020888328552246), ('brother', 0.6985775232315063), ('monarch', 0.6977890729904175), ('throne', 0.6919989585876465), ('kingdom', 0.6811409592628479), ('father', 0.6802029013633728), ('emperor', 0.6712858080863953), ('ii', 0.6676074266433716)]\n",
            "\n",
            "Analogy: king - man + woman = ?\n",
            "[('queen', 0.7698540687561035)]\n",
            "\n",
            "Similarity between 'cat' and 'dog': 0.8798075\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load(\"glove-wiki-gigaword-100\")  # Much smaller & fast\n",
        "\n",
        "# Example usage\n",
        "print(\"Most similar to 'king':\")\n",
        "print(model.most_similar(\"king\"))\n",
        "\n",
        "print(\"\\nAnalogy: king - man + woman = ?\")\n",
        "print(model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1))\n",
        "\n",
        "print(\"\\nSimilarity between 'cat' and 'dog':\", model.similarity(\"cat\", \"dog\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise: Train Your Own Word2Vec Model on a Small Text Corpus\n",
        "\n",
        "#### Description\n",
        "\n",
        "In this exercise, you'll train a **Word2Vec model** from scratch using Gensim on a small sample dataset (such as a list of simple sentences). This will help you understand how word vectors are learned based on **context**.\n",
        "\n",
        "#### Objectives\n",
        "\n",
        "1. Prepare a small text corpus (manually or with NLTK).\n",
        "2. Tokenize and preprocess the text (lowercase, remove punctuation, etc.).\n",
        "3. Train a Word2Vec model using Gensim.\n",
        "4. Explore:\n",
        "   - Most similar words\n",
        "   - Word similarity scores\n",
        "   - Word analogies\n",
        "\n",
        "---\n",
        "\n",
        "### Suggested Workflow\n",
        "\n",
        "#### 1. Tokenize a small corpus (or load a sample one like `nltk.corpus.brown`)\n",
        "#### 2. Train `Word2Vec` using `gensim.models.Word2Vec`\n",
        "#### 3. Play with `.most_similar()` and `.similarity()`\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Notes\n",
        "\n",
        "- You can replace `brown` with your own list of sentences.\n",
        "- Adjust `vector_size`, `window`, and `min_count` to see how they affect results.\n",
        "- This approach avoids downloading large files and still teaches key Word2Vec concepts.\n",
        "\n",
        "---\n",
        "\n",
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /home/fabio/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /home/fabio/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most similar to 'government':\n",
            "[('from', 0.9998201727867126), ('up', 0.9998200535774231), ('his', 0.9998137950897217), ('an', 0.9998091459274292), ('a', 0.9998082518577576), ('but', 0.9998072981834412), ('or', 0.9998056292533875), ('which', 0.9998018145561218), ('her', 0.9997975826263428), ('who', 0.99979567527771)]\n",
            "\n",
            "Similarity between 'president' and 'leader':\n",
            "0.99913955\n",
            "\n",
            "Analogy: king - man + woman = ? (if vocab allows)\n",
            "[('fear', 0.960841178894043), ('win', 0.9602473974227905), ('vienna', 0.9600226283073425), ('whom', 0.9599260091781616), ('working', 0.9598702192306519), ('institutions', 0.9595360159873962), ('tomorrow', 0.9594978094100952), ('personal', 0.9591019749641418), ('effort', 0.9589598774909973), ('road', 0.9589173197746277)]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "\n",
        "# 1. Download and load the Brown corpus\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 2. Use a subset of Brown corpus for speed\n",
        "sentences = brown.sents(categories='news')[:5000]\n",
        "\n",
        "# 3. Preprocess: lowercase and remove punctuation\n",
        "def clean(sentence):\n",
        "    return [word.lower() for word in sentence if word.isalpha()]\n",
        "\n",
        "cleaned_sentences = [clean(sent) for sent in sentences if len(sent) > 2]\n",
        "\n",
        "# 4. Train a Word2Vec model\n",
        "model = Word2Vec(sentences=cleaned_sentences, vector_size=100, window=5, min_count=2, workers=2)\n",
        "\n",
        "# 5. Explore model\n",
        "print(\"Most similar to 'government':\")\n",
        "print(model.wv.most_similar(\"government\"))\n",
        "\n",
        "print(\"\\nSimilarity between 'president' and 'leader':\")\n",
        "print(model.wv.similarity(\"president\", \"leader\"))\n",
        "\n",
        "print(\"\\nAnalogy: king - man + woman = ? (if vocab allows)\")\n",
        "try:\n",
        "    print(model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"]))\n",
        "except KeyError:\n",
        "    print(\"Words not found in the vocabulary. Try a different analogy.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
