{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltmpJDvvaZi-"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44cPyhGxaZi_"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G-2cCFpaZi_",
        "outputId": "8e355e1d-2172-45f3-c987-23150e740bf5"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj0hHTngaZjA",
        "outputId": "64bd9ca3-a34f-4afd-fd2f-2096013271f5"
      },
      "outputs": [],
      "source": [
        "pip install \"numpy<2.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qah31RHdaZjA",
        "outputId": "fc64c2d4-5e7d-4fdf-ba4f-43eb5ae9fda8"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbCsbagBaZjA",
        "outputId": "ebcaecc0-57fb-4d21-987a-cdd70a150bfc"
      },
      "outputs": [],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMWj0PosaZjA"
      },
      "outputs": [],
      "source": [
        "pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HMfmvC0aaZjA"
      },
      "outputs": [],
      "source": [
        "# import statements\n",
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_text\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR) # suppress warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY0qcEQvaZjA"
      },
      "source": [
        "## Load and preprocess the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "V-wAInonaZjA",
        "outputId": "32743cc5-450d-4a87-f195-00afd1d609f3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 74742,\n  \"fields\": [\n    {\n      \"column\": \"sentence1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6205,\n        \"samples\": [\n          \"\\u5973\\u4eba\\u5728\\u8bf4\\u8bdd\",\n          \"een hond springt op een trampoline\",\n          \"\\u4e00\\u4e2a\\u5973\\u4eba\\u6b63\\u5728\\u5207\\u5927\\u8fa3\\u6912\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6166,\n        \"samples\": [\n          \"una mujer lleva un canguro\",\n          \"un uomo sta scolpendo una statua\",\n          \"een leeuw loopt in een pen\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.6047856807859955,\n        \"min\": 0.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 83,\n        \"samples\": [\n          3.5,\n          5.0,\n          3.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lang1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"ja\",\n          \"de\",\n          \"ru\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lang2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"nl\",\n          \"en\",\n          \"zh\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_language1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5948,\n        \"samples\": [\n          \"Tier spielen Wasser\",\n          \"man knuffelt\",\n          \"donna prendere canguro\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_language2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5894,\n        \"samples\": [\n          \"\\u7537 \\u934b \\u91ce\\u83dc \\u5165\\u308c\\u308b\",\n          \"homme chant sc\\u00e8ne\",\n          \"stare sciogliere burro padella\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-37062b22-50f9-44b3-af1f-0238bf2fd42b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>score</th>\n",
              "      <th>lang1</th>\n",
              "      <th>lang2</th>\n",
              "      <th>processed_language1</th>\n",
              "      <th>processed_language2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ein flugzeug hebt gerade ab</td>\n",
              "      <td>an air plane is taking off</td>\n",
              "      <td>5.0</td>\n",
              "      <td>de</td>\n",
              "      <td>en</td>\n",
              "      <td>Flugzeug heben</td>\n",
              "      <td>air plane</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ein flugzeug hebt gerade ab</td>\n",
              "      <td>un avión está despegando</td>\n",
              "      <td>5.0</td>\n",
              "      <td>de</td>\n",
              "      <td>es</td>\n",
              "      <td>Flugzeug heben</td>\n",
              "      <td>avión despegar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ein flugzeug hebt gerade ab</td>\n",
              "      <td>un avion est en train de décoller</td>\n",
              "      <td>5.0</td>\n",
              "      <td>de</td>\n",
              "      <td>fr</td>\n",
              "      <td>Flugzeug heben</td>\n",
              "      <td>avion train décoller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ein flugzeug hebt gerade ab</td>\n",
              "      <td>un aereo sta decollando</td>\n",
              "      <td>5.0</td>\n",
              "      <td>de</td>\n",
              "      <td>it</td>\n",
              "      <td>Flugzeug heben</td>\n",
              "      <td>aereo stare decollare</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ein flugzeug hebt gerade ab</td>\n",
              "      <td>飛行機が離陸します</td>\n",
              "      <td>5.0</td>\n",
              "      <td>de</td>\n",
              "      <td>ja</td>\n",
              "      <td>Flugzeug heben</td>\n",
              "      <td>飛行機 離陸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74737</th>\n",
              "      <td>un uomo sta tagliando i pomodori</td>\n",
              "      <td>uma mulher está a cortar brócolos</td>\n",
              "      <td>1.5</td>\n",
              "      <td>it</td>\n",
              "      <td>pt</td>\n",
              "      <td>stare tagliare pomodoro</td>\n",
              "      <td>mulher cortar brócolo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74738</th>\n",
              "      <td>un uomo sta tagliando i pomodori</td>\n",
              "      <td>женщина режет брокколи</td>\n",
              "      <td>1.5</td>\n",
              "      <td>it</td>\n",
              "      <td>ru</td>\n",
              "      <td>stare tagliare pomodoro</td>\n",
              "      <td>женщина режет брокколи</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74739</th>\n",
              "      <td>un uomo sta tagliando i pomodori</td>\n",
              "      <td>一个女人正在切西兰花</td>\n",
              "      <td>1.5</td>\n",
              "      <td>it</td>\n",
              "      <td>zh</td>\n",
              "      <td>stare tagliare pomodoro</td>\n",
              "      <td>女人             切       西兰花</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74740</th>\n",
              "      <td>男性がトマトを切っています</td>\n",
              "      <td>eine frau schneidet brokkoli</td>\n",
              "      <td>1.5</td>\n",
              "      <td>ja</td>\n",
              "      <td>de</td>\n",
              "      <td>男性 トマト 切る</td>\n",
              "      <td>Frau schneiden brokkoli</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74741</th>\n",
              "      <td>男性がトマトを切っています</td>\n",
              "      <td>a woman is cutting broccoli</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>74742 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37062b22-50f9-44b3-af1f-0238bf2fd42b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-37062b22-50f9-44b3-af1f-0238bf2fd42b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-37062b22-50f9-44b3-af1f-0238bf2fd42b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6837b374-e3a0-4479-9cff-d4ff1b2024da\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6837b374-e3a0-4479-9cff-d4ff1b2024da')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6837b374-e3a0-4479-9cff-d4ff1b2024da button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_b99ccc9c-c52f-4c57-a61e-e764bf8923da\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b99ccc9c-c52f-4c57-a61e-e764bf8923da button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                              sentence1                          sentence2  \\\n",
              "0           ein flugzeug hebt gerade ab         an air plane is taking off   \n",
              "1           ein flugzeug hebt gerade ab           un avión está despegando   \n",
              "2           ein flugzeug hebt gerade ab  un avion est en train de décoller   \n",
              "3           ein flugzeug hebt gerade ab            un aereo sta decollando   \n",
              "4           ein flugzeug hebt gerade ab                          飛行機が離陸します   \n",
              "...                                 ...                                ...   \n",
              "74737  un uomo sta tagliando i pomodori  uma mulher está a cortar brócolos   \n",
              "74738  un uomo sta tagliando i pomodori             женщина режет брокколи   \n",
              "74739  un uomo sta tagliando i pomodori                         一个女人正在切西兰花   \n",
              "74740                     男性がトマトを切っています       eine frau schneidet brokkoli   \n",
              "74741                     男性がトマトを切っています        a woman is cutting broccoli   \n",
              "\n",
              "       score lang1 lang2      processed_language1  \\\n",
              "0        5.0    de    en           Flugzeug heben   \n",
              "1        5.0    de    es           Flugzeug heben   \n",
              "2        5.0    de    fr           Flugzeug heben   \n",
              "3        5.0    de    it           Flugzeug heben   \n",
              "4        5.0    de    ja           Flugzeug heben   \n",
              "...      ...   ...   ...                      ...   \n",
              "74737    1.5    it    pt  stare tagliare pomodoro   \n",
              "74738    1.5    it    ru  stare tagliare pomodoro   \n",
              "74739    1.5    it    zh  stare tagliare pomodoro   \n",
              "74740    1.5    ja    de                男性 トマト 切る   \n",
              "74741    NaN   NaN   NaN                      NaN   \n",
              "\n",
              "                    processed_language2  \n",
              "0                             air plane  \n",
              "1                        avión despegar  \n",
              "2                  avion train décoller  \n",
              "3                 aereo stare decollare  \n",
              "4                                飛行機 離陸  \n",
              "...                                 ...  \n",
              "74737             mulher cortar brócolo  \n",
              "74738            женщина режет брокколи  \n",
              "74739        女人             切       西兰花  \n",
              "74740           Frau schneiden brokkoli  \n",
              "74741                               NaN  \n",
              "\n",
              "[74742 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# read CSV file\n",
        "df = pd.read_csv('/stopword_removal_dataframe.csv')\n",
        "\n",
        "# display the first few rows of the dataframe\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NorMEpXjaZjA",
        "outputId": "137f5246-829a-4166-b6a9-a59de70f18d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimum score: 0.0\n",
            "Maximum score: 5.0\n"
          ]
        }
      ],
      "source": [
        "# I want to get the minimum and maximum for column 'score'\n",
        "min_score = df['score'].min()\n",
        "max_score = df['score'].max()\n",
        "print(f\"Minimum score: {min_score}\")\n",
        "print(f\"Maximum score: {max_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRjMTd0NaZjA",
        "outputId": "44244290-f9a5-449b-d7a1-6075abbbba3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimum score: 0.0\n",
            "Maximum score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Just to check if the values are actually between 0 and 1\n",
        "df['normalized_score'] = df['score'] / 5.0\n",
        "min_normalized_score = df['normalized_score'].min()\n",
        "max_normalized_score = df['normalized_score'].max()\n",
        "print(f\"Minimum score: {min_normalized_score}\")\n",
        "print(f\"Maximum score: {max_normalized_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jWoaig9iaZjA"
      },
      "outputs": [],
      "source": [
        "# # Convert the filtered dataframe to two lists\n",
        "# input_texts = df['processed_language1'].tolist()\n",
        "# target_texts = df['processed_language2'].tolist()\n",
        "\n",
        "# print(type(input_texts), type(target_texts))         # should both be <class 'list'>\n",
        "# print(type(input_texts[0]), type(target_texts[0]))   # should both be <class 'str'>\n",
        "\n",
        "# # Ensure all elements in input_texts and target_texts are strings\n",
        "# input_texts = [str(text) for text in input_texts]\n",
        "# target_texts = [str(text) for text in target_texts]\n",
        "\n",
        "# # Create the TensorFlow dataset\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((input_texts, target_texts))\n",
        "\n",
        "# # Define split ratio\n",
        "# split_ratio = 0.8\n",
        "# total_size = len(input_texts)\n",
        "# train_size = int(total_size * split_ratio)\n",
        "\n",
        "# # Create train and validation datasets\n",
        "# train_examples = dataset.take(train_size)\n",
        "# val_examples = dataset.skip(train_size)\n",
        "\n",
        "# train_examples = train_examples.repeat()\n",
        "\n",
        "# here we are creating the train and val examples\n",
        "examples = [\n",
        "    {\n",
        "        \"sentence1\": row['processed_language1'],\n",
        "        \"sentence2\": row['processed_language2'],\n",
        "        \"score\": row['normalized_score']\n",
        "    }\n",
        "    for _, row in df.iterrows()\n",
        "]\n",
        "\n",
        "random.shuffle(examples)\n",
        "\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(examples) * split_ratio)\n",
        "\n",
        "train_examples = examples[:split_index]\n",
        "val_examples = examples[split_index:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIM7prtHaZjB",
        "outputId": "09cdf010-4671-4f63-db8e-bc2555a619dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sentence1': '誰 恩 物 混ぜる', 'sentence2': 'quelquun train coudre chose', 'score': 0.05}\n",
            "{'sentence1': 'arbre courber tempête', 'sentence2': 'drzewo uderzać silny wiatr', 'score': 0.5334}\n"
          ]
        }
      ],
      "source": [
        "# Let's check if both are in the right format\n",
        "print(train_examples[0])\n",
        "print(val_examples[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7qna1MMaZjB"
      },
      "source": [
        "## Build tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MG4i_DnRaZjB"
      },
      "outputs": [],
      "source": [
        "# # Build tokenizers from my text\n",
        "# input_tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "#     (text for text in input_texts), target_vocab_size=2**13)\n",
        "\n",
        "# target_tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "#     (text for text in target_texts), target_vocab_size=2**13)\n",
        "\n",
        "# Combine both sides of the pairs for a shared vocabulary\n",
        "all_sentences = pd.concat([df['processed_language1'], df['processed_language2']]).astype(str)\n",
        "\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (text for text in all_sentences), target_vocab_size=2**13)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDozk2oLaZjB"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_to_file('/Users/dionnespaltman/Desktop/Luiss/Machine Learning/Project/tokenizer_input')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKEdi3E1aZjB"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mBaLK0cLaZjB"
      },
      "outputs": [],
      "source": [
        "# # Function to tokenize input-target pairs\n",
        "# # So it gets the input string and the target string and returns tokenized version\n",
        "# def encode_pair(input_str, target_str):\n",
        "#     input_tokens = input_tokenizer.encode(input_str.numpy().decode('utf-8'))\n",
        "#     target_tokens = target_tokenizer.encode(target_str.numpy().decode('utf-8'))\n",
        "#     return input_tokens, target_tokens\n",
        "\n",
        "def encode_pair(sentence1, sentence2, score):\n",
        "    tokens1 = tokenizer.encode(sentence1.numpy().decode('utf-8'))\n",
        "    tokens2 = tokenizer.encode(sentence2.numpy().decode('utf-8'))\n",
        "    return tokens1, tokens2, score\n",
        "\n",
        "# def tf_encode(input_str, target_str):\n",
        "#     input_tokens, target_tokens = tf.py_function(encode_pair, [input_str, target_str], [tf.int64, tf.int64])\n",
        "#     return input_tokens, target_tokens\n",
        "\n",
        "# Wrap with tf.py_function to use in TensorFlow pipeline\n",
        "# This lets you use regular Python code (like calling .numpy() and .decode()) inside a TensorFlow data pipeline\n",
        "def tf_encode(sentence1, sentence2, score):\n",
        "    tokens1, tokens2, score = tf.py_function(\n",
        "        encode_pair, [sentence1, sentence2, score], [tf.int64, tf.int64, tf.float32]\n",
        "    )\n",
        "    tokens1.set_shape([None])\n",
        "    tokens2.set_shape([None])\n",
        "    score.set_shape([])  # scalar float\n",
        "    return tokens1, tokens2, score\n",
        "\n",
        "# # Map the datasets through the tokenizer\n",
        "# # It applies tf_encode to every pair in the dataset. Now each example in train_dataset is a pair of tokenized sequences\n",
        "# train_dataset = train_examples.map(tf_encode)\n",
        "# val_dataset = val_examples.map(tf_encode)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4EzdK5HaZjB"
      },
      "source": [
        "# Make batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UodJPd8IaZjB"
      },
      "source": [
        "A batch is a group of training examples processed together in one forward/backward pass, which speeds up training and stabilizes gradients.\n",
        "\n",
        "In NLP, each batch contains tokenized and padded sentence pairs (plus labels like similarity scores) arranged as tensors of equal shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bdglh9XGaZjB"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LEN = 128\n",
        "\n",
        "def encode_pair_fixed(input_str, target_str):\n",
        "    input_tokens = tokenizer.encode(input_str.numpy().decode('utf-8'))[:MAX_SEQ_LEN]\n",
        "    target_tokens = tokenizer.encode(target_str.numpy().decode('utf-8'))[:MAX_SEQ_LEN]\n",
        "\n",
        "    # Pad manually\n",
        "    input_tokens += [0] * (MAX_SEQ_LEN - len(input_tokens))\n",
        "    target_tokens += [0] * (MAX_SEQ_LEN - len(target_tokens))\n",
        "\n",
        "    return input_tokens, target_tokens\n",
        "\n",
        "def tf_encode(input_str, target_str, score):\n",
        "    input_tokens, target_tokens = tf.py_function(\n",
        "        encode_pair_fixed, [input_str, target_str], [tf.int64, tf.int64]\n",
        "    )\n",
        "    input_tokens.set_shape([MAX_SEQ_LEN])\n",
        "    target_tokens.set_shape([MAX_SEQ_LEN])\n",
        "    return input_tokens, target_tokens, score\n",
        "\n",
        "\n",
        "def make_batches(examples, batch_size=32):\n",
        "    sent1_list = []\n",
        "    sent2_list = []\n",
        "    score_list = []\n",
        "\n",
        "    for ex in examples:\n",
        "        try:\n",
        "            s1 = str(ex[\"sentence1\"])\n",
        "            s2 = str(ex[\"sentence2\"])\n",
        "            score = float(ex[\"score\"])\n",
        "            sent1_list.append(s1)\n",
        "            sent2_list.append(s2)\n",
        "            score_list.append(score)\n",
        "        except Exception as e:\n",
        "            print(\"Skipping example due to error:\", ex, e)\n",
        "            continue\n",
        "\n",
        "    sent1_tensor = tf.constant(sent1_list)\n",
        "    sent2_tensor = tf.constant(sent2_list)\n",
        "    score_tensor = tf.constant(score_list, dtype=tf.float32)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((sent1_tensor, sent2_tensor, score_tensor))\n",
        "    dataset = dataset.map(tf_encode, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    # dataset = dataset.padded_batch(batch_size, padded_shapes=([None], [None], []))\n",
        "    # MAX_SEQ_LEN = 128\n",
        "    # dataset = dataset.padded_batch(batch_size, padded_shapes=([MAX_SEQ_LEN], [MAX_SEQ_LEN], []))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    # dataset = dataset.repeat()  # Infinite dataset\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    # return dataset, sent1_list, sent2_list\n",
        "\n",
        "    return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9n-U3-6aZjB",
        "outputId": "8ec70b2f-d064-416f-989a-e1c8a196cb8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape sent1: (32, 128)\n",
            "Shape sent2: (32, 128)\n",
            "Shape score: (32,)\n"
          ]
        }
      ],
      "source": [
        "train_batches = make_batches(train_examples)\n",
        "\n",
        "# Inspect a single batch\n",
        "for sent1, sent2, score in train_batches.take(1):\n",
        "    print(\"Shape sent1:\", sent1.shape)\n",
        "    print(\"Shape sent2:\", sent2.shape)\n",
        "    print(\"Shape score:\", score.shape)\n",
        "\n",
        "    # Check if all shapes in the batch are as expected\n",
        "    assert sent1.shape[1] == MAX_SEQ_LEN, f\"sent1 sequence length mismatch: {sent1.shape[1]}\"\n",
        "    assert sent2.shape[1] == MAX_SEQ_LEN, f\"sent2 sequence length mismatch: {sent2.shape[1]}\"\n",
        "    assert sent1.shape == sent2.shape, f\"Mismatch between sent1 and sent2: {sent1.shape} vs {sent2.shape}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hL3qVdYMaZjB"
      },
      "outputs": [],
      "source": [
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l42UXDioaZjB"
      },
      "source": [
        "## Get everything ready\n",
        "\n",
        "- Prepare tokenized, batched, and masked data for input into a Transformer model.\n",
        "- Implement core components of the Transformer: positional encoding and multi-head attention.\n",
        "- This sits between raw data preprocessing and full model definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "ouPiXdsGaZjB",
        "outputId": "19b2d729-8c62-4aa6-a3cb-b283316f38dd"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'sent1_list' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-22c92ff7ba79>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample types in sent1_list:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent1_list\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample types in sent2_list:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent2_list\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sent1_list' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"Sample types in sent1_list:\", {type(s) for s in sent1_list})\n",
        "print(\"Sample types in sent2_list:\", {type(s) for s in sent2_list})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iKrv4PHMaZjB"
      },
      "outputs": [],
      "source": [
        "# DATASET PREPARATION\n",
        "\n",
        "# Define constants for dataset preparation\n",
        "# MAX_TOKENS = 128\n",
        "# BUFFER_SIZE = 20000  # Size of the buffer for shuffling the dataset.\n",
        "# BATCH_SIZE = 64  # Number of samples per batch.\n",
        "\n",
        "# Lower constants for baseline\n",
        "MAX_TOKENS = 128\n",
        "BUFFER_SIZE = 1000  # Size of the buffer for shuffling the dataset.\n",
        "BATCH_SIZE = 16  # Number of samples per batch.\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_pairs(input_text, target_text):\n",
        "    \"\"\"\n",
        "    Tokenizes both input and target strings using SubwordTextEncoder-like tokenizers.\n",
        "    Returns int64 token sequences with shape info attached.\n",
        "    \"\"\"\n",
        "    # Encode using SubwordTextEncoder and convert to padded tensors\n",
        "    input_tokens = tf.py_function(\n",
        "        lambda x: tf.constant(tokenizer.encode(x.numpy().decode('utf-8')), dtype=tf.int64),\n",
        "        [input_text],\n",
        "        tf.int64\n",
        "    )\n",
        "    target_tokens = tf.py_function(\n",
        "        lambda x: tf.constant(tokenizer.encode(x.numpy().decode('utf-8')), dtype=tf.int64),\n",
        "        [target_text],\n",
        "        tf.int64\n",
        "    )\n",
        "\n",
        "    # Help TF understand the rank of returned tensors for later batching\n",
        "    input_tokens.set_shape([None])\n",
        "    target_tokens.set_shape([None])\n",
        "    return input_tokens, target_tokens\n",
        "\n",
        "# Filtering function to exclude long sequences\n",
        "def filter_max_tokens(input_tensor, target_tensor):\n",
        "    \"\"\"\n",
        "    Filters out input-target pairs where either sequence exceeds MAX_TOKENS.\n",
        "    \"\"\"\n",
        "    num_tokens = tf.maximum(tf.shape(input_tensor)[0], tf.shape(target_tensor)[0])\n",
        "    return num_tokens < MAX_TOKENS\n",
        "\n",
        "# # Batching and preprocessing wrapper\n",
        "# def make_batches(ds):\n",
        "#     \"\"\"\n",
        "#     Caches, shuffles, tokenizes, filters, pads, and batches the dataset.\n",
        "#     \"\"\"\n",
        "#     return (\n",
        "#         ds\n",
        "#         .cache()\n",
        "#         .shuffle(BUFFER_SIZE)\n",
        "#         .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "#         .filter(filter_max_tokens)\n",
        "#         .padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))  # pad manually here\n",
        "#         # .repeat() # NEW ADDITION\n",
        "#         .prefetch(tf.data.AUTOTUNE)\n",
        "#     )\n",
        "\n",
        "\n",
        "# Prepare the training and validation datasets by applying the make_batches function.\n",
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0ue2J-0aZjB",
        "outputId": "3e9b8fce-7f7d-4e95-ac99-b70bccf44136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Sentence 1 Examples:\n",
            "誰 恩 物 混ぜる\n",
            "rapaz tocar tecladomudo\n",
            "носорог пасся трава\n",
            "Mann spielen flöte\n",
            "persona stare preparare letto\n",
            "チンパンジー 女性 傷つける\n",
            "ктото режет тортилу\n",
            "jung spielen Gitarre\n",
            "femme ajout épice viande\n",
            "twee man dansen\n",
            "女性 レモン 絞る\n",
            "homme dun tshirt you tube gris\n",
            "chef limpar tigela sopa\n",
            "man sit chair stare\n",
            "食堂 男 女 話す\n",
            "człowiek umierać sala operacyjny\n",
            "femme coup citron\n",
            "man vissen\n",
            "señora bebé cubo basura\n",
            "shirtless man sit chair\n",
            "fußballspiel kicken Fußball Tor\n",
            "suonare pianoforte\n",
            "chłopak jechać trójkołowy rower\n",
            "女性 ボウル 卵 2 入れる 叩く\n",
            "persona stare affettare pomodoro\n",
            "Mann steigen Auto\n",
            "человек играть пианино\n",
            "カンガルー 抱く 女性\n",
            "男 腕立て 伏せ\n",
            "man play guitar\n",
            "человек чистить яблоко\n",
            "man liggen grond richten geweer\n",
            "----------------------------------------------\n",
            "> Sentence 2 Examples:\n",
            "quelquun train coudre chose\n",
            "男の子 ピアノ 弾く\n",
            "nosorożec pasie pole\n",
            "человек играть флейта\n",
            "persona comer mesa\n",
            "man rijden auto\n",
            "quelquun mont cheval\n",
            "hombre tocar guitarra\n",
            "женщина добавлять сахар мясо\n",
            "二人 女性 買い物\n",
            "Frau drücken Saft Zitrone\n",
            "homem mostrar camisa\n",
            "hombre limpiar borde tazón\n",
            "mały dziewczynka wkładać włos kucyk\n",
            "mężczyzna kobieta jechać rower\n",
            "hombre besar mujer escenario\n",
            "mężczyzna mówić mikrofon\n",
            "мужчина тренироваться\n",
            "vrouw stoppen baby vuilnisbak\n",
            "мужчина рубашка сидеть стул\n",
            "jugador fútbol marcar gol\n",
            "hombre tocar piano\n",
            "mulher andar scooter água\n",
            "person mix ingredient bowl\n",
            "człowiek kroi trochę mięso\n",
            "man car garage\n",
            "vrouw spelen gitaar\n",
            "femme train ramasser kangourou\n",
            "donna stare tagliare polpo\n",
            "человек играть гитара друг\n",
            "      男人             削苹果\n",
            "stare spalmare burro tortilla\n",
            "----------------------------------------------\n",
            "> Similarity Scores:\n",
            "[0.05   0.64   0.8    1.     0.04   0.     0.     0.64   0.55   0.08\n",
            " 0.88   0.55   0.5066 0.     0.24   0.08   0.     0.1    1.     0.76\n",
            " 0.76   0.8    0.25   0.6    0.35   0.7666 0.24   0.65   0.     0.64\n",
            " 1.     0.    ]\n",
            "----------------------------------------------\n",
            "> Token IDs (Sentence 1):\n",
            "[ 173 2309 8390 1741  999    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 128   11 3624    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[5397 5362  505 1816    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  6  29 412   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[  43    4 6520 2748    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[1795  598 3261 5096 8390   25 8587 8488 8541 1804 5162    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 138   46 5243 1817 1839 1023    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[142  29  53   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[  22 1168 5689   72  458    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 94   3 436   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[  25 1786 8589 8539 8516  287    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[   9  193 3580 1250 3578 7315    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[1149 2039 1053 6221    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[   3 1064 4316 8390 1537 8459    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[3050 4900 8390   10  286  447    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  23 3566 4081 2618 2697 1555 8479 8464 8468 8479    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  22  112 2153    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[   3 3543    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[1065  117 1143 7997    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[6261    3 1064 4316    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[2101  415 3669 8390 4038 4482 8390 4441    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 36 185   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[ 211   81 5981 3861 2692  518    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  25  403  594  353 1449  373    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 43   4  64 341   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[  6 902 308   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[ 12  27 164   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[ 541 3182  354    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  10 4631 4981    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 3 28 51  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n",
            "[  12  184 1808    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[   3 2041 1351 8390 6418  170 1353    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "----------------------------------------------\n",
            "> Token IDs (Sentence 2):\n",
            "[ 130   35 4269 1655    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[241 158  42   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[6790 1570 4083 1990    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 12  27 215   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[  43  104 1591    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  3  66 253   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[130 343 234   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[ 8 11 37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n",
            "[  20 1035 2410 1815  606 8390  419    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 225   25 8590 8536 8541 3308 2279    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  18 4222 3028 2827 2209    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[   7 2019 2165  789    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[   8 2038 4357  383    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 261 2123  550 1507 8390 2761 2907 8465    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 26  21  81 518   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[   8 7971   19  731    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  26 2016 6901    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  31 1219    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  17 2571   84 5829    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  31 5301  542 3341    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[7168 2824 6956 2088    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 8 11 40  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n",
            "[  15  299 1066  327    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  76 1093 2779  335    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 23  44 337 428   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "[   3  655 2096    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[17 30 52  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n",
            "[  22   35 2645  943    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  16    4   58 1989    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[  12   27 1495 3428    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[   5   34    2 2332    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[   4 1059  836  381    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ]
        }
      ],
      "source": [
        "# Take a small batch (e.g., 3) from the batched and tokenized dataset\n",
        "for sent1_batch, sent2_batch, score_batch in train_batches.take(1):\n",
        "    print(\"> Sentence 1 Examples:\")\n",
        "    for line in sent1_batch.numpy():\n",
        "        print(tokenizer.decode([token for token in line if token != 0]))\n",
        "\n",
        "    print(\"----------------------------------------------\")\n",
        "\n",
        "    print(\"> Sentence 2 Examples:\")\n",
        "    for line in sent2_batch.numpy():\n",
        "        print(tokenizer.decode([token for token in line if token != 0]))\n",
        "\n",
        "    print(\"----------------------------------------------\")\n",
        "\n",
        "    print(\"> Similarity Scores:\")\n",
        "    print(score_batch.numpy())\n",
        "\n",
        "    print(\"----------------------------------------------\")\n",
        "\n",
        "    print(\"> Token IDs (Sentence 1):\")\n",
        "    for line in sent1_batch.numpy():\n",
        "        print(line)\n",
        "\n",
        "    print(\"----------------------------------------------\")\n",
        "\n",
        "    print(\"> Token IDs (Sentence 2):\")\n",
        "    for line in sent2_batch.numpy():\n",
        "        print(line)\n",
        "\n",
        "# # Take a small batch (e.g., 3) from the dataset\n",
        "# for src_examples, tgt_examples in train_examples.batch(3).take(1):\n",
        "#     print(\"> Examples in Source Language:\")\n",
        "#     for line in src_examples.numpy():\n",
        "#         print(line.decode('utf-8'))\n",
        "\n",
        "# print(\"----------------------------------------------\")\n",
        "\n",
        "# print(\"> Examples in Target Language:\")\n",
        "# for line in tgt_examples.numpy():\n",
        "#     print(line.decode('utf-8'))\n",
        "\n",
        "# print(\"----------------------------------------------\")\n",
        "\n",
        "# # Tokenize the target examples using your custom tokenizer\n",
        "# encoded = [tokenizer.encode(text.numpy().decode('utf-8')) for text in tgt_examples]\n",
        "\n",
        "# # Print tokenized form\n",
        "# for row in encoded:\n",
        "#     print(row)\n",
        "\n",
        "# print(\"----------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "y8S8AdfOaZjC"
      },
      "outputs": [],
      "source": [
        "# POSITIONAL ENCODING\n",
        "# Transformers have no recurrence or convolution, so we inject sequence order using sine/cosine signals.\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    \"\"\"\n",
        "    Computes the angle rates for the positional encoding.\n",
        "    The formula ensures that each dimension of the embedding varies at a different wavelength (some change faster than others).\n",
        "\n",
        "    Parameters:\n",
        "    - pos: Position index.\n",
        "    - i: Dimension index.\n",
        "    - d_model: Depth of the model (number of dimensions).\n",
        "\n",
        "    Returns:\n",
        "    - The angle rates for positional encoding.\n",
        "    \"\"\"\n",
        "    # Calculate the angles based on position and dimension index.\n",
        "    # This formula helps in varying the wavelength across different dimensions.\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    \"\"\"\n",
        "    Generates a positional encoding matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - position: The maximum position index.\n",
        "    - d_model: The depth of the model (number of dimensions).\n",
        "\n",
        "    Returns:\n",
        "    - A positional encoding matrix of shape (1, position, d_model).\n",
        "    \"\"\"\n",
        "    # Generate angles based on positions and dimensions.\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    # Apply sine to even indices in the angles array (2i).\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    # Apply cosine to odd indices in the angles array (2i+1).\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    # Expand the dimensions to fit the model requirements.\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    # Cast the encoding to TensorFlow float32 type.\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "skZeSiVCaZjC"
      },
      "outputs": [],
      "source": [
        "# MASKING\n",
        "# Masks prevent attention to padding tokens or future tokens during decoding.\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    \"\"\"\n",
        "    Creates a padding mask for sequences.\n",
        "    This mask hides the padding tokens (i.e., zeros) so they don't affect the attention mechanism. It returns 1s where padding exists, and 0s elsewhere.\n",
        "    Parameters:\n",
        "    - seq: The sequence of tokens.\n",
        "\n",
        "    Returns:\n",
        "    - A padding mask for the sequence.\n",
        "    \"\"\"\n",
        "    # Create a mask where every zero in the sequence is marked with a 1 (padding) and others with a 0.\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    # Add extra dimensions to the mask so it can be added to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # Shape: (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    \"\"\"\n",
        "    Creates a look-ahead mask to mask future tokens in a sequence.\n",
        "    Each token can only see previous ones (or itself), but not the next ones, ensuring proper autoregressive behavior.\n",
        "    Parameters:\n",
        "    - size: Size of the mask.\n",
        "\n",
        "    Returns:\n",
        "    - A look-ahead mask of shape (size, size).\n",
        "    \"\"\"\n",
        "    # Create a mask where every entry that is in the lower triangle (including the diagonal) is 0, and everything else is 1.\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # Shape: (seq_len, seq_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5trxk8UqaZjC"
      },
      "outputs": [],
      "source": [
        "# SCALING AND DOT PRODUCT ATTENTION\n",
        "# This is the core building block of attention: compare queries with keys, weigh values\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Calculates the attention weights and applies them to the value vectors.\n",
        "\n",
        "    Parameters:\n",
        "    - q (query): Tensor with shape (..., seq_len_q, depth)\n",
        "    - k (key): Tensor with shape (..., seq_len_k, depth)\n",
        "    - v (value): Tensor with shape (..., seq_len_v, depth_v)\n",
        "    - mask: (Optional) Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k).\n",
        "\n",
        "    Returns:\n",
        "    - output: The result of applying attention weights to the value vectors.\n",
        "    - attention_weights: The attention weights.\n",
        "    \"\"\"\n",
        "    # Compute the dot product of the query and key tensors. Transpose the key tensor for proper alignment.\n",
        "    # This gives us a similarity score between each query and key.\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # Shape: (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # Scale the dot product by the square root of the depth of the key tensor.\n",
        "    # This helps in preventing the softmax function from having extremely small gradients.\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)  # Get the depth of the keys.\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # Apply the mask if provided. The mask is used to nullify the effect of padding or future information.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  # Use a large negative number to mask.\n",
        "\n",
        "    # Apply softmax to get the attention weights. The softmax is applied on the key sequence dimension.\n",
        "    # It shows how much attention each word pays to others.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # Shape: (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # Apply the attention weights to the value tensor to get the output.\n",
        "    output = tf.matmul(attention_weights, v)  # Shape: (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vNYgGLsAaZjC"
      },
      "outputs": [],
      "source": [
        "# MULTIHEAD ATTENTION\n",
        "# Instead of attending once, we split into multiple attention \"heads\" for richer representations\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, d_model, num_heads):\n",
        "        \"\"\"\n",
        "        Initialize the MultiHeadAttention layer.\n",
        "\n",
        "        Parameters:\n",
        "        - d_model: Dimensionality of the model's output space.\n",
        "        - num_heads: Number of attention heads.\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads  # Number of attention heads.\n",
        "        self.d_model = d_model  # Dimensionality of the model's output space.\n",
        "\n",
        "        # Ensure the model's dimension is divisible by the number of heads to evenly distribute dimensions to each head.\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads  # Dimensionality per attention head.\n",
        "\n",
        "        # Define dense layers for the queries, keys, and values.\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        # Final dense layer.\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (num_heads, depth) and transpose the result.\n",
        "\n",
        "        Parameters:\n",
        "        - x: Input tensor.\n",
        "        - batch_size: Size of the batch.\n",
        "\n",
        "        Returns:\n",
        "        - Tensor with shape (batch_size, num_heads, seq_len, depth).\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        \"\"\"\n",
        "        The logic for the multi-head attention layer's forward pass.\n",
        "\n",
        "        Parameters:\n",
        "        - v: Value tensor.\n",
        "        - k: Key tensor.\n",
        "        - q: Query tensor.\n",
        "        - mask: Mask to be applied.\n",
        "\n",
        "        Returns:\n",
        "        - output: Output tensor.\n",
        "        - attention_weights: Attention weights.\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # Apply dense layers to queries, keys, and values.\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Split the dense outputs into multiple heads and transpose.\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # Perform scaled dot product attention.\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        # Transpose and reshape the attention output to match the input's dimensionality.\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # Apply the final dense layer.\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hJTr89JLaZjC"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    \"\"\"\n",
        "    Creates a point-wise feed forward network. This consists of two dense layers with a ReLU activation\n",
        "    in between, which is used within each transformer block.\n",
        "\n",
        "    Parameters:\n",
        "    - d_model: The dimensionality of the input and output of the transformer model.\n",
        "    - dff: The dimensionality of the inner layer, typically much larger than d_model to allow\n",
        "           the model to combine features in the data in a high-dimensional space before projecting\n",
        "           back down to d_model dimensions.\n",
        "\n",
        "    Returns:\n",
        "    - A tf.keras.Sequential model representing the feed forward network.\n",
        "    \"\"\"\n",
        "    return tf.keras.Sequential([\n",
        "        # First dense layer with dff units and ReLU activation. This expands the dimensionality to dff,\n",
        "        # allowing the network to learn more complex features.\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # Output shape: (batch_size, seq_len, dff)\n",
        "\n",
        "        # Second dense layer that projects the outputs back down to d_model dimensions.\n",
        "        tf.keras.layers.Dense(d_model)  # Output shape: (batch_size, seq_len, d_model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypzFUBdGaZjC"
      },
      "source": [
        "## Encoder layer\n",
        "We're now implementing the core building block of the Transformer encoder — the EncoderLayer. This is one layer in the stacked encoder block of the Transformer architecture.\n",
        "\n",
        "High-Level Role of EncoderLayer is:\n",
        "- Each encoder layer transforms an input sequence into a richer representation, by letting each token attend to all others (via self-attention);\n",
        "- Then refining that with a feed-forward network;\n",
        "- You’ll stack multiple of these layers to form the full encoder (e.g., num_layers = 6).\n",
        "\n",
        "\n",
        "The EncoderLayer **combines**:\n",
        "\n",
        "* Multi-Head Self-Attention\n",
        "\n",
        "* Feed Forward Network\n",
        "\n",
        "* Layer Normalization\n",
        "\n",
        "* Dropout\n",
        "\n",
        "* Residual Connections\n",
        "\n",
        "Each EncoderLayer **learns** to:\n",
        "\n",
        "* Focus on different parts of the input through self-attention.\n",
        "\n",
        "* Extract features and relationships (such as distances) between words.\n",
        "\n",
        "* Stabilize training using layer normalization and residual connections.*κείμενο σε πλάγια γραφή*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kNkcbiO6aZjC"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, d_model, num_heads, dff, rate=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the EncoderLayer with multi-head attention, point-wise feed-forward network,\n",
        "        dropout, and layer normalization components.\n",
        "\n",
        "        Parameters:\n",
        "        - d_model: Dimensionality of the model's output space.\n",
        "        - num_heads: Number of attention heads.\n",
        "        - dff: Dimensionality of the feed-forward network's inner layer.\n",
        "        - rate: Dropout rate.\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)  # Multi-head attention layer.\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)  # Point-wise feed-forward network.\n",
        "\n",
        "        # Layer normalization (first instance).\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        # Layer normalization (second instance).\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout (first instance).\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        # Dropout (second instance).\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        \"\"\"\n",
        "        The logic for one pass of the encoder layer.\n",
        "\n",
        "        Parameters:\n",
        "        - x: Input tensor.\n",
        "        - training: Boolean indicating if the layer should behave in training mode (applying dropout) or in inference mode.\n",
        "        - mask: Mask to be applied on the multi-head attention layer.\n",
        "\n",
        "        Returns:\n",
        "        - The output tensor of the encoder layer.\n",
        "        \"\"\"\n",
        "        # Apply multi-head attention to the input (self attention).\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # Output shape: (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)  # Apply dropout to the attention output.\n",
        "\n",
        "        # Add & normalize.\n",
        "        out1 = self.layernorm1(x + attn_output)  # Residual connection followed by layer normalization.\n",
        "\n",
        "        # Apply the feed-forward network to the normalized attention output.\n",
        "        ffn_output = self.ffn(out1)  # Output shape: (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)  # Apply dropout to the feed-forward network output.\n",
        "\n",
        "        # Final add & normalize step.\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection followed by another layer normalization.\n",
        "\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RdhAi0saZjD"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "h3NKmSxiaZjD"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the Encoder part of the Transformer.\n",
        "\n",
        "        Parameters:\n",
        "        - num_layers: Number of encoder layers.\n",
        "        - d_model: Dimensionality of the model's output space.\n",
        "        - num_heads: Number of attention heads.\n",
        "        - dff: Dimensionality of the feed-forward network's inner layer.\n",
        "        - input_vocab_size: Size of the input vocabulary.\n",
        "        - rate: Dropout rate.\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer for the input tokens.\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding up to MAX_TOKENS.\n",
        "        self.pos_encoding = positional_encoding(MAX_TOKENS, self.d_model)\n",
        "\n",
        "        # Encoder layers\n",
        "        self.enc_layers = [EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate) for _ in range(num_layers)]\n",
        "\n",
        "        # Dropout layer.\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # Adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # Scale embeddings.\n",
        "        x += self.pos_encoding[:, :seq_len, :]  # Add position encoding.\n",
        "        x = self.dropout(x, training=training)  # Apply dropout.\n",
        "\n",
        "        # Pass the input through each encoder layer.\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeoKBSMTaZjD"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SEdGIij7aZjD"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, *, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the Transformer model for sentence similarity.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, input_vocab_size=input_vocab_size, rate=rate)\n",
        "\n",
        "        # # Final linear layer that projects the decoder's output to the target vocabulary size.\n",
        "        # self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        \"\"\"\n",
        "        The logic for one forward pass through the model.\n",
        "\n",
        "        Parameters:\n",
        "        - inputs: A tuple of input tensor and target tensor.\n",
        "        - training: Boolean indicating if the layer should behave in training mode or inference mode.\n",
        "\n",
        "        Returns:\n",
        "        - sentence_embedding: A fixed-size vector representation (e.g., mean-pooled encoder output).\n",
        "        \"\"\"\n",
        "        # Create encoder padding mask\n",
        "        enc_padding_mask = create_padding_mask(inputs)\n",
        "\n",
        "        # Run encoder\n",
        "        enc_output = self.encoder(inputs, training=training, mask=enc_padding_mask)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Reduce to fixed-size vector (e.g., mean pooling)\n",
        "        sentence_embedding = tf.reduce_mean(enc_output, axis=1)  # (batch_size, d_model)\n",
        "        return sentence_embedding\n",
        "\n",
        "        # inp, tar = inputs\n",
        "\n",
        "        # # Create masks for padding and future tokens.\n",
        "        # enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
        "\n",
        "        # # Pass the input through the encoder.\n",
        "        # #enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "        # #self.encoder(inp, training=training, mask=enc_padding_mask)\n",
        "        # enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
        "\n",
        "        # # Pass the encoder output and target through the decoder.\n",
        "        # #dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        # #self.decoder(tar, enc_output, training=training,\n",
        "        #  #    look_ahead_mask=look_ahead_mask,\n",
        "        #   #   padding_mask=dec_padding_mask)\n",
        "\n",
        "        # dec_output, attention_weights = self.decoder(\n",
        "        #       tar,\n",
        "        #       enc_output,\n",
        "        #       training=training,\n",
        "        #       look_ahead_mask=look_ahead_mask,\n",
        "        #       padding_mask=dec_padding_mask)\n",
        "\n",
        "\n",
        "        # # Pass the decoder output through the final linear layer.\n",
        "        # final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        # return final_output, attention_weights\n",
        "\n",
        "\n",
        "    # def create_masks(self, inp, tar):\n",
        "    #     \"\"\"\n",
        "    #     Creates masks for padding and look ahead for the encoder and decoder.\n",
        "\n",
        "    #     Parameters:\n",
        "    #     - inp: Input tensor.\n",
        "    #     - tar: Target tensor.\n",
        "\n",
        "    #     Returns:\n",
        "    #     - enc_padding_mask: Padding mask for the encoder.\n",
        "    #     - look_ahead_mask: Look-ahead mask for the decoder.\n",
        "    #     - dec_padding_mask: Padding mask for the decoder to mask the encoder outputs.\n",
        "    #     \"\"\"\n",
        "    #     # Encoder padding mask.\n",
        "    #     enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    #     # Decoder padding mask for the second attention block (to mask encoder outputs).\n",
        "    #     dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    #     # Look-ahead mask (to mask future tokens) and decoder target padding mask combined.\n",
        "    #     look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    #     dec_target_padding_mask = create_padding_mask(tar)\n",
        "    #     look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    #     return enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
        "\n",
        "# -------------------------------\n",
        "# Transformer Model Hyperparameters\n",
        "# -------------------------------\n",
        "\n",
        "num_layers = 4 # number of encoder/decoder layers in the Transformer\n",
        "d_model = 128 # size of the embedding vector for each word\n",
        "dff = 512 # size of the hidden layer inside the Feed Forward Neural Network\n",
        "num_heads = 8 # number of attention heads (must divide d_model evenly)\n",
        "dropout_rate = 0.1 # dropout rate for regularization\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, tf.float32)  # Model dimensionality, cast to float32 for calculation.\n",
        "        self.warmup_steps = warmup_steps  # Number of steps to linearly increase the learning rate.\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)  # Linearly increase then decrease based on warmup steps.\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)  # Calculate the learning rate.\n",
        "        # Final learning rate: (1 / sqrt(d_model)) * min(arg1, arg2)\n",
        "\n",
        "# Instantiate the learning rate schedule and Adam optimizer\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate,\n",
        "    beta_1=0.9,         # First moment decay (default)\n",
        "    beta_2=0.98,        # Second moment decay (used in the original Transformer paper)\n",
        "    epsilon=1e-9        # Small value to avoid division by zero\n",
        ")\n",
        "\n",
        "# def loss_function(real, pred):\n",
        "#     mask = tf.math.logical_not(tf.math.equal(real, 0))  # Create a mask for non-zero tokens.\n",
        "#     loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "#     loss_ = loss_object(real, pred)  # Calculate loss using some loss object not defined here.\n",
        "#     mask = tf.cast(mask, dtype=loss_.dtype)  # Cast mask to the same type as the loss.\n",
        "#     loss_ *= mask  # Apply mask to the loss.\n",
        "#     return tf.reduce_sum(loss_) / tf.reduce_sum(mask)  # Calculate the average loss.\n",
        "\n",
        "# def accuracy_function(real, pred):\n",
        "#     accuracies = tf.equal(real, tf.argmax(pred, axis=2))  # Check if real values match predictions.\n",
        "#     mask = tf.math.logical_not(tf.math.equal(real, 0))  # Create a mask for non-zero tokens.\n",
        "#     accuracies = tf.math.logical_and(mask, accuracies)  # Apply mask to accuracies.\n",
        "#     accuracies = tf.cast(accuracies, dtype=tf.float32)  # Cast to float32 for calculation.\n",
        "#     mask = tf.cast(mask, dtype=tf.float32)  # Cast mask to float32.\n",
        "#     return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)  # Calculate the average accuracy.\n",
        "\n",
        "# train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "# train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6EqAgEIaZjH"
      },
      "source": [
        "# Similiarity Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oEp9QrDnaZjH"
      },
      "outputs": [],
      "source": [
        "class SimilarityModel(tf.keras.Model):\n",
        "    def __init__(self, transformer):\n",
        "        super().__init__()\n",
        "        self.transformer = transformer\n",
        "        self.dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')  # Output similarity score between 0 and 1\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        sent1, sent2 = inputs  # each: (batch_size, seq_len)\n",
        "\n",
        "        embed1 = self.transformer(sent1, training=training)  # (batch_size, d_model)\n",
        "        embed2 = self.transformer(sent2, training=training)  # (batch_size, d_model)\n",
        "\n",
        "        # Combine embeddings (common in sentence similarity tasks)\n",
        "        combined = tf.concat([\n",
        "            embed1,\n",
        "            embed2,\n",
        "            tf.abs(embed1 - embed2),\n",
        "            embed1 * embed2\n",
        "        ], axis=1)  # (batch_size, 4 * d_model)\n",
        "\n",
        "        return self.dense(combined)  # (batch_size, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tbOkICRnaZjH"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Instantiate the Transformer model\n",
        "# -------------------------------\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,                        # Number of encoder and decoder layers\n",
        "    d_model=d_model,                              # Embedding size / model dimensionality\n",
        "    num_heads=num_heads,                          # Number of attention heads\n",
        "    dff=dff,                                      # Hidden layer size in feed-forward network\n",
        "    input_vocab_size = tokenizer.vocab_size,\n",
        "    # target_vocab_size = target_tokenizer.vocab_size,\n",
        "    rate=dropout_rate                             # Dropout rate\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Checkpointing: Saving and restoring model state\n",
        "# -------------------------------\n",
        "\n",
        "checkpoint_path = './checkpoints/train'   # Directory to save training checkpoints\n",
        "\n",
        "# Create a checkpoint object that tracks the transformer and optimizer state\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "# Manage multiple checkpoints (e.g., keep the 5 latest ones)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# If a previous checkpoint exists, restore the model and optimizer state\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print('Latest checkpoint restored!!')\n",
        "\n",
        "# -------------------------------\n",
        "# Define input signature for tf.function (for performance optimization)\n",
        "# Used to decorate the training step function later\n",
        "# -------------------------------\n",
        "# train_step_signature = [\n",
        "#     tf.TensorSpec(shape=(None, None), dtype=tf.int64),  # Input sequence shape: (batch_size, input_seq_len)\n",
        "#     tf.TensorSpec(shape=(None, None), dtype=tf.int64),  # Target sequence shape: (batch_size, target_seq_len)\n",
        "# ]\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),  # sentence1\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),  # sentence2\n",
        "    tf.TensorSpec(shape=(None,), dtype=tf.float32),     # similarity label (e.g., 0.0 to 1.0)\n",
        "]\n",
        "\n",
        "# -------------------------------\n",
        "# Instantiate the Sentence similarity model\n",
        "# -------------------------------\n",
        "similarity_model = SimilarityModel(transformer)\n",
        "\n",
        "# Initiate loss function and metrics\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2K698bkaZjH"
      },
      "source": [
        "## Baseline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgsU215FaZjH",
        "outputId": "1dc6ea28-497e-4d9d-e31c-3753d05d1753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inspecting batch structure...\n",
            "Raw output of next(train_iter): <class 'tuple'>\n",
            "Length of output: 3\n",
            "\n",
            "Element 0:\n",
            "  Type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "  Shape: (32, 128)\n",
            "  DType: <dtype: 'int64'>\n",
            "  Example values: [[ 173 2309 8390 1741  999    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Element 1:\n",
            "  Type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "  Shape: (32, 128)\n",
            "  DType: <dtype: 'int64'>\n",
            "  Example values: [[ 130   35 4269 1655    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Element 2:\n",
            "  Type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "  Shape: (32,)\n",
            "  DType: <dtype: 'float32'>\n",
            "  Example values: [0.05]\n"
          ]
        }
      ],
      "source": [
        "# DEBUG: Peek at one batch from train_batches\n",
        "print(\"Inspecting batch structure...\")\n",
        "train_batches = make_batches(train_examples)\n",
        "train_iter = iter(train_batches)\n",
        "\n",
        "first = next(train_iter)\n",
        "\n",
        "print(\"Raw output of next(train_iter):\", type(first))\n",
        "print(\"Length of output:\", len(first))\n",
        "\n",
        "# Try printing all contents in the batch\n",
        "for i, item in enumerate(first):\n",
        "    print(f\"\\nElement {i}:\")\n",
        "    print(\"  Type:\", type(item))\n",
        "    print(\"  Shape:\", item.shape)\n",
        "    print(\"  DType:\", item.dtype)\n",
        "    print(\"  Example values:\", item.numpy()[:1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1TPcuw1aZjH",
        "outputId": "747d31e7-d390-4839-cc1b-790cc0c52334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 -------------------\n",
            "Epoch 1 Step 0 Loss 0.2150\n",
            "Epoch 1 Step 50 Loss 0.1204\n",
            "Epoch 1 Step 100 Loss 0.1146\n",
            "Epoch 1 Step 150 Loss 0.1114\n",
            "Epoch 1 Step 200 Loss nan\n",
            "Epoch 1 Step 250 Loss nan\n",
            "Epoch 1 Step 300 Loss nan\n",
            "Epoch 1 Step 350 Loss nan\n",
            "Epoch 1 Step 400 Loss nan\n",
            "Epoch 1 Step 450 Loss nan\n",
            "Epoch 1 Step 500 Loss nan\n",
            "Epoch 1 Step 550 Loss nan\n",
            "Epoch 1 Step 600 Loss nan\n",
            "Epoch 1 Step 650 Loss nan\n",
            "Epoch 1 Step 700 Loss nan\n",
            "Epoch 1 Step 750 Loss nan\n",
            "Epoch 1 Step 800 Loss nan\n",
            "Epoch 1 Step 850 Loss nan\n",
            "Epoch 1 Step 900 Loss nan\n",
            "Epoch 1 Step 950 Loss nan\n",
            "Epoch 1 Step 1000 Loss nan\n",
            "Epoch 1 Step 1050 Loss nan\n",
            "Epoch 1 Step 1100 Loss nan\n",
            "Epoch 1 Step 1150 Loss nan\n",
            "Epoch 1 Step 1200 Loss nan\n",
            "Epoch 1 Step 1250 Loss nan\n",
            "Epoch 1 Step 1300 Loss nan\n",
            "Epoch 1 Step 1350 Loss nan\n",
            "Epoch 1 Step 1400 Loss nan\n",
            "Epoch 1 Step 1450 Loss nan\n",
            "Epoch 1 Step 1500 Loss nan\n",
            "Epoch 1 Step 1550 Loss nan\n",
            "Epoch 1 Step 1600 Loss nan\n",
            "Epoch 1 Step 1650 Loss nan\n",
            "Epoch 1 Step 1700 Loss nan\n",
            "Epoch 1 Step 1750 Loss nan\n",
            "Epoch 1 Step 1800 Loss nan\n",
            "Epoch 1 Step 1850 Loss nan\n",
            "Epoch 1 Step 1900 Loss nan\n",
            "Epoch 1 Step 1950 Loss nan\n",
            "Epoch 1 Step 2000 Loss nan\n",
            "Epoch 1 Step 2050 Loss nan\n",
            "Epoch 1 Step 2100 Loss nan\n",
            "Epoch 1 Step 2150 Loss nan\n",
            "Epoch 1 Step 2200 Loss nan\n",
            "Epoch 1 Step 2250 Loss nan\n",
            "Epoch 1 Step 2300 Loss nan\n",
            "Epoch 1 Step 2350 Loss nan\n",
            "Epoch 1 Step 2400 Loss nan\n",
            "Epoch 1 Step 2450 Loss nan\n",
            "Epoch 1 Step 2500 Loss nan\n",
            "Epoch 1 Step 2550 Loss nan\n",
            "Epoch 1 Step 2600 Loss nan\n",
            "Epoch 1 Step 2650 Loss nan\n",
            "Epoch 1 Step 2700 Loss nan\n",
            "Epoch 1 Step 2750 Loss nan\n",
            "Epoch 1 Step 2800 Loss nan\n",
            "Epoch 1 Step 2850 Loss nan\n",
            "Epoch 1 Step 2900 Loss nan\n",
            "Epoch 1 Step 2950 Loss nan\n",
            "Epoch 1 Step 3000 Loss nan\n",
            "Epoch 1 Step 3050 Loss nan\n",
            "Epoch 1 Step 3100 Loss nan\n",
            "Epoch 1 Step 3150 Loss nan\n",
            "Epoch 1 Step 3200 Loss nan\n",
            "Epoch 1 Step 3250 Loss nan\n",
            "Epoch 1 Step 3300 Loss nan\n",
            "Epoch 1 Step 3350 Loss nan\n",
            "Epoch 1 Step 3400 Loss nan\n",
            "Epoch 1 Step 3450 Loss nan\n",
            "Epoch 1 Step 3500 Loss nan\n",
            "Epoch 1 Step 3550 Loss nan\n",
            "Epoch 1 Step 3600 Loss nan\n",
            "Epoch 1 Step 3650 Loss nan\n",
            "Epoch 1 Step 3700 Loss nan\n",
            "Epoch 1 Loss nan\n",
            "Time taken for 1 epoch: 1524.70 secs\n",
            "\n",
            "\n",
            "Epoch 2 -------------------\n",
            "Epoch 2 Step 0 Loss 0.0000\n",
            "Epoch 2 Step 50 Loss 0.0000\n",
            "Epoch 2 Step 100 Loss 0.0000\n",
            "Epoch 2 Step 150 Loss 0.0000\n",
            "Epoch 2 Step 200 Loss 0.0000\n",
            "Epoch 2 Step 250 Loss 0.0000\n",
            "Epoch 2 Step 300 Loss 0.0000\n",
            "Epoch 2 Step 350 Loss 0.0000\n",
            "Epoch 2 Step 400 Loss 0.0000\n",
            "Epoch 2 Step 450 Loss 0.0000\n",
            "Epoch 2 Step 500 Loss 0.0000\n",
            "Epoch 2 Step 550 Loss 0.0000\n",
            "Epoch 2 Step 600 Loss 0.0000\n",
            "Epoch 2 Step 650 Loss 0.0000\n",
            "Epoch 2 Step 700 Loss 0.0000\n",
            "Epoch 2 Step 750 Loss 0.0000\n",
            "Epoch 2 Step 800 Loss 0.0000\n",
            "Epoch 2 Step 850 Loss 0.0000\n",
            "Epoch 2 Step 900 Loss 0.0000\n",
            "Epoch 2 Step 950 Loss 0.0000\n",
            "Epoch 2 Step 1000 Loss 0.0000\n",
            "Epoch 2 Step 1050 Loss 0.0000\n",
            "Epoch 2 Step 1100 Loss 0.0000\n",
            "Epoch 2 Step 1150 Loss 0.0000\n",
            "Epoch 2 Step 1200 Loss 0.0000\n",
            "Epoch 2 Step 1250 Loss 0.0000\n",
            "Epoch 2 Step 1300 Loss 0.0000\n",
            "Epoch 2 Step 1350 Loss 0.0000\n",
            "Epoch 2 Step 1400 Loss 0.0000\n",
            "Epoch 2 Step 1450 Loss 0.0000\n",
            "Epoch 2 Step 1500 Loss 0.0000\n",
            "Epoch 2 Step 1550 Loss 0.0000\n",
            "Epoch 2 Step 1600 Loss 0.0000\n",
            "Epoch 2 Step 1650 Loss 0.0000\n",
            "Epoch 2 Step 1700 Loss 0.0000\n",
            "Epoch 2 Step 1750 Loss 0.0000\n",
            "Epoch 2 Step 1800 Loss 0.0000\n",
            "Epoch 2 Step 1850 Loss 0.0000\n",
            "Epoch 2 Step 1900 Loss 0.0000\n",
            "Epoch 2 Step 1950 Loss 0.0000\n",
            "Epoch 2 Step 2000 Loss 0.0000\n",
            "Epoch 2 Step 2050 Loss 0.0000\n",
            "Epoch 2 Step 2100 Loss 0.0000\n",
            "Epoch 2 Step 2150 Loss 0.0000\n",
            "Epoch 2 Step 2200 Loss 0.0000\n",
            "Epoch 2 Step 2250 Loss 0.0000\n",
            "Epoch 2 Step 2300 Loss 0.0000\n",
            "Epoch 2 Step 2350 Loss 0.0000\n",
            "Epoch 2 Step 2400 Loss 0.0000\n",
            "Epoch 2 Step 2450 Loss 0.0000\n",
            "Epoch 2 Step 2500 Loss 0.0000\n",
            "Epoch 2 Step 2550 Loss 0.0000\n",
            "Epoch 2 Step 2600 Loss 0.0000\n",
            "Epoch 2 Step 2650 Loss 0.0000\n",
            "Epoch 2 Step 2700 Loss 0.0000\n",
            "Epoch 2 Step 2750 Loss 0.0000\n",
            "Epoch 2 Step 2800 Loss 0.0000\n",
            "Epoch 2 Step 2850 Loss 0.0000\n",
            "Epoch 2 Step 2900 Loss 0.0000\n",
            "Epoch 2 Step 2950 Loss 0.0000\n",
            "Epoch 2 Step 3000 Loss 0.0000\n",
            "Epoch 2 Step 3050 Loss 0.0000\n",
            "Epoch 2 Step 3100 Loss 0.0000\n",
            "Epoch 2 Step 3150 Loss 0.0000\n",
            "Epoch 2 Step 3200 Loss 0.0000\n",
            "Epoch 2 Step 3250 Loss 0.0000\n",
            "Epoch 2 Step 3300 Loss 0.0000\n",
            "Epoch 2 Step 3350 Loss 0.0000\n",
            "Epoch 2 Step 3400 Loss 0.0000\n",
            "Epoch 2 Step 3450 Loss 0.0000\n",
            "Epoch 2 Step 3500 Loss 0.0000\n",
            "Epoch 2 Step 3550 Loss 0.0000\n",
            "Epoch 2 Step 3600 Loss 0.0000\n",
            "Epoch 2 Step 3650 Loss 0.0000\n",
            "Epoch 2 Step 3700 Loss 0.0000\n",
            "Epoch 2 Loss 0.0000\n",
            "Time taken for 1 epoch: 1506.66 secs\n",
            "\n",
            "\n",
            "Epoch 3 -------------------\n",
            "Epoch 3 Step 0 Loss 0.0000\n",
            "Epoch 3 Step 50 Loss 0.0000\n",
            "Epoch 3 Step 100 Loss 0.0000\n",
            "Epoch 3 Step 150 Loss 0.0000\n",
            "Epoch 3 Step 200 Loss 0.0000\n",
            "Epoch 3 Step 250 Loss 0.0000\n",
            "Epoch 3 Step 300 Loss 0.0000\n",
            "Epoch 3 Step 350 Loss 0.0000\n",
            "Epoch 3 Step 400 Loss 0.0000\n",
            "Epoch 3 Step 450 Loss 0.0000\n",
            "Epoch 3 Step 500 Loss 0.0000\n",
            "Epoch 3 Step 550 Loss 0.0000\n",
            "Epoch 3 Step 600 Loss 0.0000\n",
            "Epoch 3 Step 650 Loss 0.0000\n",
            "Epoch 3 Step 700 Loss 0.0000\n",
            "Epoch 3 Step 750 Loss 0.0000\n",
            "Epoch 3 Step 800 Loss 0.0000\n",
            "Epoch 3 Step 850 Loss 0.0000\n",
            "Epoch 3 Step 900 Loss 0.0000\n",
            "Epoch 3 Step 950 Loss 0.0000\n",
            "Epoch 3 Step 1000 Loss 0.0000\n",
            "Epoch 3 Step 1050 Loss 0.0000\n",
            "Epoch 3 Step 1100 Loss 0.0000\n",
            "Epoch 3 Step 1150 Loss 0.0000\n",
            "Epoch 3 Step 1200 Loss 0.0000\n",
            "Epoch 3 Step 1250 Loss 0.0000\n",
            "Epoch 3 Step 1300 Loss 0.0000\n",
            "Epoch 3 Step 1350 Loss 0.0000\n",
            "Epoch 3 Step 1400 Loss 0.0000\n",
            "Epoch 3 Step 1450 Loss 0.0000\n",
            "Epoch 3 Step 1500 Loss 0.0000\n",
            "Epoch 3 Step 1550 Loss 0.0000\n",
            "Epoch 3 Step 1600 Loss 0.0000\n",
            "Epoch 3 Step 1650 Loss 0.0000\n",
            "Epoch 3 Step 1700 Loss 0.0000\n",
            "Epoch 3 Step 1750 Loss 0.0000\n",
            "Epoch 3 Step 1800 Loss 0.0000\n",
            "Epoch 3 Step 1850 Loss 0.0000\n",
            "Epoch 3 Step 1900 Loss 0.0000\n",
            "Epoch 3 Step 1950 Loss 0.0000\n",
            "Epoch 3 Step 2000 Loss 0.0000\n",
            "Epoch 3 Step 2050 Loss 0.0000\n",
            "Epoch 3 Step 2100 Loss 0.0000\n",
            "Epoch 3 Step 2150 Loss 0.0000\n",
            "Epoch 3 Step 2200 Loss 0.0000\n",
            "Epoch 3 Step 2250 Loss 0.0000\n",
            "Epoch 3 Step 2300 Loss 0.0000\n",
            "Epoch 3 Step 2350 Loss 0.0000\n",
            "Epoch 3 Step 2400 Loss 0.0000\n",
            "Epoch 3 Step 2450 Loss 0.0000\n",
            "Epoch 3 Step 2500 Loss 0.0000\n",
            "Epoch 3 Step 2550 Loss 0.0000\n",
            "Epoch 3 Step 2600 Loss 0.0000\n",
            "Epoch 3 Step 2650 Loss 0.0000\n",
            "Epoch 3 Step 2700 Loss 0.0000\n",
            "Epoch 3 Step 2750 Loss 0.0000\n",
            "Epoch 3 Step 2800 Loss 0.0000\n",
            "Epoch 3 Step 2850 Loss 0.0000\n",
            "Epoch 3 Step 2900 Loss 0.0000\n",
            "Epoch 3 Step 2950 Loss 0.0000\n",
            "Epoch 3 Step 3000 Loss 0.0000\n",
            "Epoch 3 Step 3050 Loss 0.0000\n",
            "Epoch 3 Step 3100 Loss 0.0000\n",
            "Epoch 3 Step 3150 Loss 0.0000\n",
            "Epoch 3 Step 3200 Loss 0.0000\n",
            "Epoch 3 Step 3250 Loss 0.0000\n",
            "Epoch 3 Step 3300 Loss 0.0000\n",
            "Epoch 3 Step 3350 Loss 0.0000\n",
            "Epoch 3 Step 3400 Loss 0.0000\n",
            "Epoch 3 Step 3450 Loss 0.0000\n",
            "Epoch 3 Step 3500 Loss 0.0000\n",
            "Epoch 3 Step 3550 Loss 0.0000\n",
            "Epoch 3 Step 3600 Loss 0.0000\n",
            "Epoch 3 Step 3650 Loss 0.0000\n",
            "Epoch 3 Step 3700 Loss 0.0000\n",
            "Saving checkpoint for epoch 3 at ./checkpoints/train/ckpt-1\n",
            "Epoch 3 Loss 0.0000\n",
            "Time taken for 1 epoch: 1504.84 secs\n",
            "\n",
            "\n",
            "Epoch 4 -------------------\n",
            "Epoch 4 Step 0 Loss 0.0000\n",
            "Epoch 4 Step 50 Loss 0.0000\n",
            "Epoch 4 Step 100 Loss 0.0000\n",
            "Epoch 4 Step 150 Loss 0.0000\n",
            "Epoch 4 Step 200 Loss 0.0000\n",
            "Epoch 4 Step 250 Loss 0.0000\n",
            "Epoch 4 Step 300 Loss 0.0000\n",
            "Epoch 4 Step 350 Loss 0.0000\n",
            "Epoch 4 Step 400 Loss 0.0000\n",
            "Epoch 4 Step 450 Loss 0.0000\n",
            "Epoch 4 Step 500 Loss 0.0000\n",
            "Epoch 4 Step 550 Loss 0.0000\n",
            "Epoch 4 Step 600 Loss 0.0000\n",
            "Epoch 4 Step 650 Loss 0.0000\n",
            "Epoch 4 Step 700 Loss 0.0000\n",
            "Epoch 4 Step 750 Loss 0.0000\n",
            "Epoch 4 Step 800 Loss 0.0000\n",
            "Epoch 4 Step 850 Loss 0.0000\n",
            "Epoch 4 Step 900 Loss 0.0000\n",
            "Epoch 4 Step 950 Loss 0.0000\n",
            "Epoch 4 Step 1000 Loss 0.0000\n",
            "Epoch 4 Step 1050 Loss 0.0000\n",
            "Epoch 4 Step 1100 Loss 0.0000\n",
            "Epoch 4 Step 1150 Loss 0.0000\n",
            "Epoch 4 Step 1200 Loss 0.0000\n",
            "Epoch 4 Step 1250 Loss 0.0000\n",
            "Epoch 4 Step 1300 Loss 0.0000\n",
            "Epoch 4 Step 1350 Loss 0.0000\n",
            "Epoch 4 Step 1400 Loss 0.0000\n",
            "Epoch 4 Step 1450 Loss 0.0000\n",
            "Epoch 4 Step 1500 Loss 0.0000\n",
            "Epoch 4 Step 1550 Loss 0.0000\n",
            "Epoch 4 Step 1600 Loss 0.0000\n",
            "Epoch 4 Step 1650 Loss 0.0000\n",
            "Epoch 4 Step 1700 Loss 0.0000\n",
            "Epoch 4 Step 1750 Loss 0.0000\n",
            "Epoch 4 Step 1800 Loss 0.0000\n",
            "Epoch 4 Step 1850 Loss 0.0000\n",
            "Epoch 4 Step 1900 Loss 0.0000\n",
            "Epoch 4 Step 1950 Loss 0.0000\n",
            "Epoch 4 Step 2000 Loss 0.0000\n",
            "Epoch 4 Step 2050 Loss 0.0000\n",
            "Epoch 4 Step 2100 Loss 0.0000\n",
            "Epoch 4 Step 2150 Loss 0.0000\n",
            "Epoch 4 Step 2200 Loss 0.0000\n",
            "Epoch 4 Step 2250 Loss 0.0000\n",
            "Epoch 4 Step 2300 Loss 0.0000\n",
            "Epoch 4 Step 2350 Loss 0.0000\n",
            "Epoch 4 Step 2400 Loss 0.0000\n",
            "Epoch 4 Step 2450 Loss 0.0000\n",
            "Epoch 4 Step 2500 Loss 0.0000\n",
            "Epoch 4 Step 2550 Loss 0.0000\n",
            "Epoch 4 Step 2600 Loss 0.0000\n",
            "Epoch 4 Step 2650 Loss 0.0000\n",
            "Epoch 4 Step 2700 Loss 0.0000\n",
            "Epoch 4 Step 2750 Loss 0.0000\n",
            "Epoch 4 Step 2800 Loss 0.0000\n",
            "Epoch 4 Step 2850 Loss 0.0000\n",
            "Epoch 4 Step 2900 Loss 0.0000\n",
            "Epoch 4 Step 2950 Loss 0.0000\n",
            "Epoch 4 Step 3000 Loss 0.0000\n",
            "Epoch 4 Step 3050 Loss 0.0000\n",
            "Epoch 4 Step 3100 Loss 0.0000\n",
            "Epoch 4 Step 3150 Loss 0.0000\n",
            "Epoch 4 Step 3200 Loss 0.0000\n",
            "Epoch 4 Step 3250 Loss 0.0000\n",
            "Epoch 4 Step 3300 Loss 0.0000\n",
            "Epoch 4 Step 3350 Loss 0.0000\n",
            "Epoch 4 Step 3400 Loss 0.0000\n",
            "Epoch 4 Step 3450 Loss 0.0000\n",
            "Epoch 4 Step 3500 Loss 0.0000\n",
            "Epoch 4 Step 3550 Loss 0.0000\n",
            "Epoch 4 Step 3600 Loss 0.0000\n",
            "Epoch 4 Step 3650 Loss 0.0000\n",
            "Epoch 4 Step 3700 Loss 0.0000\n",
            "Epoch 4 Loss 0.0000\n",
            "Time taken for 1 epoch: 1497.86 secs\n",
            "\n",
            "\n",
            "Epoch 5 -------------------\n",
            "Epoch 5 Step 0 Loss 0.0000\n",
            "Epoch 5 Step 50 Loss 0.0000\n",
            "Epoch 5 Step 100 Loss 0.0000\n",
            "Epoch 5 Step 150 Loss 0.0000\n",
            "Epoch 5 Step 200 Loss 0.0000\n",
            "Epoch 5 Step 250 Loss 0.0000\n",
            "Epoch 5 Step 300 Loss 0.0000\n",
            "Epoch 5 Step 350 Loss 0.0000\n",
            "Epoch 5 Step 400 Loss 0.0000\n",
            "Epoch 5 Step 450 Loss 0.0000\n",
            "Epoch 5 Step 500 Loss 0.0000\n",
            "Epoch 5 Step 550 Loss 0.0000\n",
            "Epoch 5 Step 600 Loss 0.0000\n",
            "Epoch 5 Step 650 Loss 0.0000\n",
            "Epoch 5 Step 700 Loss 0.0000\n",
            "Epoch 5 Step 750 Loss 0.0000\n",
            "Epoch 5 Step 800 Loss 0.0000\n",
            "Epoch 5 Step 850 Loss 0.0000\n",
            "Epoch 5 Step 900 Loss 0.0000\n",
            "Epoch 5 Step 950 Loss 0.0000\n",
            "Epoch 5 Step 1000 Loss 0.0000\n",
            "Epoch 5 Step 1050 Loss 0.0000\n",
            "Epoch 5 Step 1100 Loss 0.0000\n",
            "Epoch 5 Step 1150 Loss 0.0000\n",
            "Epoch 5 Step 1200 Loss 0.0000\n",
            "Epoch 5 Step 1250 Loss 0.0000\n",
            "Epoch 5 Step 1300 Loss 0.0000\n",
            "Epoch 5 Step 1350 Loss 0.0000\n",
            "Epoch 5 Step 1400 Loss 0.0000\n",
            "Epoch 5 Step 1450 Loss 0.0000\n",
            "Epoch 5 Step 1500 Loss 0.0000\n",
            "Epoch 5 Step 1550 Loss 0.0000\n",
            "Epoch 5 Step 1600 Loss 0.0000\n",
            "Epoch 5 Step 1650 Loss 0.0000\n",
            "Epoch 5 Step 1700 Loss 0.0000\n",
            "Epoch 5 Step 1750 Loss 0.0000\n",
            "Epoch 5 Step 1800 Loss 0.0000\n",
            "Epoch 5 Step 1850 Loss 0.0000\n",
            "Epoch 5 Step 1900 Loss 0.0000\n",
            "Epoch 5 Step 1950 Loss 0.0000\n",
            "Epoch 5 Step 2000 Loss 0.0000\n",
            "Epoch 5 Step 2050 Loss 0.0000\n",
            "Epoch 5 Step 2100 Loss 0.0000\n",
            "Epoch 5 Step 2150 Loss 0.0000\n",
            "Epoch 5 Step 2200 Loss 0.0000\n",
            "Epoch 5 Step 2250 Loss 0.0000\n",
            "Epoch 5 Step 2300 Loss 0.0000\n",
            "Epoch 5 Step 2350 Loss 0.0000\n",
            "Epoch 5 Step 2400 Loss 0.0000\n",
            "Epoch 5 Step 2450 Loss 0.0000\n",
            "Epoch 5 Step 2500 Loss 0.0000\n",
            "Epoch 5 Step 2550 Loss 0.0000\n",
            "Epoch 5 Step 2600 Loss 0.0000\n",
            "Epoch 5 Step 2650 Loss 0.0000\n",
            "Epoch 5 Step 2700 Loss 0.0000\n",
            "Epoch 5 Step 2750 Loss 0.0000\n",
            "Epoch 5 Step 2800 Loss 0.0000\n",
            "Epoch 5 Step 2850 Loss 0.0000\n",
            "Epoch 5 Step 2900 Loss 0.0000\n",
            "Epoch 5 Step 2950 Loss 0.0000\n",
            "Epoch 5 Step 3000 Loss 0.0000\n",
            "Epoch 5 Step 3050 Loss 0.0000\n",
            "Epoch 5 Step 3100 Loss 0.0000\n",
            "Epoch 5 Step 3150 Loss 0.0000\n",
            "Epoch 5 Step 3200 Loss 0.0000\n",
            "Epoch 5 Step 3250 Loss 0.0000\n",
            "Epoch 5 Step 3300 Loss 0.0000\n",
            "Epoch 5 Step 3350 Loss 0.0000\n",
            "Epoch 5 Step 3400 Loss 0.0000\n",
            "Epoch 5 Step 3450 Loss 0.0000\n",
            "Epoch 5 Step 3500 Loss 0.0000\n",
            "Epoch 5 Step 3550 Loss 0.0000\n",
            "Epoch 5 Step 3600 Loss 0.0000\n",
            "Epoch 5 Step 3650 Loss 0.0000\n",
            "Epoch 5 Step 3700 Loss 0.0000\n",
            "Epoch 5 Loss 0.0000\n",
            "Time taken for 1 epoch: 1490.08 secs\n",
            "\n",
            "\n",
            "Epoch 6 -------------------\n",
            "Epoch 6 Step 0 Loss 0.0000\n",
            "Epoch 6 Step 50 Loss 0.0000\n",
            "Epoch 6 Step 100 Loss 0.0000\n",
            "Epoch 6 Step 150 Loss 0.0000\n",
            "Epoch 6 Step 200 Loss 0.0000\n",
            "Epoch 6 Step 250 Loss 0.0000\n",
            "Epoch 6 Step 300 Loss 0.0000\n",
            "Epoch 6 Step 350 Loss 0.0000\n",
            "Epoch 6 Step 400 Loss 0.0000\n",
            "Epoch 6 Step 450 Loss 0.0000\n",
            "Epoch 6 Step 500 Loss 0.0000\n",
            "Epoch 6 Step 550 Loss 0.0000\n",
            "Epoch 6 Step 600 Loss 0.0000\n",
            "Epoch 6 Step 650 Loss 0.0000\n",
            "Epoch 6 Step 700 Loss 0.0000\n",
            "Epoch 6 Step 750 Loss 0.0000\n",
            "Epoch 6 Step 800 Loss 0.0000\n",
            "Epoch 6 Step 850 Loss 0.0000\n",
            "Epoch 6 Step 900 Loss 0.0000\n",
            "Epoch 6 Step 950 Loss 0.0000\n",
            "Epoch 6 Step 1000 Loss 0.0000\n",
            "Epoch 6 Step 1050 Loss 0.0000\n",
            "Epoch 6 Step 1100 Loss 0.0000\n",
            "Epoch 6 Step 1150 Loss 0.0000\n",
            "Epoch 6 Step 1200 Loss 0.0000\n",
            "Epoch 6 Step 1250 Loss 0.0000\n",
            "Epoch 6 Step 1300 Loss 0.0000\n",
            "Epoch 6 Step 1350 Loss 0.0000\n",
            "Epoch 6 Step 1400 Loss 0.0000\n",
            "Epoch 6 Step 1450 Loss 0.0000\n",
            "Epoch 6 Step 1500 Loss 0.0000\n",
            "Epoch 6 Step 1550 Loss 0.0000\n",
            "Epoch 6 Step 1600 Loss 0.0000\n",
            "Epoch 6 Step 1650 Loss 0.0000\n",
            "Epoch 6 Step 1700 Loss 0.0000\n",
            "Epoch 6 Step 1750 Loss 0.0000\n",
            "Epoch 6 Step 1800 Loss 0.0000\n",
            "Epoch 6 Step 1850 Loss 0.0000\n",
            "Epoch 6 Step 1900 Loss 0.0000\n",
            "Epoch 6 Step 1950 Loss 0.0000\n",
            "Epoch 6 Step 2000 Loss 0.0000\n",
            "Epoch 6 Step 2050 Loss 0.0000\n",
            "Epoch 6 Step 2100 Loss 0.0000\n",
            "Epoch 6 Step 2150 Loss 0.0000\n",
            "Epoch 6 Step 2200 Loss 0.0000\n",
            "Epoch 6 Step 2250 Loss 0.0000\n",
            "Epoch 6 Step 2300 Loss 0.0000\n",
            "Epoch 6 Step 2350 Loss 0.0000\n",
            "Epoch 6 Step 2400 Loss 0.0000\n",
            "Epoch 6 Step 2450 Loss 0.0000\n",
            "Epoch 6 Step 2500 Loss 0.0000\n",
            "Epoch 6 Step 2550 Loss 0.0000\n",
            "Epoch 6 Step 2600 Loss 0.0000\n",
            "Epoch 6 Step 2650 Loss 0.0000\n",
            "Epoch 6 Step 2700 Loss 0.0000\n",
            "Epoch 6 Step 2750 Loss 0.0000\n",
            "Epoch 6 Step 2800 Loss 0.0000\n",
            "Epoch 6 Step 2850 Loss 0.0000\n",
            "Epoch 6 Step 2900 Loss 0.0000\n",
            "Epoch 6 Step 2950 Loss 0.0000\n",
            "Epoch 6 Step 3000 Loss 0.0000\n",
            "Epoch 6 Step 3050 Loss 0.0000\n",
            "Epoch 6 Step 3100 Loss 0.0000\n",
            "Epoch 6 Step 3150 Loss 0.0000\n",
            "Epoch 6 Step 3200 Loss 0.0000\n",
            "Epoch 6 Step 3250 Loss 0.0000\n",
            "Epoch 6 Step 3300 Loss 0.0000\n",
            "Epoch 6 Step 3350 Loss 0.0000\n",
            "Epoch 6 Step 3400 Loss 0.0000\n",
            "Epoch 6 Step 3450 Loss 0.0000\n",
            "Epoch 6 Step 3500 Loss 0.0000\n",
            "Epoch 6 Step 3550 Loss 0.0000\n",
            "Epoch 6 Step 3600 Loss 0.0000\n",
            "Epoch 6 Step 3650 Loss 0.0000\n",
            "Epoch 6 Step 3700 Loss 0.0000\n",
            "Saving checkpoint for epoch 6 at ./checkpoints/train/ckpt-2\n",
            "Epoch 6 Loss 0.0000\n",
            "Time taken for 1 epoch: 1476.79 secs\n",
            "\n",
            "\n",
            "Epoch 7 -------------------\n",
            "Epoch 7 Step 0 Loss 0.0000\n",
            "Epoch 7 Step 50 Loss 0.0000\n",
            "Epoch 7 Step 100 Loss 0.0000\n",
            "Epoch 7 Step 150 Loss 0.0000\n",
            "Epoch 7 Step 200 Loss 0.0000\n",
            "Epoch 7 Step 250 Loss 0.0000\n",
            "Epoch 7 Step 300 Loss 0.0000\n",
            "Epoch 7 Step 350 Loss 0.0000\n",
            "Epoch 7 Step 400 Loss 0.0000\n",
            "Epoch 7 Step 450 Loss 0.0000\n",
            "Epoch 7 Step 500 Loss 0.0000\n",
            "Epoch 7 Step 550 Loss 0.0000\n",
            "Epoch 7 Step 600 Loss 0.0000\n",
            "Epoch 7 Step 650 Loss 0.0000\n",
            "Epoch 7 Step 700 Loss 0.0000\n",
            "Epoch 7 Step 750 Loss 0.0000\n",
            "Epoch 7 Step 800 Loss 0.0000\n",
            "Epoch 7 Step 850 Loss 0.0000\n",
            "Epoch 7 Step 900 Loss 0.0000\n",
            "Epoch 7 Step 950 Loss 0.0000\n",
            "Epoch 7 Step 1000 Loss 0.0000\n",
            "Epoch 7 Step 1050 Loss 0.0000\n",
            "Epoch 7 Step 1100 Loss 0.0000\n",
            "Epoch 7 Step 1150 Loss 0.0000\n",
            "Epoch 7 Step 1200 Loss 0.0000\n",
            "Epoch 7 Step 1250 Loss 0.0000\n",
            "Epoch 7 Step 1300 Loss 0.0000\n",
            "Epoch 7 Step 1350 Loss 0.0000\n",
            "Epoch 7 Step 1400 Loss 0.0000\n",
            "Epoch 7 Step 1450 Loss 0.0000\n",
            "Epoch 7 Step 1500 Loss 0.0000\n",
            "Epoch 7 Step 1550 Loss 0.0000\n",
            "Epoch 7 Step 1600 Loss 0.0000\n",
            "Epoch 7 Step 1650 Loss 0.0000\n",
            "Epoch 7 Step 1700 Loss 0.0000\n",
            "Epoch 7 Step 1750 Loss 0.0000\n",
            "Epoch 7 Step 1800 Loss 0.0000\n",
            "Epoch 7 Step 1850 Loss 0.0000\n",
            "Epoch 7 Step 1900 Loss 0.0000\n",
            "Epoch 7 Step 1950 Loss 0.0000\n",
            "Epoch 7 Step 2000 Loss 0.0000\n",
            "Epoch 7 Step 2050 Loss 0.0000\n",
            "Epoch 7 Step 2100 Loss 0.0000\n",
            "Epoch 7 Step 2150 Loss 0.0000\n",
            "Epoch 7 Step 2200 Loss 0.0000\n",
            "Epoch 7 Step 2250 Loss 0.0000\n",
            "Epoch 7 Step 2300 Loss 0.0000\n",
            "Epoch 7 Step 2350 Loss 0.0000\n",
            "Epoch 7 Step 2400 Loss 0.0000\n",
            "Epoch 7 Step 2450 Loss 0.0000\n",
            "Epoch 7 Step 2500 Loss 0.0000\n",
            "Epoch 7 Step 2550 Loss 0.0000\n",
            "Epoch 7 Step 2600 Loss 0.0000\n",
            "Epoch 7 Step 2650 Loss 0.0000\n",
            "Epoch 7 Step 2700 Loss 0.0000\n",
            "Epoch 7 Step 2750 Loss 0.0000\n",
            "Epoch 7 Step 2800 Loss 0.0000\n",
            "Epoch 7 Step 2850 Loss 0.0000\n",
            "Epoch 7 Step 2900 Loss 0.0000\n",
            "Epoch 7 Step 2950 Loss 0.0000\n",
            "Epoch 7 Step 3000 Loss 0.0000\n",
            "Epoch 7 Step 3050 Loss 0.0000\n",
            "Epoch 7 Step 3100 Loss 0.0000\n",
            "Epoch 7 Step 3150 Loss 0.0000\n",
            "Epoch 7 Step 3200 Loss 0.0000\n",
            "Epoch 7 Step 3250 Loss 0.0000\n",
            "Epoch 7 Step 3300 Loss 0.0000\n",
            "Epoch 7 Step 3350 Loss 0.0000\n",
            "Epoch 7 Step 3400 Loss 0.0000\n",
            "Epoch 7 Step 3450 Loss 0.0000\n",
            "Epoch 7 Step 3500 Loss 0.0000\n",
            "Epoch 7 Step 3550 Loss 0.0000\n",
            "Epoch 7 Step 3600 Loss 0.0000\n",
            "Epoch 7 Step 3650 Loss 0.0000\n",
            "Epoch 7 Step 3700 Loss 0.0000\n",
            "Epoch 7 Loss 0.0000\n",
            "Time taken for 1 epoch: 1471.26 secs\n",
            "\n",
            "\n",
            "Epoch 8 -------------------\n",
            "Epoch 8 Step 0 Loss 0.0000\n",
            "Epoch 8 Step 50 Loss 0.0000\n",
            "Epoch 8 Step 100 Loss 0.0000\n",
            "Epoch 8 Step 150 Loss 0.0000\n",
            "Epoch 8 Step 200 Loss 0.0000\n",
            "Epoch 8 Step 250 Loss 0.0000\n",
            "Epoch 8 Step 300 Loss 0.0000\n",
            "Epoch 8 Step 350 Loss 0.0000\n",
            "Epoch 8 Step 400 Loss 0.0000\n",
            "Epoch 8 Step 450 Loss 0.0000\n",
            "Epoch 8 Step 500 Loss 0.0000\n",
            "Epoch 8 Step 550 Loss 0.0000\n",
            "Epoch 8 Step 600 Loss 0.0000\n",
            "Epoch 8 Step 650 Loss 0.0000\n",
            "Epoch 8 Step 700 Loss 0.0000\n",
            "Epoch 8 Step 750 Loss 0.0000\n",
            "Epoch 8 Step 800 Loss 0.0000\n",
            "Epoch 8 Step 850 Loss 0.0000\n",
            "Epoch 8 Step 900 Loss 0.0000\n",
            "Epoch 8 Step 950 Loss 0.0000\n",
            "Epoch 8 Step 1000 Loss 0.0000\n",
            "Epoch 8 Step 1050 Loss 0.0000\n",
            "Epoch 8 Step 1100 Loss 0.0000\n",
            "Epoch 8 Step 1150 Loss 0.0000\n",
            "Epoch 8 Step 1200 Loss 0.0000\n",
            "Epoch 8 Step 1250 Loss 0.0000\n",
            "Epoch 8 Step 1300 Loss 0.0000\n",
            "Epoch 8 Step 1350 Loss 0.0000\n",
            "Epoch 8 Step 1400 Loss 0.0000\n",
            "Epoch 8 Step 1450 Loss 0.0000\n",
            "Epoch 8 Step 1500 Loss 0.0000\n",
            "Epoch 8 Step 1550 Loss 0.0000\n",
            "Epoch 8 Step 1600 Loss 0.0000\n",
            "Epoch 8 Step 1650 Loss 0.0000\n",
            "Epoch 8 Step 1700 Loss 0.0000\n",
            "Epoch 8 Step 1750 Loss 0.0000\n",
            "Epoch 8 Step 1800 Loss 0.0000\n",
            "Epoch 8 Step 1850 Loss 0.0000\n",
            "Epoch 8 Step 1900 Loss 0.0000\n",
            "Epoch 8 Step 1950 Loss 0.0000\n",
            "Epoch 8 Step 2000 Loss 0.0000\n",
            "Epoch 8 Step 2050 Loss 0.0000\n",
            "Epoch 8 Step 2100 Loss 0.0000\n",
            "Epoch 8 Step 2150 Loss 0.0000\n",
            "Epoch 8 Step 2200 Loss 0.0000\n",
            "Epoch 8 Step 2250 Loss 0.0000\n",
            "Epoch 8 Step 2300 Loss 0.0000\n",
            "Epoch 8 Step 2350 Loss 0.0000\n",
            "Epoch 8 Step 2400 Loss 0.0000\n",
            "Epoch 8 Step 2450 Loss 0.0000\n",
            "Epoch 8 Step 2500 Loss 0.0000\n",
            "Epoch 8 Step 2550 Loss 0.0000\n",
            "Epoch 8 Step 2600 Loss 0.0000\n",
            "Epoch 8 Step 2650 Loss 0.0000\n",
            "Epoch 8 Step 2700 Loss 0.0000\n",
            "Epoch 8 Step 2750 Loss 0.0000\n",
            "Epoch 8 Step 2800 Loss 0.0000\n",
            "Epoch 8 Step 2850 Loss 0.0000\n",
            "Epoch 8 Step 2900 Loss 0.0000\n",
            "Epoch 8 Step 2950 Loss 0.0000\n",
            "Epoch 8 Step 3000 Loss 0.0000\n",
            "Epoch 8 Step 3050 Loss 0.0000\n",
            "Epoch 8 Step 3100 Loss 0.0000\n",
            "Epoch 8 Step 3150 Loss 0.0000\n",
            "Epoch 8 Step 3200 Loss 0.0000\n",
            "Epoch 8 Step 3250 Loss 0.0000\n",
            "Epoch 8 Step 3300 Loss 0.0000\n",
            "Epoch 8 Step 3350 Loss 0.0000\n",
            "Epoch 8 Step 3400 Loss 0.0000\n",
            "Epoch 8 Step 3450 Loss 0.0000\n",
            "Epoch 8 Step 3500 Loss 0.0000\n",
            "Epoch 8 Step 3550 Loss 0.0000\n",
            "Epoch 8 Step 3600 Loss 0.0000\n",
            "Epoch 8 Step 3650 Loss 0.0000\n",
            "Epoch 8 Step 3700 Loss 0.0000\n",
            "Epoch 8 Loss 0.0000\n",
            "Time taken for 1 epoch: 1474.77 secs\n",
            "\n",
            "\n",
            "Epoch 9 -------------------\n",
            "Epoch 9 Step 0 Loss 0.0000\n",
            "Epoch 9 Step 50 Loss 0.0000\n",
            "Epoch 9 Step 100 Loss 0.0000\n",
            "Epoch 9 Step 150 Loss 0.0000\n",
            "Epoch 9 Step 200 Loss 0.0000\n",
            "Epoch 9 Step 250 Loss 0.0000\n",
            "Epoch 9 Step 300 Loss 0.0000\n",
            "Epoch 9 Step 350 Loss 0.0000\n",
            "Epoch 9 Step 400 Loss 0.0000\n",
            "Epoch 9 Step 450 Loss 0.0000\n",
            "Epoch 9 Step 500 Loss 0.0000\n",
            "Epoch 9 Step 550 Loss 0.0000\n",
            "Epoch 9 Step 600 Loss 0.0000\n",
            "Epoch 9 Step 650 Loss 0.0000\n",
            "Epoch 9 Step 700 Loss 0.0000\n",
            "Epoch 9 Step 750 Loss 0.0000\n",
            "Epoch 9 Step 800 Loss 0.0000\n",
            "Epoch 9 Step 850 Loss 0.0000\n",
            "Epoch 9 Step 900 Loss 0.0000\n",
            "Epoch 9 Step 950 Loss 0.0000\n",
            "Epoch 9 Step 1000 Loss 0.0000\n",
            "Epoch 9 Step 1050 Loss 0.0000\n",
            "Epoch 9 Step 1100 Loss 0.0000\n",
            "Epoch 9 Step 1150 Loss 0.0000\n",
            "Epoch 9 Step 1200 Loss 0.0000\n",
            "Epoch 9 Step 1250 Loss 0.0000\n",
            "Epoch 9 Step 1300 Loss 0.0000\n",
            "Epoch 9 Step 1350 Loss 0.0000\n",
            "Epoch 9 Step 1400 Loss 0.0000\n",
            "Epoch 9 Step 1450 Loss 0.0000\n",
            "Epoch 9 Step 1500 Loss 0.0000\n",
            "Epoch 9 Step 1550 Loss 0.0000\n",
            "Epoch 9 Step 1600 Loss 0.0000\n",
            "Epoch 9 Step 1650 Loss 0.0000\n",
            "Epoch 9 Step 1700 Loss 0.0000\n",
            "Epoch 9 Step 1750 Loss 0.0000\n",
            "Epoch 9 Step 1800 Loss 0.0000\n",
            "Epoch 9 Step 1850 Loss 0.0000\n",
            "Epoch 9 Step 1900 Loss 0.0000\n",
            "Epoch 9 Step 1950 Loss 0.0000\n",
            "Epoch 9 Step 2000 Loss 0.0000\n",
            "Epoch 9 Step 2050 Loss 0.0000\n",
            "Epoch 9 Step 2100 Loss 0.0000\n",
            "Epoch 9 Step 2150 Loss 0.0000\n",
            "Epoch 9 Step 2200 Loss 0.0000\n",
            "Epoch 9 Step 2250 Loss 0.0000\n",
            "Epoch 9 Step 2300 Loss 0.0000\n",
            "Epoch 9 Step 2350 Loss 0.0000\n",
            "Epoch 9 Step 2400 Loss 0.0000\n",
            "Epoch 9 Step 2450 Loss 0.0000\n",
            "Epoch 9 Step 2500 Loss 0.0000\n",
            "Epoch 9 Step 2550 Loss 0.0000\n",
            "Epoch 9 Step 2600 Loss 0.0000\n",
            "Epoch 9 Step 2650 Loss 0.0000\n",
            "Epoch 9 Step 2700 Loss 0.0000\n",
            "Epoch 9 Step 2750 Loss 0.0000\n",
            "Epoch 9 Step 2800 Loss 0.0000\n",
            "Epoch 9 Step 2850 Loss 0.0000\n",
            "Epoch 9 Step 2900 Loss 0.0000\n",
            "Epoch 9 Step 2950 Loss 0.0000\n",
            "Epoch 9 Step 3000 Loss 0.0000\n",
            "Epoch 9 Step 3050 Loss 0.0000\n",
            "Epoch 9 Step 3100 Loss 0.0000\n",
            "Epoch 9 Step 3150 Loss 0.0000\n",
            "Epoch 9 Step 3200 Loss 0.0000\n",
            "Epoch 9 Step 3250 Loss 0.0000\n",
            "Epoch 9 Step 3300 Loss 0.0000\n",
            "Epoch 9 Step 3350 Loss 0.0000\n",
            "Epoch 9 Step 3400 Loss 0.0000\n",
            "Epoch 9 Step 3450 Loss 0.0000\n",
            "Epoch 9 Step 3500 Loss 0.0000\n",
            "Epoch 9 Step 3550 Loss 0.0000\n",
            "Epoch 9 Step 3600 Loss 0.0000\n",
            "Epoch 9 Step 3650 Loss 0.0000\n",
            "Epoch 9 Step 3700 Loss 0.0000\n",
            "Saving checkpoint for epoch 9 at ./checkpoints/train/ckpt-3\n",
            "Epoch 9 Loss 0.0000\n",
            "Time taken for 1 epoch: 1477.61 secs\n",
            "\n",
            "\n",
            "Epoch 10 -------------------\n",
            "Epoch 10 Step 0 Loss 0.0000\n",
            "Epoch 10 Step 50 Loss 0.0000\n",
            "Epoch 10 Step 100 Loss 0.0000\n",
            "Epoch 10 Step 150 Loss 0.0000\n",
            "Epoch 10 Step 200 Loss 0.0000\n",
            "Epoch 10 Step 250 Loss 0.0000\n",
            "Epoch 10 Step 300 Loss 0.0000\n",
            "Epoch 10 Step 350 Loss 0.0000\n",
            "Epoch 10 Step 400 Loss 0.0000\n",
            "Epoch 10 Step 450 Loss 0.0000\n",
            "Epoch 10 Step 500 Loss 0.0000\n",
            "Epoch 10 Step 550 Loss 0.0000\n",
            "Epoch 10 Step 600 Loss 0.0000\n",
            "Epoch 10 Step 650 Loss 0.0000\n",
            "Epoch 10 Step 700 Loss 0.0000\n",
            "Epoch 10 Step 750 Loss 0.0000\n",
            "Epoch 10 Step 800 Loss 0.0000\n",
            "Epoch 10 Step 850 Loss 0.0000\n",
            "Epoch 10 Step 900 Loss 0.0000\n",
            "Epoch 10 Step 950 Loss 0.0000\n",
            "Epoch 10 Step 1000 Loss 0.0000\n",
            "Epoch 10 Step 1050 Loss 0.0000\n",
            "Epoch 10 Step 1100 Loss 0.0000\n",
            "Epoch 10 Step 1150 Loss 0.0000\n",
            "Epoch 10 Step 1200 Loss 0.0000\n",
            "Epoch 10 Step 1250 Loss 0.0000\n",
            "Epoch 10 Step 1300 Loss 0.0000\n",
            "Epoch 10 Step 1350 Loss 0.0000\n",
            "Epoch 10 Step 1400 Loss 0.0000\n",
            "Epoch 10 Step 1450 Loss 0.0000\n",
            "Epoch 10 Step 1500 Loss 0.0000\n",
            "Epoch 10 Step 1550 Loss 0.0000\n",
            "Epoch 10 Step 1600 Loss 0.0000\n",
            "Epoch 10 Step 1650 Loss 0.0000\n",
            "Epoch 10 Step 1700 Loss 0.0000\n",
            "Epoch 10 Step 1750 Loss 0.0000\n",
            "Epoch 10 Step 1800 Loss 0.0000\n",
            "Epoch 10 Step 1850 Loss 0.0000\n",
            "Epoch 10 Step 1900 Loss 0.0000\n",
            "Epoch 10 Step 1950 Loss 0.0000\n",
            "Epoch 10 Step 2000 Loss 0.0000\n",
            "Epoch 10 Step 2050 Loss 0.0000\n",
            "Epoch 10 Step 2100 Loss 0.0000\n",
            "Epoch 10 Step 2150 Loss 0.0000\n",
            "Epoch 10 Step 2200 Loss 0.0000\n",
            "Epoch 10 Step 2250 Loss 0.0000\n",
            "Epoch 10 Step 2300 Loss 0.0000\n",
            "Epoch 10 Step 2350 Loss 0.0000\n",
            "Epoch 10 Step 2400 Loss 0.0000\n",
            "Epoch 10 Step 2450 Loss 0.0000\n",
            "Epoch 10 Step 2500 Loss 0.0000\n",
            "Epoch 10 Step 2550 Loss 0.0000\n",
            "Epoch 10 Step 2600 Loss 0.0000\n",
            "Epoch 10 Step 2650 Loss 0.0000\n",
            "Epoch 10 Step 2700 Loss 0.0000\n",
            "Epoch 10 Step 2750 Loss 0.0000\n",
            "Epoch 10 Step 2800 Loss 0.0000\n",
            "Epoch 10 Step 2850 Loss 0.0000\n",
            "Epoch 10 Step 2900 Loss 0.0000\n",
            "Epoch 10 Step 2950 Loss 0.0000\n",
            "Epoch 10 Step 3000 Loss 0.0000\n",
            "Epoch 10 Step 3050 Loss 0.0000\n",
            "Epoch 10 Step 3100 Loss 0.0000\n",
            "Epoch 10 Step 3150 Loss 0.0000\n",
            "Epoch 10 Step 3200 Loss 0.0000\n",
            "Epoch 10 Step 3250 Loss 0.0000\n",
            "Epoch 10 Step 3300 Loss 0.0000\n",
            "Epoch 10 Step 3350 Loss 0.0000\n",
            "Epoch 10 Step 3400 Loss 0.0000\n",
            "Epoch 10 Step 3450 Loss 0.0000\n",
            "Epoch 10 Step 3500 Loss 0.0000\n",
            "Epoch 10 Step 3550 Loss 0.0000\n",
            "Epoch 10 Step 3600 Loss 0.0000\n",
            "Epoch 10 Step 3650 Loss 0.0000\n",
            "Epoch 10 Step 3700 Loss 0.0000\n",
            "Epoch 10 Loss 0.0000\n",
            "Time taken for 1 epoch: 1488.32 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(sent1, sent2, score):\n",
        "    \"\"\"\n",
        "    Performs a single training step for the similarity model.\n",
        "\n",
        "    Args:\n",
        "    - sent1: Tokenized first sentence (batch_size, seq_len)\n",
        "    - sent2: Tokenized second sentence (batch_size, seq_len)\n",
        "    - score: Continuous similarity score (batch_size,)\n",
        "\n",
        "    Returns:\n",
        "    - loss: Scalar MSE loss\n",
        "    - pred: Predicted similarity score\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred = similarity_model((sent1, sent2), training=True)\n",
        "        pred = tf.squeeze(pred, axis=1)  # (batch_size,)\n",
        "        loss = loss_fn(score, pred)\n",
        "\n",
        "    gradients = tape.gradient(loss, similarity_model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, similarity_model.trainable_variables))\n",
        "    train_loss.update_state(loss)\n",
        "\n",
        "    return loss, pred\n",
        "\n",
        "    # # Prepare target inputs and outputs\n",
        "    # tar_inp = tar[:, :-1]\n",
        "    # tar_real = tar[:, 1:]\n",
        "\n",
        "    # with tf.GradientTape() as tape:\n",
        "    #     predictions, _ = transformer([inp, tar_inp], training=True)\n",
        "    #     loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    # # Compute gradients and apply them\n",
        "    # gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    # optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    # # Update the training loss and accuracy metrics\n",
        "    # train_loss(loss)\n",
        "    # train_accuracy(accuracy_function(tar_real, predictions))\n",
        "\n",
        "    # return loss, predictions\n",
        "\n",
        "# epoch_accuracies = []\n",
        "# all_batch_accuracies = []\n",
        "\n",
        "EPOCHS = 10\n",
        "steps_per_epoch = len(train_examples) // BATCH_SIZE\n",
        "# steps_per_epoch = 1000  # Set based on dataset size / batch size\n",
        "\n",
        "epoch_losses = []\n",
        "all_batch_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1} -------------------\")\n",
        "    # print(\"Re-initializing dataset iterator\")\n",
        "\n",
        "    # Reset metrics\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "    train_batches = make_batches(train_examples)  # Must return (sent1, sent2, score)\n",
        "    train_iter = iter(train_batches)\n",
        "\n",
        "    batch_losses = []\n",
        "    start = time.time()\n",
        "\n",
        "    # # Peek at 1 batch\n",
        "    # try:\n",
        "    #     sample = next(train_iter)\n",
        "    #     print(\"Batch loaded successfully.\")\n",
        "    # except Exception as e:\n",
        "    #     print(\"Batch loading failed:\", e)\n",
        "\n",
        "\n",
        "    # # Recreate infinite train_batches with .repeat()\n",
        "    # train_batches = make_batches(train_examples)\n",
        "    # train_iter = iter(train_batches)\n",
        "\n",
        "    # batch_accuracies = []\n",
        "\n",
        "    # # Reset metrics\n",
        "    # train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    # train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "\n",
        "    # Use fixed number of steps instead of iterating over the whole dataset\n",
        "    for step in range(steps_per_epoch):\n",
        "        # inp, tar = next(train_iter)\n",
        "        try:\n",
        "            # (batch,) = next(train_iter)  # Unpack the tuple\n",
        "            # sent1, sent2, score = batch\n",
        "            sent1, sent2, score = next(train_iter)\n",
        "\n",
        "\n",
        "        except StopIteration:\n",
        "            # If the iterator is exhausted, recreate it\n",
        "            # train_iter = iter(train_batches)\n",
        "            # (batch,) = next(train_iter)\n",
        "            # sent1, sent2, score = batch\n",
        "\n",
        "            train_iter = iter(train_batches)\n",
        "            sent1, sent2, score = next(train_iter)\n",
        "\n",
        "\n",
        "\n",
        "        # loss, predictions = train_step(inp, tar)\n",
        "        loss, _ = train_step(sent1, sent2, score)\n",
        "\n",
        "        batch_losses.append(loss.numpy())\n",
        "\n",
        "        # batch_accuracies.append(accuracy_function(tar[:, 1:], predictions).numpy())\n",
        "\n",
        "        # if step % 50 == 0:\n",
        "        #     print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f}')\n",
        "\n",
        "\n",
        "    all_batch_losses.append(batch_losses)\n",
        "    epoch_losses.append(train_loss.result().numpy())\n",
        "    # all_batch_accuracies.append(batch_accuracies)\n",
        "\n",
        "    # epoch_accuracies.append(train_accuracy.result().numpy())\n",
        "\n",
        "    if (epoch + 1) % 3 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "    # print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "    # print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f}')\n",
        "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09cbKM0naZjH"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcIkYoFpaZjH",
        "outputId": "58f6fc13-c38c-4cf4-ec40-fef64e075cb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss (MSE): nan\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on the validation set\n",
        "\n",
        "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
        "\n",
        "for sent1_val, sent2_val, score_val in val_batches:\n",
        "    pred_val = similarity_model((sent1_val, sent2_val), training=False)\n",
        "    pred_val = tf.squeeze(pred_val, axis=1)\n",
        "    loss = loss_fn(score_val, pred_val)\n",
        "    val_loss.update_state(loss)\n",
        "\n",
        "print(f\"Validation Loss (MSE): {val_loss.result().numpy():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "sFxx0oxzaZjH"
      },
      "outputs": [],
      "source": [
        "# Save\n",
        "#similarity_model.save_weights(\"transformer_similarity_weights.h5\")\n",
        "\n",
        "# Later, to load them back:\n",
        "# similarity_model.load_weights(\"transformer_similarity_weights.h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py_a5nxNaZjH"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "VrIYhLazaZjI",
        "outputId": "641e96c2-a6a7-45e9-89f8-2df53e2afeb9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(all_batch_losses[0], label='Batch Loss')\n",
        "#plt.plot(all_batch_accuracies[0], label='Batch Accuracy')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Loss & Accuracy per Batch (Epoch 1)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F02j-mGOaZjI"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
