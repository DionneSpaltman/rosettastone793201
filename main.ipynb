{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Pre-process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load language models only once\n",
    "nlp_dict = {\n",
    "    \"de\": spacy.load(\"de_core_news_sm\"),\n",
    "    \"en\": spacy.load(\"en_core_web_sm\"),\n",
    "    \"es\": spacy.load(\"es_core_news_sm\"),\n",
    "    \"fr\": spacy.load(\"fr_core_news_sm\"),\n",
    "    \"it\": spacy.load(\"it_core_news_sm\"),\n",
    "    \"ja\": spacy.load(\"ja_core_news_sm\"),\n",
    "    \"nl\": spacy.load(\"nl_core_news_sm\"),\n",
    "    \"pl\": spacy.load(\"pl_core_news_sm\"),\n",
    "    \"pt\": spacy.load(\"pt_core_news_sm\"),\n",
    "    \"ru\": spacy.load(\"ru_core_news_sm\"),\n",
    "    \"zh\": spacy.load(\"zh_core_web_sm\"),\n",
    "}\n",
    "\n",
    "# Define custom English stopword list\n",
    "default_en_stops = nlp_dict[\"en\"].Defaults.stop_words\n",
    "not_stop = {\"take\", 'off', 'nor', 'no', 'through', 'elsewhere', 'anyway', 'until', 'without', 'noone', 'otherwise', 'not', 'none', 'else', 'nobody', 'anyhow', 'less', 'whatever', 'never', 'few', 'rather', 'however', 'nowhere'}\n",
    "\n",
    "my_stop = {word for word in default_en_stops if word not in not_stop}\n",
    "\n",
    "# Stopwords per language\n",
    "stop_words = {\n",
    "    \"de\": nlp_dict[\"de\"].Defaults.stop_words,\n",
    "    \"en\": my_stop,\n",
    "    \"es\": nlp_dict[\"es\"].Defaults.stop_words,\n",
    "    \"fr\": nlp_dict[\"fr\"].Defaults.stop_words,\n",
    "    \"it\": nlp_dict[\"it\"].Defaults.stop_words,\n",
    "    \"ja\": nlp_dict[\"ja\"].Defaults.stop_words,\n",
    "    \"nl\": nlp_dict[\"nl\"].Defaults.stop_words,\n",
    "    \"pl\": nlp_dict[\"pl\"].Defaults.stop_words,\n",
    "    \"pt\": nlp_dict[\"pt\"].Defaults.stop_words,\n",
    "    \"ru\": nlp_dict[\"ru\"].Defaults.stop_words,\n",
    "    \"zh\": nlp_dict[\"zh\"].Defaults.stop_words\n",
    "}\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def spacy_batch_tokenizer(texts, lang):\n",
    "    if lang == \"zh\":\n",
    "        return texts  # no processing\n",
    "    tokenizer = nlp_dict[lang]\n",
    "    result = []\n",
    "    for doc in tokenizer.pipe(texts, batch_size=1024, n_process=1):  # set n_process>1 to enable multiprocessing\n",
    "        tokens = [\n",
    "            token.lemma_.lower().strip()\n",
    "            for token in doc\n",
    "            if token.lemma_.lower().strip() not in stop_words[lang]\n",
    "            and token.lemma_.lower().strip() not in punctuations\n",
    "            and token.lemma_.replace(\"-\", \"\").replace(\"'\", \"\").isalpha()\n",
    "        ]\n",
    "        result.append(\" \".join(tokens))\n",
    "    return result\n",
    "\n",
    "# --- Apply on the DataFrame in batches ---\n",
    "df = pd.read_csv(\"rs2.csv\")\n",
    "batch_size = 1000  # adjust this based on available RAM\n",
    "\n",
    "def process_column_in_batches(df, text_col, lang_col, out_col):\n",
    "    results = []\n",
    "    for lang in tqdm(df[lang_col].unique(), desc=f\"Processing {out_col} by language\"):\n",
    "        sub_df = df[df[lang_col] == lang]\n",
    "        processed = spacy_batch_tokenizer(sub_df[text_col].tolist(), lang)\n",
    "        results.append(pd.Series(processed, index=sub_df.index))\n",
    "    df[out_col] = pd.concat(results).sort_index()  # maintain order\n",
    "\n",
    "# Process both columns\n",
    "process_column_in_batches(df, \"sentence1\", \"lang1\", \"sentence1_clean\")\n",
    "process_column_in_batches(df, \"sentence2\", \"lang2\", \"sentence2_clean\")\n",
    "\n",
    "# Save result\n",
    "df.to_csv(\"rs2_pre_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Paraphrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Function to load model/tokenizer for a language pair (cached)\n",
    "model_cache = {}\n",
    "\n",
    "\n",
    "def get_model_and_tokenizer(src_lang, tgt_lang):\n",
    "    if f\"{src_lang}-{tgt_lang}\" in model_cache:\n",
    "        return model_cache[f\"{src_lang}-{tgt_lang}\"]\n",
    "\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
    "\n",
    "    try:\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "    except:\n",
    "        print(f\"Direct model {model_name} not found â€” falling back to multilingual model.\")\n",
    "        if tgt_lang == 'en':\n",
    "            model_name = 'Helsinki-NLP/opus-mt-mul-en'\n",
    "        elif src_lang == 'en':\n",
    "            model_name = 'Helsinki-NLP/opus-mt-en-mul'\n",
    "        else:\n",
    "            raise ValueError(f\"No available translation model for {src_lang} to {tgt_lang}\")\n",
    "\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "    model_cache[f\"{src_lang}-{tgt_lang}\"] = (tokenizer, model)\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "# Batched translation function\n",
    "def translate_batch(texts, src_lang, tgt_lang, batch_size=16):\n",
    "    tokenizer, model = get_model_and_tokenizer(src_lang, tgt_lang)\n",
    "    translated_texts = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        print(i, i + batch_size)\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        print(batch_texts)\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs)\n",
    "        decoded = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        translated_texts.extend(decoded)\n",
    "\n",
    "    return translated_texts\n",
    "\n",
    "\n",
    "# Function for batched back-translation\n",
    "def back_translate_batch(sentences, src_langs, batch_size=1024):\n",
    "    back_translated = []\n",
    "\n",
    "    unique_src_langs = set(src_langs)\n",
    "    for src_lang in tqdm(unique_src_langs, desc=f\"Processing  by language\"):\n",
    "        print(f\"Language :{src_lang}\")\n",
    "        intermediate_lang = \"en\"\n",
    "        if src_lang == \"en\":\n",
    "            intermediate_lang = \"de\"\n",
    "        elif src_lang == \"pl\":\n",
    "            intermediate_lang = \"de\"\n",
    "        elif src_lang == \"pt\":\n",
    "            intermediate_lang = \"tl\"\n",
    "\n",
    "        indices = [i for i, lang in enumerate(src_langs) if lang == src_lang]\n",
    "        src_sentences = [sentences[i] for i in indices]\n",
    "        unique_src_sentences = list(set(src_sentences))\n",
    "        print(f\"Unique sentences :{len(unique_src_sentences)}\")\n",
    "        # Translate to intermediate\n",
    "        unique_intermediate_sentences = translate_batch(unique_src_sentences, src_lang, intermediate_lang, batch_size)\n",
    "        # Translate back to source\n",
    "        unique_back_sentences = translate_batch(unique_intermediate_sentences, intermediate_lang, src_lang, batch_size)\n",
    "        back_sentences = list(src_sentences)\n",
    "\n",
    "        for ind in range(len(unique_src_sentences)):\n",
    "            unique_word = unique_src_sentences[ind]\n",
    "            back_sentences = [unique_back_sentences[ind] if x == unique_word else x for x in back_sentences]\n",
    "\n",
    "        # Assign results back\n",
    "        for idx, sent in zip(indices, back_sentences):\n",
    "            back_translated.append((idx, sent))\n",
    "\n",
    "    # Restore original order\n",
    "    back_translated.sort()\n",
    "    return [bt[1] for bt in back_translated]\n",
    "\n",
    "\n",
    "# Language code map\n",
    "lang_code_map = {\n",
    "    \"en\": \"en\", \"de\": \"de\", \"es\": \"es\", \"fr\": \"fr\", \"it\": \"it\",\n",
    "    \"pt\": \"pt\", \"nl\": \"nl\", \"pl\": \"pl\", \"ru\": \"ru\", \"ja\": \"jap\", \"zh\": \"zh\"\n",
    "}\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv(\"rs2_pre_processed.csv\")\n",
    "df.dropna(inplace=True)\n",
    "# Common pivot\n",
    "pivot_lang = \"en\"\n",
    "\n",
    "# Batched back-translation for sentence1_clean\n",
    "tqdm.pandas(desc=\"Preparing sentence1 back-translation\")\n",
    "sentences1 = df[\"sentence1_clean\"].tolist()\n",
    "langs1 = [lang_code_map[lang] for lang in df[\"lang1\"]]\n",
    "\n",
    "sentence1_bt = back_translate_batch(sentences1, langs1, batch_size=1)\n",
    "df[\"sentence1_bt\"] = sentence1_bt\n",
    "\n",
    "# Batched back-translation for sentence2_clean\n",
    "tqdm.pandas(desc=\"Preparing sentence2 back-translation\")\n",
    "sentences2 = df[\"sentence2_clean\"].tolist()\n",
    "langs2 = [lang_code_map[lang] for lang in df[\"lang2\"]]\n",
    "\n",
    "sentence2_bt = back_translate_batch(sentences2, langs2, batch_size=1)\n",
    "df[\"sentence2_bt\"] = sentence2_bt\n",
    "\n",
    "# Save results\n",
    "df.to_csv(\"rs2_backtranslated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Augmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "# Load data\n",
    "df = pd.read_csv('drive/MyDrive/MACHINE PROJECT/rs2_backtranslated.csv')\n",
    "df_1 = df.copy()\n",
    "df_4 = df.copy()\n",
    "\n",
    "df_4[\"sentence1_clean\"] = df_4[\"sentence1_bt\"]\n",
    "df_4[\"sentence2_clean\"] = df_4[\"sentence2_bt\"]\n",
    "df_1 = df_1.drop(df_1.sample(frac=0.5, random_state=42).index).reset_index(drop=True)\n",
    "df_4 = df_4.drop(df_4.sample(frac=0.99, random_state=42).index).reset_index(drop=True)\n",
    "print(df_1.shape)\n",
    "print(df_4.shape)\n",
    "df_final=pd.concat([df_1, df_4], axis=0, ignore_index=True)\n",
    "\n",
    "df_final =df_final.drop(columns=[\"sentence1_bt\",\"sentence2_bt\"])\n",
    "df_final.to_csv(\"/content/drive/MyDrive/MACHINE PROJECT/rs2_augmented.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.chdir(\"MACHINE PROJECT\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install keras\n",
    "!pip install tf_keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keras.api.preprocessing.sequence import pad_sequences\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "# Load data\n",
    "df = pd.read_csv('drive/MyDrive/MACHINE PROJECT/rs2_augmented.csv')\n",
    "\n",
    "# Load multilingual transformer model\n",
    "sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "\n",
    "# Token-level embeddings\n",
    "def get_token_embeddings(sentences, max_len=30):\n",
    "    \"\"\"Returns padded token embeddings for each sentence.\"\"\"\n",
    "    embeddings = sbert_model.encode(\n",
    "        sentences, output_value='token_embeddings', convert_to_numpy=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # Truncate/pad each sentence embedding to fixed number of tokens\n",
    "    padded = pad_sequences(embeddings, maxlen=max_len, padding='post', truncating='post', dtype='float32')\n",
    "    return padded\n",
    "\n",
    "\n",
    "sentence_list = df['sentence1_clean'].tolist() + df['sentence2_clean'].tolist()\n",
    "unique_sentence_list = list(set(sentence_list))\n",
    "\n",
    "max_len = 30  # max number of tokens per sentence\n",
    "unique_sentence_token_embeddings = get_token_embeddings(unique_sentence_list, max_len=max_len)\n",
    "# Create dictionary for fast lookup\n",
    "sentence_to_embedding = {\n",
    "    sent: embedding for sent, embedding in zip(unique_sentence_list, unique_sentence_token_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/content/drive/MyDrive/MACHINE PROJECT/sentence_to_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sentence_to_embedding, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN with Transformer - RAM Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Lambda, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    # Reduce precision to float16 to save memory\n",
    "    return {k: np.array(v, dtype=np.float16) for k, v in data.items()}\n",
    "\n",
    "def generate_dataset(df_path, embedding_map):\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "    # Only keep required columns and drop the rest\n",
    "    df = df[['sentence1_clean', 'sentence2_clean', 'score']]\n",
    "\n",
    "    # Use map with memory-efficient data types\n",
    "    df['sentence1_embedding'] = df['sentence1_clean'].map(embedding_map)\n",
    "    df['sentence2_embedding'] = df['sentence2_clean'].map(embedding_map)\n",
    "\n",
    "    # Convert to float16 arrays (RAM-efficient)\n",
    "    X1 = np.stack(df['sentence1_embedding'].values).astype(np.float16)\n",
    "    X2 = np.stack(df['sentence2_embedding'].values).astype(np.float16)\n",
    "    y = df['score'].astype(np.float16).values\n",
    "\n",
    "    return X1, X2, y\n",
    "\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "def build_model(input_shape):\n",
    "    input1 = Input(shape=input_shape)\n",
    "    input2 = Input(shape=input_shape)\n",
    "\n",
    "    # Replace RNN with global average pooling followed by dense\n",
    "    pooled1 = GlobalAveragePooling1D()(input1)\n",
    "    pooled2 = GlobalAveragePooling1D()(input2)\n",
    "\n",
    "    dense1 = Dense(128, activation='relu')(pooled1)\n",
    "    dense2 = Dense(128, activation='relu')(pooled2)\n",
    "\n",
    "    abs_diff = Lambda(lambda x: tf.abs(x[0] - x[1]))([dense1, dense2])\n",
    "    mult = Lambda(lambda x: x[0] * x[1])([dense1, dense2])\n",
    "\n",
    "    merged = concatenate([dense1, dense2, abs_diff, mult])\n",
    "    dense = Dense(128, activation='relu')(merged)\n",
    "    drop = Dropout(0.3)(dense)\n",
    "    output = Dense(1, activation='linear')(drop)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=output)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "max_len = 30\n",
    "sentence_to_embedding = load_embeddings(\"drive/MyDrive/MACHINE PROJECT/sentence_to_embedding.pkl\")\n",
    "\n",
    "sample_embedding = next(iter(sentence_to_embedding.values()))\n",
    "embedding_dim = sample_embedding.shape[1] if sample_embedding.ndim == 2 else sample_embedding.shape[0]\n",
    "X1, X2, y = generate_dataset(\"drive/MyDrive/MACHINE PROJECT/rs2_augmented.csv\", sentence_to_embedding)\n",
    "\n",
    "# Clean up large variables no longer needed\n",
    "del sentence_to_embedding\n",
    "gc.collect()\n",
    "\n",
    "# Train-test split\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(\n",
    "    X1, X2, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "del X1, X2, y\n",
    "gc.collect()\n",
    "\n",
    "model = build_model((max_len, embedding_dim))\n",
    "\n",
    "history = model.fit(\n",
    "    [X1_train, X2_train], y_train,\n",
    "    validation_data=([X1_val, X2_val], y_val),\n",
    "    epochs=10, batch_size=16  # Reduce batch size to save RAM\n",
    ")\n",
    "\n",
    "y_pred = model.predict([X1_val, X2_val], batch_size=16).flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr, _ = pearsonr(y_val, y_pred)\n",
    "spearman_corr, _ = spearmanr(y_val, y_pred)\n",
    "\n",
    "print(f\"ðŸ“ˆ Pearson Correlation:  {pearson_corr:.4f}\")\n",
    "print(f\"ðŸ“Š Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "plt.title('MAE over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper function\n",
    "def evaluate(y_true, y_pred):\n",
    "    return {\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MAPE\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "        \"RÂ²\": r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "print(evaluate(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN without Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"drive/MyDrive/MACHINE PROJECT/rs2_augmented.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"sentence1_clean\"].tolist() + df[\"sentence2_clean\"].tolist())\n",
    "\n",
    "# Convert to sequences\n",
    "s1_seq = tokenizer.texts_to_sequences(df[\"sentence1_clean\"])\n",
    "s2_seq = tokenizer.texts_to_sequences(df[\"sentence2_clean\"])\n",
    "\n",
    "# Pad sequences\n",
    "max_len = 50\n",
    "X1 = pad_sequences(s1_seq, maxlen=max_len, padding='post')\n",
    "X2 = pad_sequences(s2_seq, maxlen=max_len, padding='post')\n",
    "print(X1.shape)\n",
    "y = df[\"score\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Lambda, Dense, Dropout, concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Train-validation split\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(\n",
    "    X1, X2, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "\n",
    "# Inputs\n",
    "input1 = Input(shape=(max_len,))\n",
    "input2 = Input(shape=(max_len,))\n",
    "\n",
    "# Shared embedding + RNN encoder\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)\n",
    "\n",
    "embedded1 = embedding_layer(input1)\n",
    "embedded2 = embedding_layer(input2)\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "# Replace RNN with global average pooling followed by dense\n",
    "pooled1 = GlobalAveragePooling1D()(embedded1)\n",
    "pooled2 = GlobalAveragePooling1D()(embedded2)\n",
    "\n",
    "dense1 = Dense(128, activation='relu')(pooled1)\n",
    "dense2 = Dense(128, activation='relu')(pooled2)\n",
    "\n",
    "abs_diff = Lambda(lambda x: tf.abs(x[0] - x[1]))([dense1, dense2])\n",
    "mult = Lambda(lambda x: x[0] * x[1])([dense1, dense2])\n",
    "\n",
    "merged = concatenate([dense1, dense2, abs_diff, mult])\n",
    "dense = Dense(128, activation='relu')(merged)\n",
    "drop = Dropout(0.3)(dense)\n",
    "output = Dense(1, activation='linear')(drop)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    [X1_train, X2_train], y_train,\n",
    "    validation_data=([X1_val, X2_val], y_val),\n",
    "    epochs=10, batch_size=32\n",
    ")\n",
    "y_pred = model.predict([X1_val, X2_val]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "# Pearson Correlation\n",
    "pearson_corr, _ = pearsonr(y_val, y_pred)\n",
    "\n",
    "# Spearman Correlation\n",
    "spearman_corr, _ = spearmanr(y_val, y_pred)\n",
    "\n",
    "print(f\"ðŸ“ˆ Pearson Correlation:  {pearson_corr:.4f}\")\n",
    "print(f\"ðŸ“Š Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "# Plot loss and MAE\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "# MAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "plt.title('MAE over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper function\n",
    "def evaluate(y_true, y_pred):\n",
    "    return {\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MAPE\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "        \"RÂ²\": r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "print(evaluate(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, concatenate, GlobalAveragePooling1D\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "# Load pickled sentence embeddings\n",
    "def load_embeddings(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return {k: np.array(v, dtype=np.float16) for k, v in data.items()}\n",
    "\n",
    "# Define data generator\n",
    "def data_generator(df, embedding_map):\n",
    "    for _, row in df.iterrows():\n",
    "        emb1 = embedding_map.get(row['sentence1_clean'])\n",
    "        emb2 = embedding_map.get(row['sentence2_clean'])\n",
    "        if emb1 is not None and emb2 is not None:\n",
    "            yield (emb1.astype(np.float16), emb2.astype(np.float16)), np.float16(row['score'])\n",
    "\n",
    "# Build ANN model\n",
    "def build_ann_model(input_shape):\n",
    "    input1 = Input(shape=input_shape)\n",
    "    input2 = Input(shape=input_shape)\n",
    "\n",
    "    pooled1 = GlobalAveragePooling1D()(input1)\n",
    "    pooled2 = GlobalAveragePooling1D()(input2)\n",
    "\n",
    "    abs_diff = Lambda(lambda x: tf.abs(x[0] - x[1]))([pooled1, pooled2])\n",
    "    mult = Lambda(lambda x: x[0] * x[1])([pooled1, pooled2])\n",
    "\n",
    "    merged = concatenate([pooled1, pooled2, abs_diff, mult])\n",
    "\n",
    "    dense = Dense(128, activation='relu')(merged)\n",
    "    drop = Dropout(0.3)(dense)\n",
    "    output = Dense(1, activation='linear')(drop)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Load resources\n",
    "embedding_path = \"drive/MyDrive/sentence_to_embedding.pkl\"\n",
    "sentence_to_embedding = load_embeddings(embedding_path)\n",
    "\n",
    "csv_path = \"drive/MyDrive/rs2_augmented.csv\"\n",
    "df_full = pd.read_csv(csv_path)[['sentence1_clean', 'sentence2_clean', 'score']]\n",
    "\n",
    "# Remove unknown embeddings\n",
    "df_full = df_full[\n",
    "    df_full['sentence1_clean'].isin(sentence_to_embedding) &\n",
    "    df_full['sentence2_clean'].isin(sentence_to_embedding)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Train-test split\n",
    "df_train, df_val = train_test_split(df_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Infer input shape\n",
    "sample_embedding = next(iter(sentence_to_embedding.values()))\n",
    "embedding_dim = sample_embedding.shape[1] if sample_embedding.ndim == 2 else sample_embedding.shape[0]\n",
    "max_len = 30\n",
    "input_shape = (max_len, embedding_dim)\n",
    "\n",
    "# Create tf.data datasets\n",
    "batch_size = 40\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(df_train, sentence_to_embedding),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=input_shape, dtype=tf.float16),\n",
    "         tf.TensorSpec(shape=input_shape, dtype=tf.float16)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float16)\n",
    "    )\n",
    ").batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(df_val, sentence_to_embedding),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=input_shape, dtype=tf.float16),\n",
    "         tf.TensorSpec(shape=input_shape, dtype=tf.float16)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float16)\n",
    "    )\n",
    ").batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Free memory\n",
    "del df_full\n",
    "gc.collect()\n",
    "\n",
    "# Build and train model\n",
    "model = build_ann_model(input_shape)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3000\n",
    "\n",
    "# Recreate val_dataset without initial batching to avoid nested batches\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(df_val, sentence_to_embedding),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=input_shape, dtype=tf.float16),\n",
    "         tf.TensorSpec(shape=input_shape, dtype=tf.float16)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float16)\n",
    "    )\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)  # Apply desired batch size\n",
    "\n",
    "y_pred = model.predict(val_dataset).flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "y_val = df_val['score'].values\n",
    "pearson_corr, _ = pearsonr(y_val, y_pred)\n",
    "spearman_corr, _ = spearmanr(y_val, y_pred)\n",
    "\n",
    "print(f\"ðŸ“ˆ Pearson Correlation:  {pearson_corr:.4f}\")\n",
    "print(f\"ðŸ“Š Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "plt.title('MAE over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper function\n",
    "def evaluate(y_true, y_pred):\n",
    "    return {\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MAPE\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "        \"RÂ²\": r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "print(evaluate(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Lambda, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    # Reduce precision to float16 to save memory\n",
    "    return {k: np.array(v, dtype=np.float16) for k, v in data.items()}\n",
    "\n",
    "def generate_dataset(df_path, embedding_map):\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "    # Only keep required columns and drop the rest\n",
    "    df = df[['sentence1_clean', 'sentence2_clean', 'score']]\n",
    "\n",
    "    # Use map with memory-efficient data types\n",
    "    df['sentence1_embedding'] = df['sentence1_clean'].map(embedding_map)\n",
    "    df['sentence2_embedding'] = df['sentence2_clean'].map(embedding_map)\n",
    "\n",
    "    # Convert to float16 arrays (RAM-efficient)\n",
    "    X1 = np.stack(df['sentence1_embedding'].values).astype(np.float16)\n",
    "    X2 = np.stack(df['sentence2_embedding'].values).astype(np.float16)\n",
    "    y = df['score'].astype(np.float16).values\n",
    "\n",
    "    return X1, X2, y\n",
    "\n",
    "def build_model(input_shape):\n",
    "    input1 = Input(shape=input_shape)\n",
    "    input2 = Input(shape=input_shape)\n",
    "\n",
    "    shared_rnn = Bidirectional(LSTM(64, return_sequences=False))\n",
    "\n",
    "    encoded1 = shared_rnn(input1)\n",
    "    encoded2 = shared_rnn(input2)\n",
    "\n",
    "    abs_diff = Lambda(lambda x: tf.abs(x[0] - x[1]))([encoded1, encoded2])\n",
    "    mult = Lambda(lambda x: x[0] * x[1])([encoded1, encoded2])\n",
    "\n",
    "    merged = concatenate([encoded1, encoded2, abs_diff, mult])\n",
    "    dense = Dense(128, activation='relu')(merged)\n",
    "    drop = Dropout(0.3)(dense)\n",
    "    output = Dense(1, activation='linear')(drop)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=output)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "max_len = 30\n",
    "sentence_to_embedding = load_embeddings(\"drive/MyDrive/MACHINE PROJECT/sentence_to_embedding.pkl\")\n",
    "\n",
    "sample_embedding = next(iter(sentence_to_embedding.values()))\n",
    "embedding_dim = sample_embedding.shape[1] if sample_embedding.ndim == 2 else sample_embedding.shape[0]\n",
    "X1, X2, y = generate_dataset(\"drive/MyDrive/MACHINE PROJECT/rs2_augmented.csv\", sentence_to_embedding)\n",
    "\n",
    "# Clean up large variables no longer needed\n",
    "del sentence_to_embedding\n",
    "gc.collect()\n",
    "\n",
    "# Train-test split\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(\n",
    "    X1, X2, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "del X1, X2, y\n",
    "gc.collect()\n",
    "\n",
    "model = build_model((max_len, embedding_dim))\n",
    "\n",
    "history = model.fit(\n",
    "    [X1_train, X2_train], y_train,\n",
    "    validation_data=([X1_val, X2_val], y_val),\n",
    "    epochs=10, batch_size=16  # Reduce batch size to save RAM\n",
    ")\n",
    "\n",
    "y_pred = model.predict([X1_val, X2_val], batch_size=16).flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr, _ = pearsonr(y_val, y_pred)\n",
    "spearman_corr, _ = spearmanr(y_val, y_pred)\n",
    "\n",
    "print(f\"ðŸ“ˆ Pearson Correlation:  {pearson_corr:.4f}\")\n",
    "print(f\"ðŸ“Š Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "plt.title('MAE over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper function\n",
    "def evaluate(y_true, y_pred):\n",
    "    return {\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MAPE\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "        \"RÂ²\": r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "print(evaluate(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN without transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"drive/MyDrive/MACHINE PROJECT/rs2_augmented.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"sentence1_clean\"].tolist() + df[\"sentence2_clean\"].tolist())\n",
    "\n",
    "# Convert to sequences\n",
    "s1_seq = tokenizer.texts_to_sequences(df[\"sentence1_clean\"])\n",
    "s2_seq = tokenizer.texts_to_sequences(df[\"sentence2_clean\"])\n",
    "\n",
    "# Pad sequences\n",
    "max_len = 50\n",
    "X1 = pad_sequences(s1_seq, maxlen=max_len, padding='post')\n",
    "X2 = pad_sequences(s2_seq, maxlen=max_len, padding='post')\n",
    "y = df[\"score\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Lambda, Dense, Dropout, concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Train-validation split\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(\n",
    "    X1, X2, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "\n",
    "# Inputs\n",
    "input1 = Input(shape=(max_len,))\n",
    "input2 = Input(shape=(max_len,))\n",
    "\n",
    "# Shared embedding + RNN encoder\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)\n",
    "\n",
    "embedded1 = embedding_layer(input1)\n",
    "embedded2 = embedding_layer(input2)\n",
    "\n",
    "shared_lstm = Bidirectional(LSTM(64, return_sequences=False))\n",
    "\n",
    "encoded1 = shared_lstm(embedded1)\n",
    "encoded2 = shared_lstm(embedded2)\n",
    "\n",
    "# Feature interaction\n",
    "abs_diff = Lambda(lambda x: tf.abs(x[0] - x[1]))([encoded1, encoded2])\n",
    "mult = Lambda(lambda x: x[0] * x[1])([encoded1, encoded2])\n",
    "merged = concatenate([encoded1, encoded2, abs_diff, mult])\n",
    "\n",
    "# Dense layers\n",
    "dense = Dense(128, activation='relu')(merged)\n",
    "drop = Dropout(0.3)(dense)\n",
    "output = Dense(1, activation='linear')(drop)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    [X1_train, X2_train], y_train,\n",
    "    validation_data=([X1_val, X2_val], y_val),\n",
    "    epochs=10, batch_size=32\n",
    ")\n",
    "y_pred = model.predict([X1_val, X2_val]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotib.pyplot as plt\n",
    "# Pearson Correlation\n",
    "pearson_corr, _ = pearsonr(y_val, y_pred)\n",
    "\n",
    "# Spearman Correlation\n",
    "spearman_corr, _ = spearmanr(y_val, y_pred)\n",
    "\n",
    "print(f\"ðŸ“ˆ Pearson Correlation:  {pearson_corr:.4f}\")\n",
    "print(f\"ðŸ“Š Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "# Plot loss and MAE\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "# MAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "plt.title('MAE over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper function\n",
    "def evaluate(y_true, y_pred):\n",
    "    return {\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MAPE\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "        \"RÂ²\": r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "print(evaluate(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"numpy<2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR) # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load processed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV file\n",
    "df = pd.read_csv('/stopword_removal_dataframe.csv')\n",
    "\n",
    "# display the first few rows of the dataframe\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to get the minimum and maximum for column 'score'\n",
    "min_score = df['score'].min()\n",
    "max_score = df['score'].max()\n",
    "print(f\"Minimum score: {min_score}\")\n",
    "print(f\"Maximum score: {max_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to check if the values are actually between 0 and 1\n",
    "df['normalized_score'] = df['score'] / 5.0\n",
    "min_normalized_score = df['normalized_score'].min()\n",
    "max_normalized_score = df['normalized_score'].max()\n",
    "print(f\"Minimum score: {min_normalized_score}\")\n",
    "print(f\"Maximum score: {max_normalized_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the filtered dataframe to two lists\n",
    "# input_texts = df['processed_language1'].tolist()\n",
    "# target_texts = df['processed_language2'].tolist()\n",
    "\n",
    "# print(type(input_texts), type(target_texts))         # should both be <class 'list'>\n",
    "# print(type(input_texts[0]), type(target_texts[0]))   # should both be <class 'str'>\n",
    "\n",
    "# # Ensure all elements in input_texts and target_texts are strings\n",
    "# input_texts = [str(text) for text in input_texts]\n",
    "# target_texts = [str(text) for text in target_texts]\n",
    "\n",
    "# # Create the TensorFlow dataset\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((input_texts, target_texts))\n",
    "\n",
    "# # Define split ratio\n",
    "# split_ratio = 0.8\n",
    "# total_size = len(input_texts)\n",
    "# train_size = int(total_size * split_ratio)\n",
    "\n",
    "# # Create train and validation datasets\n",
    "# train_examples = dataset.take(train_size)\n",
    "# val_examples = dataset.skip(train_size)\n",
    "\n",
    "# train_examples = train_examples.repeat()\n",
    "\n",
    "# here we are creating the train and val examples\n",
    "examples = [\n",
    "    {\n",
    "        \"sentence1\": row['processed_language1'],\n",
    "        \"sentence2\": row['processed_language2'],\n",
    "        \"score\": row['normalized_score']\n",
    "    }\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "random.shuffle(examples)\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(examples) * split_ratio)\n",
    "\n",
    "train_examples = examples[:split_index]\n",
    "val_examples = examples[split_index:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if both are in the right format\n",
    "print(train_examples[0])\n",
    "print(val_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build tokenizers from my text\n",
    "# input_tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "#     (text for text in input_texts), target_vocab_size=2**13)\n",
    "\n",
    "# target_tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "#     (text for text in target_texts), target_vocab_size=2**13)\n",
    "\n",
    "# Combine both sides of the pairs for a shared vocabulary\n",
    "all_sentences = pd.concat([df['processed_language1'], df['processed_language2']]).astype(str)\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (text for text in all_sentences), target_vocab_size=2**13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to tokenize input-target pairs\n",
    "# # So it gets the input string and the target string and returns tokenized version\n",
    "# def encode_pair(input_str, target_str):\n",
    "#     input_tokens = input_tokenizer.encode(input_str.numpy().decode('utf-8'))\n",
    "#     target_tokens = target_tokenizer.encode(target_str.numpy().decode('utf-8'))\n",
    "#     return input_tokens, target_tokens\n",
    "\n",
    "def encode_pair(sentence1, sentence2, score):\n",
    "    tokens1 = tokenizer.encode(sentence1.numpy().decode('utf-8'))\n",
    "    tokens2 = tokenizer.encode(sentence2.numpy().decode('utf-8'))\n",
    "    return tokens1, tokens2, score\n",
    "\n",
    "# def tf_encode(input_str, target_str):\n",
    "#     input_tokens, target_tokens = tf.py_function(encode_pair, [input_str, target_str], [tf.int64, tf.int64])\n",
    "#     return input_tokens, target_tokens\n",
    "\n",
    "# Wrap with tf.py_function to use in TensorFlow pipeline\n",
    "# This lets you use regular Python code (like calling .numpy() and .decode()) inside a TensorFlow data pipeline\n",
    "def tf_encode(sentence1, sentence2, score):\n",
    "    tokens1, tokens2, score = tf.py_function(\n",
    "        encode_pair, [sentence1, sentence2, score], [tf.int64, tf.int64, tf.float32]\n",
    "    )\n",
    "    tokens1.set_shape([None])\n",
    "    tokens2.set_shape([None])\n",
    "    score.set_shape([])  # scalar float\n",
    "    return tokens1, tokens2, score\n",
    "\n",
    "# # Map the datasets through the tokenizer\n",
    "# # It applies tf_encode to every pair in the dataset. Now each example in train_dataset is a pair of tokenized sequences\n",
    "# train_dataset = train_examples.map(tf_encode)\n",
    "# val_dataset = val_examples.map(tf_encode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "def encode_pair_fixed(input_str, target_str):\n",
    "    input_tokens = tokenizer.encode(input_str.numpy().decode('utf-8'))[:MAX_SEQ_LEN]\n",
    "    target_tokens = tokenizer.encode(target_str.numpy().decode('utf-8'))[:MAX_SEQ_LEN]\n",
    "\n",
    "    # Pad manually\n",
    "    input_tokens += [0] * (MAX_SEQ_LEN - len(input_tokens))\n",
    "    target_tokens += [0] * (MAX_SEQ_LEN - len(target_tokens))\n",
    "\n",
    "    return input_tokens, target_tokens\n",
    "\n",
    "def tf_encode(input_str, target_str, score):\n",
    "    input_tokens, target_tokens = tf.py_function(\n",
    "        encode_pair_fixed, [input_str, target_str], [tf.int64, tf.int64]\n",
    "    )\n",
    "    input_tokens.set_shape([MAX_SEQ_LEN])\n",
    "    target_tokens.set_shape([MAX_SEQ_LEN])\n",
    "    return input_tokens, target_tokens, score\n",
    "\n",
    "\n",
    "def make_batches(examples, batch_size=32):\n",
    "    sent1_list = []\n",
    "    sent2_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for ex in examples:\n",
    "        try:\n",
    "            s1 = str(ex[\"sentence1\"])\n",
    "            s2 = str(ex[\"sentence2\"])\n",
    "            score = float(ex[\"score\"])\n",
    "            sent1_list.append(s1)\n",
    "            sent2_list.append(s2)\n",
    "            score_list.append(score)\n",
    "        except Exception as e:\n",
    "            print(\"Skipping example due to error:\", ex, e)\n",
    "            continue\n",
    "\n",
    "    sent1_tensor = tf.constant(sent1_list)\n",
    "    sent2_tensor = tf.constant(sent2_list)\n",
    "    score_tensor = tf.constant(score_list, dtype=tf.float32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sent1_tensor, sent2_tensor, score_tensor))\n",
    "    dataset = dataset.map(tf_encode, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # dataset = dataset.padded_batch(batch_size, padded_shapes=([None], [None], []))\n",
    "    # MAX_SEQ_LEN = 128\n",
    "    # dataset = dataset.padded_batch(batch_size, padded_shapes=([MAX_SEQ_LEN], [MAX_SEQ_LEN], []))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    # dataset = dataset.repeat()  # Infinite dataset\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    # return dataset, sent1_list, sent2_list\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = make_batches(train_examples)\n",
    "\n",
    "# Inspect a single batch\n",
    "for sent1, sent2, score in train_batches.take(1):\n",
    "    print(\"Shape sent1:\", sent1.shape)\n",
    "    print(\"Shape sent2:\", sent2.shape)\n",
    "    print(\"Shape score:\", score.shape)\n",
    "\n",
    "    # Check if all shapes in the batch are as expected\n",
    "    assert sent1.shape[1] == MAX_SEQ_LEN, f\"sent1 sequence length mismatch: {sent1.shape[1]}\"\n",
    "    assert sent2.shape[1] == MAX_SEQ_LEN, f\"sent2 sequence length mismatch: {sent2.shape[1]}\"\n",
    "    assert sent1.shape == sent2.shape, f\"Mismatch between sent1 and sent2: {sent1.shape} vs {sent2.shape}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get everything ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample types in sent1_list:\", {type(s) for s in sent1_list})\n",
    "print(\"Sample types in sent2_list:\", {type(s) for s in sent2_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET PREPARATION\n",
    "\n",
    "# Define constants for dataset preparation\n",
    "# MAX_TOKENS = 128\n",
    "# BUFFER_SIZE = 20000  # Size of the buffer for shuffling the dataset.\n",
    "# BATCH_SIZE = 64  # Number of samples per batch.\n",
    "\n",
    "# Lower constants for baseline\n",
    "MAX_TOKENS = 128\n",
    "BUFFER_SIZE = 1000  # Size of the buffer for shuffling the dataset.\n",
    "BATCH_SIZE = 16  # Number of samples per batch.\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_pairs(input_text, target_text):\n",
    "    \"\"\"\n",
    "    Tokenizes both input and target strings using SubwordTextEncoder-like tokenizers.\n",
    "    Returns int64 token sequences with shape info attached.\n",
    "    \"\"\"\n",
    "    # Encode using SubwordTextEncoder and convert to padded tensors\n",
    "    input_tokens = tf.py_function(\n",
    "        lambda x: tf.constant(tokenizer.encode(x.numpy().decode('utf-8')), dtype=tf.int64),\n",
    "        [input_text],\n",
    "        tf.int64\n",
    "    )\n",
    "    target_tokens = tf.py_function(\n",
    "        lambda x: tf.constant(tokenizer.encode(x.numpy().decode('utf-8')), dtype=tf.int64),\n",
    "        [target_text],\n",
    "        tf.int64\n",
    "    )\n",
    "\n",
    "    # Help TF understand the rank of returned tensors for later batching\n",
    "    input_tokens.set_shape([None])\n",
    "    target_tokens.set_shape([None])\n",
    "    return input_tokens, target_tokens\n",
    "\n",
    "# Filtering function to exclude long sequences\n",
    "def filter_max_tokens(input_tensor, target_tensor):\n",
    "    \"\"\"\n",
    "    Filters out input-target pairs where either sequence exceeds MAX_TOKENS.\n",
    "    \"\"\"\n",
    "    num_tokens = tf.maximum(tf.shape(input_tensor)[0], tf.shape(target_tensor)[0])\n",
    "    return num_tokens < MAX_TOKENS\n",
    "\n",
    "# # Batching and preprocessing wrapper\n",
    "# def make_batches(ds):\n",
    "#     \"\"\"\n",
    "#     Caches, shuffles, tokenizes, filters, pads, and batches the dataset.\n",
    "#     \"\"\"\n",
    "#     return (\n",
    "#         ds\n",
    "#         .cache()\n",
    "#         .shuffle(BUFFER_SIZE)\n",
    "#         .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#         .filter(filter_max_tokens)\n",
    "#         .padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))  # pad manually here\n",
    "#         # .repeat() # NEW ADDITION\n",
    "#         .prefetch(tf.data.AUTOTUNE)\n",
    "#     )\n",
    "\n",
    "\n",
    "# Prepare the training and validation datasets by applying the make_batches function.\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a small batch (e.g., 3) from the batched and tokenized dataset\n",
    "for sent1_batch, sent2_batch, score_batch in train_batches.take(1):\n",
    "    print(\"> Sentence 1 Examples:\")\n",
    "    for line in sent1_batch.numpy():\n",
    "        print(tokenizer.decode([token for token in line if token != 0]))\n",
    "\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(\"> Sentence 2 Examples:\")\n",
    "    for line in sent2_batch.numpy():\n",
    "        print(tokenizer.decode([token for token in line if token != 0]))\n",
    "\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(\"> Similarity Scores:\")\n",
    "    print(score_batch.numpy())\n",
    "\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(\"> Token IDs (Sentence 1):\")\n",
    "    for line in sent1_batch.numpy():\n",
    "        print(line)\n",
    "\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(\"> Token IDs (Sentence 2):\")\n",
    "    for line in sent2_batch.numpy():\n",
    "        print(line)\n",
    "\n",
    "# # Take a small batch (e.g., 3) from the dataset\n",
    "# for src_examples, tgt_examples in train_examples.batch(3).take(1):\n",
    "#     print(\"> Examples in Source Language:\")\n",
    "#     for line in src_examples.numpy():\n",
    "#         print(line.decode('utf-8'))\n",
    "\n",
    "# print(\"----------------------------------------------\")\n",
    "\n",
    "# print(\"> Examples in Target Language:\")\n",
    "# for line in tgt_examples.numpy():\n",
    "#     print(line.decode('utf-8'))\n",
    "\n",
    "# print(\"----------------------------------------------\")\n",
    "\n",
    "# # Tokenize the target examples using your custom tokenizer\n",
    "# encoded = [tokenizer.encode(text.numpy().decode('utf-8')) for text in tgt_examples]\n",
    "\n",
    "# # Print tokenized form\n",
    "# for row in encoded:\n",
    "#     print(row)\n",
    "\n",
    "# print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSITIONAL ENCODING\n",
    "# Transformers have no recurrence or convolution, so we inject sequence order using sine/cosine signals.\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    \"\"\"\n",
    "    Computes the angle rates for the positional encoding.\n",
    "    The formula ensures that each dimension of the embedding varies at a different wavelength (some change faster than others).\n",
    "\n",
    "    Parameters:\n",
    "    - pos: Position index.\n",
    "    - i: Dimension index.\n",
    "    - d_model: Depth of the model (number of dimensions).\n",
    "\n",
    "    Returns:\n",
    "    - The angle rates for positional encoding.\n",
    "    \"\"\"\n",
    "    # Calculate the angles based on position and dimension index.\n",
    "    # This formula helps in varying the wavelength across different dimensions.\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    \"\"\"\n",
    "    Generates a positional encoding matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - position: The maximum position index.\n",
    "    - d_model: The depth of the model (number of dimensions).\n",
    "\n",
    "    Returns:\n",
    "    - A positional encoding matrix of shape (1, position, d_model).\n",
    "    \"\"\"\n",
    "    # Generate angles based on positions and dimensions.\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    # Apply sine to even indices in the angles array (2i).\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # Apply cosine to odd indices in the angles array (2i+1).\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    # Expand the dimensions to fit the model requirements.\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    # Cast the encoding to TensorFlow float32 type.\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASKING\n",
    "# Masks prevent attention to padding tokens or future tokens during decoding.\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Creates a padding mask for sequences.\n",
    "    This mask hides the padding tokens (i.e., zeros) so they don't affect the attention mechanism. It returns 1s where padding exists, and 0s elsewhere.\n",
    "    Parameters:\n",
    "    - seq: The sequence of tokens.\n",
    "\n",
    "    Returns:\n",
    "    - A padding mask for the sequence.\n",
    "    \"\"\"\n",
    "    # Create a mask where every zero in the sequence is marked with a 1 (padding) and others with a 0.\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # Add extra dimensions to the mask so it can be added to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # Shape: (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a look-ahead mask to mask future tokens in a sequence.\n",
    "    Each token can only see previous ones (or itself), but not the next ones, ensuring proper autoregressive behavior.\n",
    "    Parameters:\n",
    "    - size: Size of the mask.\n",
    "\n",
    "    Returns:\n",
    "    - A look-ahead mask of shape (size, size).\n",
    "    \"\"\"\n",
    "    # Create a mask where every entry that is in the lower triangle (including the diagonal) is 0, and everything else is 1.\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # Shape: (seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALING AND DOT PRODUCT ATTENTION\n",
    "# This is the core building block of attention: compare queries with keys, weigh values\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Calculates the attention weights and applies them to the value vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - q (query): Tensor with shape (..., seq_len_q, depth)\n",
    "    - k (key): Tensor with shape (..., seq_len_k, depth)\n",
    "    - v (value): Tensor with shape (..., seq_len_v, depth_v)\n",
    "    - mask: (Optional) Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k).\n",
    "\n",
    "    Returns:\n",
    "    - output: The result of applying attention weights to the value vectors.\n",
    "    - attention_weights: The attention weights.\n",
    "    \"\"\"\n",
    "    # Compute the dot product of the query and key tensors. Transpose the key tensor for proper alignment.\n",
    "    # This gives us a similarity score between each query and key.\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # Shape: (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # Scale the dot product by the square root of the depth of the key tensor.\n",
    "    # This helps in preventing the softmax function from having extremely small gradients.\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)  # Get the depth of the keys.\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # Apply the mask if provided. The mask is used to nullify the effect of padding or future information.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  # Use a large negative number to mask.\n",
    "\n",
    "    # Apply softmax to get the attention weights. The softmax is applied on the key sequence dimension.\n",
    "    # It shows how much attention each word pays to others.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # Shape: (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # Apply the attention weights to the value tensor to get the output.\n",
    "    output = tf.matmul(attention_weights, v)  # Shape: (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTIHEAD ATTENTION\n",
    "# Instead of attending once, we split into multiple attention \"heads\" for richer representations\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Initialize the MultiHeadAttention layer.\n",
    "\n",
    "        Parameters:\n",
    "        - d_model: Dimensionality of the model's output space.\n",
    "        - num_heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads  # Number of attention heads.\n",
    "        self.d_model = d_model  # Dimensionality of the model's output space.\n",
    "\n",
    "        # Ensure the model's dimension is divisible by the number of heads to evenly distribute dimensions to each head.\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads  # Dimensionality per attention head.\n",
    "\n",
    "        # Define dense layers for the queries, keys, and values.\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        # Final dense layer.\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, depth) and transpose the result.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor.\n",
    "        - batch_size: Size of the batch.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor with shape (batch_size, num_heads, seq_len, depth).\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        \"\"\"\n",
    "        The logic for the multi-head attention layer's forward pass.\n",
    "\n",
    "        Parameters:\n",
    "        - v: Value tensor.\n",
    "        - k: Key tensor.\n",
    "        - q: Query tensor.\n",
    "        - mask: Mask to be applied.\n",
    "\n",
    "        Returns:\n",
    "        - output: Output tensor.\n",
    "        - attention_weights: Attention weights.\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        # Apply dense layers to queries, keys, and values.\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Split the dense outputs into multiple heads and transpose.\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # Perform scaled dot product attention.\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # Transpose and reshape the attention output to match the input's dimensionality.\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        # Apply the final dense layer.\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    \"\"\"\n",
    "    Creates a point-wise feed forward network. This consists of two dense layers with a ReLU activation\n",
    "    in between, which is used within each transformer block.\n",
    "\n",
    "    Parameters:\n",
    "    - d_model: The dimensionality of the input and output of the transformer model.\n",
    "    - dff: The dimensionality of the inner layer, typically much larger than d_model to allow\n",
    "           the model to combine features in the data in a high-dimensional space before projecting\n",
    "           back down to d_model dimensions.\n",
    "\n",
    "    Returns:\n",
    "    - A tf.keras.Sequential model representing the feed forward network.\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        # First dense layer with dff units and ReLU activation. This expands the dimensionality to dff,\n",
    "        # allowing the network to learn more complex features.\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # Output shape: (batch_size, seq_len, dff)\n",
    "\n",
    "        # Second dense layer that projects the outputs back down to d_model dimensions.\n",
    "        tf.keras.layers.Dense(d_model)  # Output shape: (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the EncoderLayer with multi-head attention, point-wise feed-forward network,\n",
    "        dropout, and layer normalization components.\n",
    "\n",
    "        Parameters:\n",
    "        - d_model: Dimensionality of the model's output space.\n",
    "        - num_heads: Number of attention heads.\n",
    "        - dff: Dimensionality of the feed-forward network's inner layer.\n",
    "        - rate: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)  # Multi-head attention layer.\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)  # Point-wise feed-forward network.\n",
    "\n",
    "        # Layer normalization (first instance).\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        # Layer normalization (second instance).\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Dropout (first instance).\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        # Dropout (second instance).\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        The logic for one pass of the encoder layer.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor.\n",
    "        - training: Boolean indicating if the layer should behave in training mode (applying dropout) or in inference mode.\n",
    "        - mask: Mask to be applied on the multi-head attention layer.\n",
    "\n",
    "        Returns:\n",
    "        - The output tensor of the encoder layer.\n",
    "        \"\"\"\n",
    "        # Apply multi-head attention to the input (self attention).\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # Output shape: (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)  # Apply dropout to the attention output.\n",
    "\n",
    "        # Add & normalize.\n",
    "        out1 = self.layernorm1(x + attn_output)  # Residual connection followed by layer normalization.\n",
    "\n",
    "        # Apply the feed-forward network to the normalized attention output.\n",
    "        ffn_output = self.ffn(out1)  # Output shape: (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)  # Apply dropout to the feed-forward network output.\n",
    "\n",
    "        # Final add & normalize step.\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection followed by another layer normalization.\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder part of the Transformer.\n",
    "\n",
    "        Parameters:\n",
    "        - num_layers: Number of encoder layers.\n",
    "        - d_model: Dimensionality of the model's output space.\n",
    "        - num_heads: Number of attention heads.\n",
    "        - dff: Dimensionality of the feed-forward network's inner layer.\n",
    "        - input_vocab_size: Size of the input vocabulary.\n",
    "        - rate: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer for the input tokens.\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding up to MAX_TOKENS.\n",
    "        self.pos_encoding = positional_encoding(MAX_TOKENS, self.d_model)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.enc_layers = [EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate) for _ in range(num_layers)]\n",
    "\n",
    "        # Dropout layer.\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # Adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # Scale embeddings.\n",
    "        x += self.pos_encoding[:, :seq_len, :]  # Add position encoding.\n",
    "        x = self.dropout(x, training=training)  # Apply dropout.\n",
    "\n",
    "        # Pass the input through each encoder layer.\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
    "\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model for sentence similarity.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, input_vocab_size=input_vocab_size, rate=rate)\n",
    "\n",
    "        # # Final linear layer that projects the decoder's output to the target vocabulary size.\n",
    "        # self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        \"\"\"\n",
    "        The logic for one forward pass through the model.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs: A tuple of input tensor and target tensor.\n",
    "        - training: Boolean indicating if the layer should behave in training mode or inference mode.\n",
    "\n",
    "        Returns:\n",
    "        - sentence_embedding: A fixed-size vector representation (e.g., mean-pooled encoder output).\n",
    "        \"\"\"\n",
    "        # Create encoder padding mask\n",
    "        enc_padding_mask = create_padding_mask(inputs)\n",
    "\n",
    "        # Run encoder\n",
    "        enc_output = self.encoder(inputs, training=training, mask=enc_padding_mask)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Reduce to fixed-size vector (e.g., mean pooling)\n",
    "        sentence_embedding = tf.reduce_mean(enc_output, axis=1)  # (batch_size, d_model)\n",
    "        return sentence_embedding\n",
    "\n",
    "        # inp, tar = inputs\n",
    "\n",
    "        # # Create masks for padding and future tokens.\n",
    "        # enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "\n",
    "        # # Pass the input through the encoder.\n",
    "        # #enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        # #self.encoder(inp, training=training, mask=enc_padding_mask)\n",
    "        # enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
    "\n",
    "        # # Pass the encoder output and target through the decoder.\n",
    "        # #dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        # #self.decoder(tar, enc_output, training=training,\n",
    "        #  #    look_ahead_mask=look_ahead_mask,\n",
    "        #   #   padding_mask=dec_padding_mask)\n",
    "\n",
    "        # dec_output, attention_weights = self.decoder(\n",
    "        #       tar,\n",
    "        #       enc_output,\n",
    "        #       training=training,\n",
    "        #       look_ahead_mask=look_ahead_mask,\n",
    "        #       padding_mask=dec_padding_mask)\n",
    "\n",
    "\n",
    "        # # Pass the decoder output through the final linear layer.\n",
    "        # final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        # return final_output, attention_weights\n",
    "\n",
    "\n",
    "    # def create_masks(self, inp, tar):\n",
    "    #     \"\"\"\n",
    "    #     Creates masks for padding and look ahead for the encoder and decoder.\n",
    "\n",
    "    #     Parameters:\n",
    "    #     - inp: Input tensor.\n",
    "    #     - tar: Target tensor.\n",
    "\n",
    "    #     Returns:\n",
    "    #     - enc_padding_mask: Padding mask for the encoder.\n",
    "    #     - look_ahead_mask: Look-ahead mask for the decoder.\n",
    "    #     - dec_padding_mask: Padding mask for the decoder to mask the encoder outputs.\n",
    "    #     \"\"\"\n",
    "    #     # Encoder padding mask.\n",
    "    #     enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    #     # Decoder padding mask for the second attention block (to mask encoder outputs).\n",
    "    #     dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    #     # Look-ahead mask (to mask future tokens) and decoder target padding mask combined.\n",
    "    #     look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    #     dec_target_padding_mask = create_padding_mask(tar)\n",
    "    #     look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    #     return enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
    "\n",
    "# -------------------------------\n",
    "# Transformer Model Hyperparameters\n",
    "# -------------------------------\n",
    "\n",
    "num_layers = 4 # number of encoder/decoder layers in the Transformer\n",
    "d_model = 128 # size of the embedding vector for each word\n",
    "dff = 512 # size of the hidden layer inside the Feed Forward Neural Network\n",
    "num_heads = 8 # number of attention heads (must divide d_model evenly)\n",
    "dropout_rate = 0.1 # dropout rate for regularization\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)  # Model dimensionality, cast to float32 for calculation.\n",
    "        self.warmup_steps = warmup_steps  # Number of steps to linearly increase the learning rate.\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)  # Linearly increase then decrease based on warmup steps.\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)  # Calculate the learning rate.\n",
    "        # Final learning rate: (1 / sqrt(d_model)) * min(arg1, arg2)\n",
    "\n",
    "# Instantiate the learning rate schedule and Adam optimizer\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate,\n",
    "    beta_1=0.9,         # First moment decay (default)\n",
    "    beta_2=0.98,        # Second moment decay (used in the original Transformer paper)\n",
    "    epsilon=1e-9        # Small value to avoid division by zero\n",
    ")\n",
    "\n",
    "# def loss_function(real, pred):\n",
    "#     mask = tf.math.logical_not(tf.math.equal(real, 0))  # Create a mask for non-zero tokens.\n",
    "#     loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "#     loss_ = loss_object(real, pred)  # Calculate loss using some loss object not defined here.\n",
    "#     mask = tf.cast(mask, dtype=loss_.dtype)  # Cast mask to the same type as the loss.\n",
    "#     loss_ *= mask  # Apply mask to the loss.\n",
    "#     return tf.reduce_sum(loss_) / tf.reduce_sum(mask)  # Calculate the average loss.\n",
    "\n",
    "# def accuracy_function(real, pred):\n",
    "#     accuracies = tf.equal(real, tf.argmax(pred, axis=2))  # Check if real values match predictions.\n",
    "#     mask = tf.math.logical_not(tf.math.equal(real, 0))  # Create a mask for non-zero tokens.\n",
    "#     accuracies = tf.math.logical_and(mask, accuracies)  # Apply mask to accuracies.\n",
    "#     accuracies = tf.cast(accuracies, dtype=tf.float32)  # Cast to float32 for calculation.\n",
    "#     mask = tf.cast(mask, dtype=tf.float32)  # Cast mask to float32.\n",
    "#     return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)  # Calculate the average accuracy.\n",
    "\n",
    "# train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityModel(tf.keras.Model):\n",
    "    def __init__(self, transformer):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "        self.dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')  # Output similarity score between 0 and 1\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        sent1, sent2 = inputs  # each: (batch_size, seq_len)\n",
    "\n",
    "        embed1 = self.transformer(sent1, training=training)  # (batch_size, d_model)\n",
    "        embed2 = self.transformer(sent2, training=training)  # (batch_size, d_model)\n",
    "\n",
    "        # Combine embeddings (common in sentence similarity tasks)\n",
    "        combined = tf.concat([\n",
    "            embed1,\n",
    "            embed2,\n",
    "            tf.abs(embed1 - embed2),\n",
    "            embed1 * embed2\n",
    "        ], axis=1)  # (batch_size, 4 * d_model)\n",
    "\n",
    "        return self.dense(combined)  # (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Instantiate the Transformer model\n",
    "# -------------------------------\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,                        # Number of encoder and decoder layers\n",
    "    d_model=d_model,                              # Embedding size / model dimensionality\n",
    "    num_heads=num_heads,                          # Number of attention heads\n",
    "    dff=dff,                                      # Hidden layer size in feed-forward network\n",
    "    input_vocab_size = tokenizer.vocab_size,\n",
    "    # target_vocab_size = target_tokenizer.vocab_size,\n",
    "    rate=dropout_rate                             # Dropout rate\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Checkpointing: Saving and restoring model state\n",
    "# -------------------------------\n",
    "\n",
    "checkpoint_path = './checkpoints/train'   # Directory to save training checkpoints\n",
    "\n",
    "# Create a checkpoint object that tracks the transformer and optimizer state\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "\n",
    "# Manage multiple checkpoints (e.g., keep the 5 latest ones)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# If a previous checkpoint exists, restore the model and optimizer state\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')\n",
    "\n",
    "# -------------------------------\n",
    "# Define input signature for tf.function (for performance optimization)\n",
    "# Used to decorate the training step function later\n",
    "# -------------------------------\n",
    "# train_step_signature = [\n",
    "#     tf.TensorSpec(shape=(None, None), dtype=tf.int64),  # Input sequence shape: (batch_size, input_seq_len)\n",
    "#     tf.TensorSpec(shape=(None, None), dtype=tf.int64),  # Target sequence shape: (batch_size, target_seq_len)\n",
    "# ]\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),  # sentence1\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),  # sentence2\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.float32),     # similarity label (e.g., 0.0 to 1.0)\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Instantiate the Sentence similarity model\n",
    "# -------------------------------\n",
    "similarity_model = SimilarityModel(transformer)\n",
    "\n",
    "# Initiate loss function and metrics\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Peek at one batch from train_batches\n",
    "print(\"Inspecting batch structure...\")\n",
    "train_batches = make_batches(train_examples)\n",
    "train_iter = iter(train_batches)\n",
    "\n",
    "first = next(train_iter)\n",
    "\n",
    "print(\"Raw output of next(train_iter):\", type(first))\n",
    "print(\"Length of output:\", len(first))\n",
    "\n",
    "# Try printing all contents in the batch\n",
    "for i, item in enumerate(first):\n",
    "    print(f\"\\nElement {i}:\")\n",
    "    print(\"  Type:\", type(item))\n",
    "    print(\"  Shape:\", item.shape)\n",
    "    print(\"  DType:\", item.dtype)\n",
    "    print(\"  Example values:\", item.numpy()[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(sent1, sent2, score):\n",
    "    \"\"\"\n",
    "    Performs a single training step for the similarity model.\n",
    "\n",
    "    Args:\n",
    "    - sent1: Tokenized first sentence (batch_size, seq_len)\n",
    "    - sent2: Tokenized second sentence (batch_size, seq_len)\n",
    "    - score: Continuous similarity score (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "    - loss: Scalar MSE loss\n",
    "    - pred: Predicted similarity score\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = similarity_model((sent1, sent2), training=True)\n",
    "        pred = tf.squeeze(pred, axis=1)  # (batch_size,)\n",
    "        loss = loss_fn(score, pred)\n",
    "\n",
    "    gradients = tape.gradient(loss, similarity_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, similarity_model.trainable_variables))\n",
    "    train_loss.update_state(loss)\n",
    "\n",
    "    return loss, pred\n",
    "\n",
    "    # # Prepare target inputs and outputs\n",
    "    # tar_inp = tar[:, :-1]\n",
    "    # tar_real = tar[:, 1:]\n",
    "\n",
    "    # with tf.GradientTape() as tape:\n",
    "    #     predictions, _ = transformer([inp, tar_inp], training=True)\n",
    "    #     loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    # # Compute gradients and apply them\n",
    "    # gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    # optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    # # Update the training loss and accuracy metrics\n",
    "    # train_loss(loss)\n",
    "    # train_accuracy(accuracy_function(tar_real, predictions))\n",
    "\n",
    "    # return loss, predictions\n",
    "\n",
    "# epoch_accuracies = []\n",
    "# all_batch_accuracies = []\n",
    "\n",
    "EPOCHS = 10\n",
    "steps_per_epoch = len(train_examples) // BATCH_SIZE\n",
    "# steps_per_epoch = 1000  # Set based on dataset size / batch size\n",
    "\n",
    "epoch_losses = []\n",
    "all_batch_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1} -------------------\")\n",
    "    # print(\"Re-initializing dataset iterator\")\n",
    "\n",
    "    # Reset metrics\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "    train_batches = make_batches(train_examples)  # Must return (sent1, sent2, score)\n",
    "    train_iter = iter(train_batches)\n",
    "\n",
    "    batch_losses = []\n",
    "    start = time.time()\n",
    "\n",
    "    # # Peek at 1 batch\n",
    "    # try:\n",
    "    #     sample = next(train_iter)\n",
    "    #     print(\"Batch loaded successfully.\")\n",
    "    # except Exception as e:\n",
    "    #     print(\"Batch loading failed:\", e)\n",
    "\n",
    "\n",
    "    # # Recreate infinite train_batches with .repeat()\n",
    "    # train_batches = make_batches(train_examples)\n",
    "    # train_iter = iter(train_batches)\n",
    "\n",
    "    # batch_accuracies = []\n",
    "\n",
    "    # # Reset metrics\n",
    "    # train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    # train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "    # Use fixed number of steps instead of iterating over the whole dataset\n",
    "    for step in range(steps_per_epoch):\n",
    "        # inp, tar = next(train_iter)\n",
    "        try:\n",
    "            # (batch,) = next(train_iter)  # Unpack the tuple\n",
    "            # sent1, sent2, score = batch\n",
    "            sent1, sent2, score = next(train_iter)\n",
    "\n",
    "\n",
    "        except StopIteration:\n",
    "            # If the iterator is exhausted, recreate it\n",
    "            # train_iter = iter(train_batches)\n",
    "            # (batch,) = next(train_iter)\n",
    "            # sent1, sent2, score = batch\n",
    "\n",
    "            train_iter = iter(train_batches)\n",
    "            sent1, sent2, score = next(train_iter)\n",
    "\n",
    "\n",
    "\n",
    "        # loss, predictions = train_step(inp, tar)\n",
    "        loss, _ = train_step(sent1, sent2, score)\n",
    "\n",
    "        batch_losses.append(loss.numpy())\n",
    "\n",
    "        # batch_accuracies.append(accuracy_function(tar[:, 1:], predictions).numpy())\n",
    "\n",
    "        # if step % 50 == 0:\n",
    "        #     print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f}')\n",
    "\n",
    "\n",
    "    all_batch_losses.append(batch_losses)\n",
    "    epoch_losses.append(train_loss.result().numpy())\n",
    "    # all_batch_accuracies.append(batch_accuracies)\n",
    "\n",
    "    # epoch_accuracies.append(train_accuracy.result().numpy())\n",
    "\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "    # print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "    # print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f}')\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validation set\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "\n",
    "for sent1_val, sent2_val, score_val in val_batches:\n",
    "    pred_val = similarity_model((sent1_val, sent2_val), training=False)\n",
    "    pred_val = tf.squeeze(pred_val, axis=1)\n",
    "    loss = loss_fn(score_val, pred_val)\n",
    "    val_loss.update_state(loss)\n",
    "\n",
    "print(f\"Validation Loss (MSE): {val_loss.result().numpy():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(all_batch_losses[0], label='Batch Loss')\n",
    "#plt.plot(all_batch_accuracies[0], label='Batch Accuracy')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Loss & Accuracy per Batch (Epoch 1)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
