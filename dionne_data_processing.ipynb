{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing \n",
    "Apply NLP techniques such as word embeddings, stemming, lemmatization, and stop-word removal.\n",
    "\n",
    "### Objective\n",
    "An alien civilization has discovered an ancient text file on the abandoned Earth, containing\n",
    "millions of sentence pairs. Their challenge is to decipher how closely related these sentence\n",
    "pairs are, effectively developing a semantic similarity detection model.\n",
    "\n",
    "### Dataset Features\n",
    "- Contains ~1 million lines with paired sentences.\n",
    "- Some sentence pairs share the same meaning, while others differ.\n",
    "- Requires feature engineering and text preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(949080, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>score</th>\n",
       "      <th>lang1</th>\n",
       "      <th>lang2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ein Flugzeug hebt gerade ab.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ein Flugzeug hebt gerade ab.</td>\n",
       "      <td>Un avión está despegando.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ein Flugzeug hebt gerade ab.</td>\n",
       "      <td>Un avion est en train de décoller.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ein Flugzeug hebt gerade ab.</td>\n",
       "      <td>Un aereo sta decollando.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ein Flugzeug hebt gerade ab.</td>\n",
       "      <td>飛行機が離陸します。</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      sentence1                           sentence2  score  \\\n",
       "0  Ein Flugzeug hebt gerade ab.         An air plane is taking off.    5.0   \n",
       "1  Ein Flugzeug hebt gerade ab.           Un avión está despegando.    5.0   \n",
       "2  Ein Flugzeug hebt gerade ab.  Un avion est en train de décoller.    5.0   \n",
       "3  Ein Flugzeug hebt gerade ab.            Un aereo sta decollando.    5.0   \n",
       "4  Ein Flugzeug hebt gerade ab.                          飛行機が離陸します。    5.0   \n",
       "\n",
       "  lang1 lang2  \n",
       "0    de    en  \n",
       "1    de    es  \n",
       "2    de    fr  \n",
       "3    de    it  \n",
       "4    de    ja  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# change this file path \n",
    "file_path = \"/Users/dionnespaltman/Desktop/Luiss /Machine Learning/Project/rs2.csv\"\n",
    "\n",
    "# load the csv as a pandas dataframe \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# print dimensions \n",
    "print(df.shape)\n",
    "\n",
    "# display the first 5 rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan: \n",
    "1. Lowercase all text\n",
    "2. Remove punctuation and special characters\n",
    "3. Remove stop words (for languages where this makes sense)\n",
    "4. Apply stemming or lemmatization\n",
    "5. Tokenization\n",
    "6. Word embeddings (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import spacy\n",
    "from spacy.lang.xx import MultiLanguage\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache loaded spaCy models\n",
    "loaded_models = {}\n",
    "\n",
    "def load_spacy_model(lang_code):\n",
    "    models = {\n",
    "        \"en\": \"en_core_web_sm\",\n",
    "        \"de\": \"de_core_news_sm\",\n",
    "        \"fr\": \"fr_core_news_sm\",\n",
    "        \"es\": \"es_core_news_sm\",\n",
    "        \"it\": \"it_core_news_sm\",\n",
    "        \"pt\": \"pt_core_news_sm\",\n",
    "        \"nl\": \"nl_core_news_sm\",\n",
    "        # Add more if needed\n",
    "    }\n",
    "    model_name = models.get(lang_code)\n",
    "    if model_name:\n",
    "        try:\n",
    "            return spacy.load(model_name)\n",
    "        except:\n",
    "            print(f\"⚠️ spaCy model {model_name} not found.\")\n",
    "            return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function \n",
    "# Lowercase, lemmatize, remove punctuation, remove stopwords\n",
    "def preprocess(text, lang_code):\n",
    "    nlp = load_spacy_model(lang_code)\n",
    "    if not nlp:\n",
    "        return text.lower()  # fallback\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if token.is_alpha and not token.is_stop\n",
    "    ]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have to run the following code in your terminal to make it work\n",
    "\n",
    "# python -m spacy download de_core_news_sm\n",
    "# python -m spacy download es_core_news_sm\n",
    "# python -m spacy download fr_core_news_sm\n",
    "# python -m spacy download it_core_news_sm\n",
    "# python -m spacy download ja_core_news_sm\n",
    "# python -m spacy download pt_core_news_sm\n",
    "# python -m spacy download nl_core_news_sm\n",
    "\n",
    "\n",
    "# also run \n",
    "# python -m spacy download de_core_news_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flugzeug heben\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(preprocess(\"Ein Flugzeug hebt gerade ab.\", \"de\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN | Original: The airplane is taking off.\n",
      "         Preprocessed: airplane take\n",
      "\n",
      "DE | Original: Ein Flugzeug hebt gerade ab.\n",
      "         Preprocessed: Flugzeug heben\n",
      "\n",
      "ES | Original: Un avión está despegando.\n",
      "         Preprocessed: avión despegar\n",
      "\n",
      "FR | Original: Un avion est en train de décoller.\n",
      "         Preprocessed: avion train décoller\n",
      "\n",
      "IT | Original: Un aereo sta decollando.\n",
      "         Preprocessed: aereo decollare\n",
      "\n",
      "PT | Original: Um avião está decolando.\n",
      "         Preprocessed: avião decolar\n",
      "\n",
      "NL | Original: Een vliegtuig is aan het opstijgen.\n",
      "         Preprocessed: vliegtuig opstijgen\n",
      "\n",
      "PL | Original: Samolot właśnie startuje.\n",
      "         Preprocessed: samolot właśnie startuje.\n",
      "\n",
      "RU | Original: Самолет взлетает.\n",
      "         Preprocessed: самолет взлетает.\n",
      "\n",
      "JA | Original: 飛行機が離陸します。\n",
      "         Preprocessed: 飛行機が離陸します。\n",
      "\n",
      "ZH | Original: 飞机正在起飞。\n",
      "         Preprocessed: 飞机正在起飞。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = {\n",
    "    \"en\": \"The airplane is taking off.\",\n",
    "    \"de\": \"Ein Flugzeug hebt gerade ab.\",\n",
    "    \"es\": \"Un avión está despegando.\",\n",
    "    \"fr\": \"Un avion est en train de décoller.\",\n",
    "    \"it\": \"Un aereo sta decollando.\",\n",
    "    \"pt\": \"Um avião está decolando.\",\n",
    "    \"nl\": \"Een vliegtuig is aan het opstijgen.\",\n",
    "    \"pl\": \"Samolot właśnie startuje.\",\n",
    "    \"ru\": \"Самолет взлетает.\",\n",
    "    \"ja\": \"飛行機が離陸します。\",\n",
    "    \"zh\": \"飞机正在起飞。\"\n",
    "}\n",
    "\n",
    "for lang, sentence in test_sentences.items():\n",
    "    print(f\"{lang.upper()} | Original: {sentence}\")\n",
    "    print(f\"         Preprocessed: {preprocess(sentence, lang)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy is not working for polish, russian, japanese and chinese. So we need to find a different solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the following in your terminal\n",
    "# pip install jieba\n",
    "# pip install spacy[ja]\n",
    "# python -m spacy download ja_core_news_sm\n",
    "# pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def preprocess_zh(text):\n",
    "    tokens = jieba.lcut(text)\n",
    "    # Optional: remove stopwords if you have a list\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_ja = spacy.load(\"ja_core_news_sm\")\n",
    "\n",
    "def preprocess_ja(text):\n",
    "    doc = nlp_ja(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run the code below you need to downgrade your pytorch version\n",
    "# run in your terminal \n",
    "# pip install torch==2.1.2\n",
    "\n",
    "# import torch\n",
    "# print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92baaa25c57421d91d3a5585999efc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:11:13 INFO: Downloaded file to /Users/dionnespaltman/stanza_resources/resources.json\n",
      "2025-04-10 10:11:13 INFO: Downloading default packages for language: ru (Russian) ...\n",
      "2025-04-10 10:11:14 INFO: File exists: /Users/dionnespaltman/stanza_resources/ru/default.zip\n",
      "2025-04-10 10:11:20 INFO: Finished downloading models and saved to /Users/dionnespaltman/stanza_resources\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ac58367a89431e859bac2a11a05d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:11:21 INFO: Downloaded file to /Users/dionnespaltman/stanza_resources/resources.json\n",
      "2025-04-10 10:11:21 INFO: Downloading default packages for language: pl (Polish) ...\n",
      "2025-04-10 10:11:22 INFO: File exists: /Users/dionnespaltman/stanza_resources/pl/default.zip\n",
      "2025-04-10 10:11:25 INFO: Finished downloading models and saved to /Users/dionnespaltman/stanza_resources\n",
      "2025-04-10 10:11:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4d4feb8495495dadfd409063af3bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:11:29 INFO: Downloaded file to /Users/dionnespaltman/stanza_resources/resources.json\n",
      "2025-04-10 10:11:29 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "==================================\n",
      "\n",
      "2025-04-10 10:11:29 INFO: Using device: cpu\n",
      "2025-04-10 10:11:29 INFO: Loading: tokenize\n",
      "/Users/dionnespaltman/anaconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2025-04-10 10:11:30 INFO: Loading: lemma\n",
      "2025-04-10 10:11:34 INFO: Done loading processors!\n",
      "2025-04-10 10:11:34 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd427d7a59f4625b1aaa03b945dce70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:11:34 INFO: Downloaded file to /Users/dionnespaltman/stanza_resources/resources.json\n",
      "2025-04-10 10:11:34 WARNING: Language pl package default expects mwt, which has been added\n",
      "2025-04-10 10:11:34 INFO: Loading these models for language: pl (Polish):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | pdb          |\n",
      "| mwt       | pdb          |\n",
      "| lemma     | pdb_nocharlm |\n",
      "============================\n",
      "\n",
      "2025-04-10 10:11:34 INFO: Using device: cpu\n",
      "2025-04-10 10:11:34 INFO: Loading: tokenize\n",
      "2025-04-10 10:11:34 INFO: Loading: mwt\n",
      "2025-04-10 10:11:34 INFO: Loading: lemma\n",
      "2025-04-10 10:11:36 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "stanza.download(\"ru\")  # Russian\n",
    "stanza.download(\"pl\")  # Polish\n",
    "\n",
    "nlp_ru = stanza.Pipeline(\"ru\", processors=\"tokenize,lemma\", use_gpu=False)\n",
    "nlp_pl = stanza.Pipeline(\"pl\", processors=\"tokenize,lemma\", use_gpu=False)\n",
    "\n",
    "def preprocess_ru(text):\n",
    "    doc = nlp_ru(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.lemma.isalpha()]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def preprocess_pl(text):\n",
    "    doc = nlp_pl(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.lemma.isalpha()]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, lang_code):\n",
    "    if lang_code == \"zh\":\n",
    "        return preprocess_zh(text)\n",
    "    elif lang_code == \"ja\":\n",
    "        return preprocess_ja(text)\n",
    "    elif lang_code == \"ru\":\n",
    "        return preprocess_ru(text)\n",
    "    elif lang_code == \"pl\":\n",
    "        return preprocess_pl(text)\n",
    "    else:\n",
    "        # fallback to spaCy\n",
    "        nlp = load_spacy_model(lang_code)\n",
    "        if not nlp:\n",
    "            return text.lower()\n",
    "        doc = nlp(text.lower())\n",
    "        tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "        return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN | Original: The airplane is taking off.\n",
      "         Preprocessed: airplane take\n",
      "\n",
      "DE | Original: Ein Flugzeug hebt gerade ab.\n",
      "         Preprocessed: Flugzeug heben\n",
      "\n",
      "ES | Original: Un avión está despegando.\n",
      "         Preprocessed: avión despegar\n",
      "\n",
      "FR | Original: Un avion est en train de décoller.\n",
      "         Preprocessed: avion train décoller\n",
      "\n",
      "IT | Original: Un aereo sta decollando.\n",
      "         Preprocessed: aereo decollare\n",
      "\n",
      "PT | Original: Um avião está decolando.\n",
      "         Preprocessed: avião decolar\n",
      "\n",
      "NL | Original: Een vliegtuig is aan het opstijgen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/_4/nzq6mygj7j71_l3z_c9kc7wr0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Preprocessed: vliegtuig opstijgen\n",
      "\n",
      "PL | Original: Samolot właśnie startuje.\n",
      "         Preprocessed: samolot właśnie startuje\n",
      "\n",
      "RU | Original: Самолет взлетает.\n",
      "         Preprocessed: самолет взлетать\n",
      "\n",
      "JA | Original: 飛行機が離陸します。\n",
      "         Preprocessed: 飛行 機 が 離陸 する ます\n",
      "\n",
      "ZH | Original: 飞机正在起飞。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.670 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Preprocessed: 飞机 正在 起飞 。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = {\n",
    "    \"en\": \"The airplane is taking off.\",\n",
    "    \"de\": \"Ein Flugzeug hebt gerade ab.\",\n",
    "    \"es\": \"Un avión está despegando.\",\n",
    "    \"fr\": \"Un avion est en train de décoller.\",\n",
    "    \"it\": \"Un aereo sta decollando.\",\n",
    "    \"pt\": \"Um avião está decolando.\",\n",
    "    \"nl\": \"Een vliegtuig is aan het opstijgen.\",\n",
    "    \"pl\": \"Samolot właśnie startuje.\",\n",
    "    \"ru\": \"Самолет взлетает.\",\n",
    "    \"ja\": \"飛行機が離陸します。\",\n",
    "    \"zh\": \"飞机正在起飞。\"\n",
    "}\n",
    "\n",
    "for lang, sentence in test_sentences.items():\n",
    "    print(f\"{lang.upper()} | Original: {sentence}\")\n",
    "    print(f\"         Preprocessed: {preprocess(sentence, lang)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test is not working for all the languages yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing sentence1:   0%|          | 39/949080 [00:36<375:25:14,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ spaCy model fr_core_news_sm not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing sentence1:   0%|          | 66/949080 [00:53<179:04:53,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ spaCy model nl_core_news_sm not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing sentence1:   0%|          | 88/949080 [01:05<117:52:34,  2.24it/s]"
     ]
    }
   ],
   "source": [
    "# # Apply preprocessing\n",
    "# tqdm.pandas(desc=\"Preprocessing sentence1\")\n",
    "# df[\"sentence1_clean\"] = df.progress_apply(\n",
    "#     lambda row: preprocess(row[\"sentence1\"], row[\"lang1\"]), axis=1\n",
    "# )\n",
    "\n",
    "# tqdm.pandas(desc=\"Preprocessing sentence2\")\n",
    "# df[\"sentence2_clean\"] = df.progress_apply(\n",
    "#     lambda row: preprocess(row[\"sentence2\"], row[\"lang2\"]), axis=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete! Cleaned data saved to 'rs2_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# # Save or preview\n",
    "# file_path_clean = \"/Users/dionnespaltman/Desktop/Luiss /Machine Learning/Project/rs2_cleaned.csv\"\n",
    "# df.to_csv(file_path_clean, index=False)\n",
    "# print(\"✅ Preprocessing complete! Cleaned data saved to 'rs2_cleaned.csv'\")\n",
    "\n",
    "# # change this file path \n",
    "# file_path_clean = \"/Users/dionnespaltman/Desktop/Luiss /Machine Learning/Project/rs2_cleaned.csv\"\n",
    "\n",
    "# # load the csv as a pandas dataframe \n",
    "# df = pd.read_csv(file_path_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
