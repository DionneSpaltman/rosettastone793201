{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Daniele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>score</th>\n",
       "      <th>lang1</th>\n",
       "      <th>lang2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ein Flugzeug hebt gerade ab.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ein Flugzeug hebt gerade ab.</td>\n",
       "      <td>Un avión está despegando.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ein Flugzeug hebt gerade ab.</td>\n",
       "      <td>Un avion est en train de décoller.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ein Flugzeug hebt gerade ab.</td>\n",
       "      <td>Un aereo sta decollando.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ein Flugzeug hebt gerade ab.</td>\n",
       "      <td>飛行機が離陸します。</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      sentence1                           sentence2  score  \\\n",
       "0  Ein Flugzeug hebt gerade ab.         An air plane is taking off.    5.0   \n",
       "1  Ein Flugzeug hebt gerade ab.           Un avión está despegando.    5.0   \n",
       "2  Ein Flugzeug hebt gerade ab.  Un avion est en train de décoller.    5.0   \n",
       "3  Ein Flugzeug hebt gerade ab.            Un aereo sta decollando.    5.0   \n",
       "4  Ein Flugzeug hebt gerade ab.                          飛行機が離陸します。    5.0   \n",
       "\n",
       "  lang1 lang2  \n",
       "0    de    en  \n",
       "1    de    es  \n",
       "2    de    fr  \n",
       "3    de    it  \n",
       "4    de    ja  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/dionnespaltman/Desktop/Luiss /Machine Learning/Project/rs2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(949080, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "de    86280\n",
       "en    86280\n",
       "es    86280\n",
       "fr    86280\n",
       "it    86280\n",
       "ja    86280\n",
       "nl    86280\n",
       "pl    86280\n",
       "pt    86280\n",
       "ru    86280\n",
       "zh    86280\n",
       "Name: lang1, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check what languages are in the dataset\n",
    "df['lang1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    86280\n",
       "es    86280\n",
       "fr    86280\n",
       "it    86280\n",
       "ja    86280\n",
       "nl    86280\n",
       "pl    86280\n",
       "pt    86280\n",
       "ru    86280\n",
       "zh    86280\n",
       "de    86280\n",
       "Name: lang2, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lang2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment \n",
    "1. Data Processing \n",
    " Apply NLP techniques such as word embeddings, stemming, lemmatization, and stop-word removal. \n",
    "2. Data Augmentation o Enhance the dataset using paraphrasing techniques to improve model robustness. \n",
    "3. Model Development o Design a Neural Network for sentence similarity measurement. o Experiment with different architectures (e.g., transformers, LSTMs). \n",
    "4. Evaluation & Reporting o Test the model on unseen data and analyze performance. o Prepare a comprehensive report on methodology and insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing \n",
    "The plan: \n",
    "1. Lowercase all text\n",
    "2. Remove punctuation and special characters\n",
    "3. Remove stop words (for languages where this makes sense)\n",
    "4. Apply stemming or lemmatization\n",
    "5. Tokenization\n",
    "6. Word embeddings (later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggested order: \n",
    "1. Tokenization\n",
    "2. lemmatization \n",
    "3. stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>score</th>\n",
       "      <th>lang1</th>\n",
       "      <th>lang2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ein flugzeug hebt gerade ab.</td>\n",
       "      <td>an air plane is taking off.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ein flugzeug hebt gerade ab.</td>\n",
       "      <td>un avión está despegando.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ein flugzeug hebt gerade ab.</td>\n",
       "      <td>un avion est en train de décoller.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ein flugzeug hebt gerade ab.</td>\n",
       "      <td>un aereo sta decollando.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ein flugzeug hebt gerade ab.</td>\n",
       "      <td>飛行機が離陸します。</td>\n",
       "      <td>5.0</td>\n",
       "      <td>de</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      sentence1                           sentence2  score  \\\n",
       "0  ein flugzeug hebt gerade ab.         an air plane is taking off.    5.0   \n",
       "1  ein flugzeug hebt gerade ab.           un avión está despegando.    5.0   \n",
       "2  ein flugzeug hebt gerade ab.  un avion est en train de décoller.    5.0   \n",
       "3  ein flugzeug hebt gerade ab.            un aereo sta decollando.    5.0   \n",
       "4  ein flugzeug hebt gerade ab.                          飛行機が離陸します。    5.0   \n",
       "\n",
       "  lang1 lang2  \n",
       "0    de    en  \n",
       "1    de    es  \n",
       "2    de    fr  \n",
       "3    de    it  \n",
       "4    de    ja  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "def lower_sentences(sentence):\n",
    "    if sentence not in ['ja', 'zh']:\n",
    "        return sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "df['sentence1'] = df['sentence1'].apply(lower_sentences)\n",
    "df['sentence2'] = df['sentence2'].apply(lower_sentences)\n",
    "\n",
    "df1 = df.copy()\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>score</th>\n",
       "      <th>lang1</th>\n",
       "      <th>lang2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>884436</th>\n",
       "      <td>je ne pense pas quil y en ait</td>\n",
       "      <td>nie wydaje mi się żeby były jakieś korzyści</td>\n",
       "      <td>3.0</td>\n",
       "      <td>fr</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475587</th>\n",
       "      <td>ロムニー氏がプエルトリコで大勝</td>\n",
       "      <td>romney ganha as primárias de illinois</td>\n",
       "      <td>1.5</td>\n",
       "      <td>ja</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488176</th>\n",
       "      <td>警察在贝尔法斯特国旗抗议活动中遭到袭击</td>\n",
       "      <td>politici in vlaggengeweldgesprekken</td>\n",
       "      <td>0.6</td>\n",
       "      <td>zh</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433773</th>\n",
       "      <td>strausskahn indagato nel giro della prostituzi...</td>\n",
       "      <td>dominique strausskahn sinterroge sur le réseau...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>it</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786629</th>\n",
       "      <td>four killed scores wounded in clashes across e...</td>\n",
       "      <td>埃及暴力事件 学生在开罗伊斯兰大学的冲突中丧生</td>\n",
       "      <td>2.2</td>\n",
       "      <td>en</td>\n",
       "      <td>zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669623</th>\n",
       "      <td>ノートパソコンとパソコンのモニター</td>\n",
       "      <td>un ordinateur portable et un pc sur un poste d...</td>\n",
       "      <td>2.6</td>\n",
       "      <td>ja</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163377</th>\n",
       "      <td>gato doméstico acostado en la parte posterior ...</td>\n",
       "      <td>gato doméstico bronzeado debaixo de lona preta</td>\n",
       "      <td>2.0</td>\n",
       "      <td>es</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662600</th>\n",
       "      <td>pociąg węglowy jadący w dół torów</td>\n",
       "      <td>ein alter zug sitzt untätig auf den gleisen</td>\n",
       "      <td>2.0</td>\n",
       "      <td>pl</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264746</th>\n",
       "      <td>não tem restos de um míssil no pentágono</td>\n",
       "      <td>je hebt geen overblijfselen van een a bij het ...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>pt</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383407</th>\n",
       "      <td>アナトリーソコロフが発表した モスクワ周辺のソ連製 a135 ミサイル防衛システムは時代遅れ...</td>\n",
       "      <td>o sistema de defesa antimíssil soviético const...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>ja</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence1  \\\n",
       "884436                      je ne pense pas quil y en ait   \n",
       "475587                                    ロムニー氏がプエルトリコで大勝   \n",
       "488176                                警察在贝尔法斯特国旗抗议活动中遭到袭击   \n",
       "433773  strausskahn indagato nel giro della prostituzi...   \n",
       "786629  four killed scores wounded in clashes across e...   \n",
       "669623                                  ノートパソコンとパソコンのモニター   \n",
       "163377  gato doméstico acostado en la parte posterior ...   \n",
       "662600                  pociąg węglowy jadący w dół torów   \n",
       "264746           não tem restos de um míssil no pentágono   \n",
       "383407  アナトリーソコロフが発表した モスクワ周辺のソ連製 a135 ミサイル防衛システムは時代遅れ...   \n",
       "\n",
       "                                                sentence2  score lang1 lang2  \n",
       "884436        nie wydaje mi się żeby były jakieś korzyści    3.0    fr    pl  \n",
       "475587              romney ganha as primárias de illinois    1.5    ja    pt  \n",
       "488176                politici in vlaggengeweldgesprekken    0.6    zh    nl  \n",
       "433773  dominique strausskahn sinterroge sur le réseau...    3.8    it    fr  \n",
       "786629                            埃及暴力事件 学生在开罗伊斯兰大学的冲突中丧生    2.2    en    zh  \n",
       "669623  un ordinateur portable et un pc sur un poste d...    2.6    ja    fr  \n",
       "163377     gato doméstico bronzeado debaixo de lona preta    2.0    es    pt  \n",
       "662600        ein alter zug sitzt untätig auf den gleisen    2.0    pl    de  \n",
       "264746  je hebt geen overblijfselen van een a bij het ...    2.4    pt    nl  \n",
       "383407  o sistema de defesa antimíssil soviético const...    4.6    ja    pt  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2\n",
    "def remove_special_characters(text):\n",
    "    if isinstance(text, str):  \n",
    "        return re.sub(r'[^\\w\\s]', '', text) \n",
    "    return text \n",
    "\n",
    "df['sentence1'] = df['sentence1'].apply(remove_special_characters)\n",
    "df['sentence2'] = df['sentence2'].apply(remove_special_characters)\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install ginza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'ja_ginza'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjieba\u001b[39;00m\n\u001b[1;32m      4\u001b[0m nlp_en \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m nlp_ja \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mja_ginza\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# nlp_ja = spacy.load(\"ja_ginza_electra\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m nlp_de \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde_core_news_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'ja_ginza'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "#Tokenization \n",
    "import spacy\n",
    "import jieba\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_ja = spacy.load(\"ja_ginza\")\n",
    "# nlp_ja = spacy.load(\"ja_ginza_electra\")\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
    "nlp_it = spacy.load(\"it_core_news_sm\")\n",
    "nlp_pt = spacy.load(\"pt_core_news_sm\")\n",
    "nlp_nl = spacy.load(\"nl_core_news_sm\")\n",
    "nlp_pl = spacy.load(\"pl_core_news_sm\")\n",
    "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "nlp_models = {\n",
    "    \"en\": nlp_en,\n",
    "    \"ja\": nlp_ja,\n",
    "    \"de\": nlp_de,\n",
    "    \"es\": nlp_es,\n",
    "    \"fr\": nlp_fr,\n",
    "    \"it\": nlp_it,\n",
    "    \"pt\": nlp_pt,\n",
    "    \"nl\": nlp_nl,\n",
    "    \"pl\": nlp_pl,\n",
    "    \"ru\": nlp_ru\n",
    "}\n",
    "\n",
    "def tokenize_sentence(sentence, lang, nlp_models):\n",
    "    if lang == \"zh\":\n",
    "        tokens = jieba.cut(sentence)\n",
    "        return \" \".join(tokens)\n",
    "    elif lang in nlp_models:\n",
    "        nlp = nlp_models[lang]\n",
    "        doc = nlp(sentence)\n",
    "        return \" \".join([token.text for token in doc])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {lang}\")\n",
    "\n",
    "df['processed_language1'] = df.apply(\n",
    "    lambda row: tokenize_sentence(row['sentence1'], row['lang1'], nlp_models), axis=1)\n",
    "\n",
    "df['processed_language2'] = df.apply(\n",
    "    lambda row: tokenize_sentence(row['sentence2'], row['lang2'], nlp_models), axis=1)\n",
    "\n",
    "\n",
    "df[['sentence1', 'processed_language1', 'sentence2', 'processed_language2']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tokenized_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_language1</th>\n",
       "      <th>processed_language2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ein Flugzeug heben gerade ab</td>\n",
       "      <td>an air plane be take off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ein Flugzeug heben gerade ab</td>\n",
       "      <td>uno avión estar despegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ein Flugzeug heben gerade ab</td>\n",
       "      <td>un avion être en train de décoller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ein Flugzeug heben gerade ab</td>\n",
       "      <td>uno aereo stare decollare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ein Flugzeug heben gerade ab</td>\n",
       "      <td>飛行機 が 離陸 する ます</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            processed_language1                 processed_language2\n",
       "0  ein Flugzeug heben gerade ab            an air plane be take off\n",
       "1  ein Flugzeug heben gerade ab            uno avión estar despegar\n",
       "2  ein Flugzeug heben gerade ab  un avion être en train de décoller\n",
       "3  ein Flugzeug heben gerade ab           uno aereo stare decollare\n",
       "4  ein Flugzeug heben gerade ab                      飛行機 が 離陸 する ます"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization \n",
    "\n",
    "def lemmatize_sentence(sentence, lang, nlp_models):\n",
    "    if lang == \"zh\":\n",
    "        # For Chinese, jieba does not provide lemmatization, so we return the tokenized sentence as is\n",
    "        tokens = jieba.cut(sentence)\n",
    "        return \" \".join(tokens)\n",
    "    elif lang in nlp_models:\n",
    "        nlp = nlp_models[lang]\n",
    "        doc = nlp(sentence)\n",
    "        return \" \".join([token.lemma_ for token in doc])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {lang}\")\n",
    "\n",
    "df['processed_language1'] = df.apply(\n",
    "    lambda row: lemmatize_sentence(row['processed_language1'], row['lang1'], nlp_models), axis=1)\n",
    "\n",
    "df['processed_language2'] = df.apply(\n",
    "    lambda row: lemmatize_sentence(row['processed_language2'], row['lang2'], nlp_models), axis=1)\n",
    "\n",
    "df[['processed_language1', 'processed_language2']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('lemmatization_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_language1</th>\n",
       "      <th>processed_language2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flugzeug heben</td>\n",
       "      <td>air plane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flugzeug heben</td>\n",
       "      <td>avión despegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flugzeug heben</td>\n",
       "      <td>avion train décoller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flugzeug heben</td>\n",
       "      <td>aereo stare decollare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flugzeug heben</td>\n",
       "      <td>飛行機 離陸</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  processed_language1    processed_language2\n",
       "0      Flugzeug heben              air plane\n",
       "1      Flugzeug heben         avión despegar\n",
       "2      Flugzeug heben   avion train décoller\n",
       "3      Flugzeug heben  aereo stare decollare\n",
       "4      Flugzeug heben                 飛行機 離陸"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopword removal\n",
    "#Chinese stopwords got from this github: https://github.com/stopwords-iso/stopwords-zh/blob/master/stopwords-zh.json\n",
    "chinese_stopwords = set([\"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"正在\",\"一\",\"一个\",\"一些\",\"一何\",\"一切\",\"一则\",\"一方面\",\"一旦\",\"一来\",\"一样\",\"一种\",\"一般\",\"一转眼\",\"七\",\"万一\",\"三\",\"上\",\"上下\",\"下\",\"不\",\"不仅\",\"不但\",\"不光\",\"不单\",\"不只\",\"不外乎\",\"不如\",\"不妨\",\"不尽\",\"不尽然\",\"不得\",\"不怕\",\"不惟\",\"不成\",\"不拘\",\"不料\",\"不是\",\"不比\",\"不然\",\"不特\",\"不独\",\"不管\",\"不至于\",\"不若\",\"不论\",\"不过\",\"不问\",\"与\",\"与其\",\"与其说\",\"与否\",\"与此同时\",\"且\",\"且不说\",\"且说\",\"两者\",\"个\",\"个别\",\"中\",\"临\",\"为\",\"为了\",\"为什么\",\"为何\",\"为止\",\"为此\",\"为着\",\"乃\",\"乃至\",\"乃至于\",\"么\",\"之\",\"之一\",\"之所以\",\"之类\",\"乌乎\",\"乎\",\"乘\",\"九\",\"也\",\"也好\",\"也罢\",\"了\",\"二\",\"二来\",\"于\",\"于是\",\"于是乎\",\"云云\",\"云尔\",\"五\",\"些\",\"亦\",\"人\",\"人们\",\"人家\",\"什\",\"什么\",\"什么样\",\"今\",\"介于\",\"仍\",\"仍旧\",\"从\",\"从此\",\"从而\",\"他\",\"他人\",\"他们\",\"他们们\",\"以\",\"以上\",\"以为\",\"以便\",\"以免\",\"以及\",\"以故\",\"以期\",\"以来\",\"以至\",\"以至于\",\"以致\",\"们\",\"任\",\"任何\",\"任凭\",\"会\",\"似的\",\"但\",\"但凡\",\"但是\",\"何\",\"何以\",\"何况\",\"何处\",\"何时\",\"余外\",\"作为\",\"你\",\"你们\",\"使\",\"使得\",\"例如\",\"依\",\"依据\",\"依照\",\"便于\",\"俺\",\"俺们\",\"倘\",\"倘使\",\"倘或\",\"倘然\",\"倘若\",\"借\",\"借傥然\",\"假使\",\"假如\",\"假若\",\"做\",\"像\",\"儿\",\"先不先\",\"光\",\"光是\",\"全体\",\"全部\",\"八\",\"六\",\"兮\",\"共\",\"关于\",\"关于具体地说\",\"其\",\"其一\",\"其中\",\"其二\",\"其他\",\"其余\",\"其它\",\"其次\",\"具体地说\",\"具体说来\",\"兼之\",\"内\",\"再\",\"再其次\",\"再则\",\"再有\",\"再者\",\"再者说\",\"再说\",\"冒\",\"冲\",\"况且\",\"几\",\"几时\",\"凡\",\"凡是\",\"凭\",\"凭借\",\"出于\",\"出来\",\"分\",\"分别\",\"则\",\"则甚\",\"别\",\"别人\",\"别处\",\"别是\",\"别的\",\"别管\",\"别说\",\"到\",\"前后\",\"前此\",\"前者\",\"加之\",\"加以\",\"区\",\"即\",\"即令\",\"即使\",\"即便\",\"即如\",\"即或\",\"即若\",\"却\",\"去\",\"又\",\"又及\",\"及\",\"及其\",\"及至\",\"反之\",\"反而\",\"反过来\",\"反过来说\",\"受到\",\"另\",\"另一方面\",\"另外\",\"另悉\",\"只\",\"只当\",\"只怕\",\"只是\",\"只有\",\"只消\",\"只要\",\"只限\",\"叫\",\"叮咚\",\"可\",\"可以\",\"可是\",\"可见\",\"各\",\"各个\",\"各位\",\"各种\",\"各自\",\"同\",\"同时\",\"后\",\"后者\",\"向\",\"向使\",\"向着\",\"吓\",\"吗\",\"否则\",\"吧\",\"吧哒\",\"含\",\"吱\",\"呀\",\"呃\",\"呕\",\"呗\",\"呜\",\"呜呼\",\"呢\",\"呵\",\"呵呵\",\"呸\",\"呼哧\",\"咋\",\"和\",\"咚\",\"咦\",\"咧\",\"咱\",\"咱们\",\"咳\",\"哇\",\"哈\",\"哈哈\",\"哉\",\"哎\",\"哎呀\",\"哎哟\",\"哗\",\"哟\",\"哦\",\"哩\",\"哪\",\"哪个\",\"哪些\",\"哪儿\",\"哪天\",\"哪年\",\"哪怕\",\"哪样\",\"哪边\",\"哪里\",\"哼\",\"哼唷\",\"唉\",\"唯有\",\"啊\",\"啐\",\"啥\",\"啦\",\"啪达\",\"啷当\",\"喂\",\"喏\",\"喔唷\",\"喽\",\"嗡\",\"嗡嗡\",\"嗬\",\"嗯\",\"嗳\",\"嘎\",\"嘎登\",\"嘘\",\"嘛\",\"嘻\",\"嘿\",\"嘿嘿\",\"四\",\"因\",\"因为\",\"因了\",\"因此\",\"因着\",\"因而\",\"固然\",\"在\",\"在下\",\"在于\",\"地\",\"基于\",\"处在\",\"多\",\"多么\",\"多少\",\"大\",\"大家\",\"她\",\"她们\",\"好\",\"如\",\"如上\",\"如上所述\",\"如下\",\"如何\",\"如其\",\"如同\",\"如是\",\"如果\",\"如此\",\"如若\",\"始而\",\"孰料\",\"孰知\",\"宁\",\"宁可\",\"宁愿\",\"宁肯\",\"它\",\"它们\",\"对\",\"对于\",\"对待\",\"对方\",\"对比\",\"将\",\"小\",\"尔\",\"尔后\",\"尔尔\",\"尚且\",\"就\",\"就是\",\"就是了\",\"就是说\",\"就算\",\"就要\",\"尽\",\"尽管\",\"尽管如此\",\"岂但\",\"己\",\"已\",\"已矣\",\"巴\",\"巴巴\",\"年\",\"并\",\"并且\",\"庶乎\",\"庶几\",\"开外\",\"开始\",\"归\",\"归齐\",\"当\",\"当地\",\"当然\",\"当着\",\"彼\",\"彼时\",\"彼此\",\"往\",\"待\",\"很\",\"得\",\"得了\",\"怎\",\"怎么\",\"怎么办\",\"怎么样\",\"怎奈\",\"怎样\",\"总之\",\"总的来看\",\"总的来说\",\"总的说来\",\"总而言之\",\"恰恰相反\",\"您\",\"惟其\",\"慢说\",\"我\",\"我们\",\"或\",\"或则\",\"或是\",\"或曰\",\"或者\",\"截至\",\"所\",\"所以\",\"所在\",\"所幸\",\"所有\",\"才\",\"才能\",\"打\",\"打从\",\"把\",\"抑或\",\"拿\",\"按\",\"按照\",\"换句话说\",\"换言之\",\"据\",\"据此\",\"接着\",\"故\",\"故此\",\"故而\",\"旁人\",\"无\",\"无宁\",\"无论\",\"既\",\"既往\",\"既是\",\"既然\",\"日\",\"时\",\"时候\",\"是\",\"是以\",\"是的\",\"更\",\"曾\",\"替\",\"替代\",\"最\",\"月\",\"有\",\"有些\",\"有关\",\"有及\",\"有时\",\"有的\",\"望\",\"朝\",\"朝着\",\"本\",\"本人\",\"本地\",\"本着\",\"本身\",\"来\",\"来着\",\"来自\",\"来说\",\"极了\",\"果然\",\"果真\",\"某\",\"某个\",\"某些\",\"某某\",\"根据\",\"欤\",\"正值\",\"正如\",\"正巧\",\"正是\",\"此\",\"此地\",\"此处\",\"此外\",\"此时\",\"此次\",\"此间\",\"毋宁\",\"每\",\"每当\",\"比\",\"比及\",\"比如\",\"比方\",\"没奈何\",\"沿\",\"沿着\",\"漫说\",\"点\",\"焉\",\"然则\",\"然后\",\"然而\",\"照\",\"照着\",\"犹且\",\"犹自\",\"甚且\",\"甚么\",\"甚或\",\"甚而\",\"甚至\",\"甚至于\",\"用\",\"用来\",\"由\",\"由于\",\"由是\",\"由此\",\"由此可见\",\"的\",\"的确\",\"的话\",\"直到\",\"相对而言\",\"省得\",\"看\",\"眨眼\",\"着\",\"着呢\",\"矣\",\"矣乎\",\"矣哉\",\"离\",\"秒\",\"称\",\"竟而\",\"第\",\"等\",\"等到\",\"等等\",\"简言之\",\"管\",\"类如\",\"紧接着\",\"纵\",\"纵令\",\"纵使\",\"纵然\",\"经\",\"经过\",\"结果\",\"给\",\"继之\",\"继后\",\"继而\",\"综上所述\",\"罢了\",\"者\",\"而\",\"而且\",\"而况\",\"而后\",\"而外\",\"而已\",\"而是\",\"而言\",\"能\",\"能否\",\"腾\",\"自\",\"自个儿\",\"自从\",\"自各儿\",\"自后\",\"自家\",\"自己\",\"自打\",\"自身\",\"至\",\"至于\",\"至今\",\"至若\",\"致\",\"般的\",\"若\",\"若夫\",\"若是\",\"若果\",\"若非\",\"莫不然\",\"莫如\",\"莫若\",\"虽\",\"虽则\",\"虽然\",\"虽说\",\"被\",\"要\",\"要不\",\"要不是\",\"要不然\",\"要么\",\"要是\",\"譬喻\",\"譬如\",\"让\",\"许多\",\"论\",\"设使\",\"设或\",\"设若\",\"诚如\",\"诚然\",\"该\",\"说\",\"说来\",\"请\",\"诸\",\"诸位\",\"诸如\",\"谁\",\"谁人\",\"谁料\",\"谁知\",\"贼死\",\"赖以\",\"赶\",\"起\",\"起见\",\"趁\",\"趁着\",\"越是\",\"距\",\"跟\",\"较\",\"较之\",\"边\",\"过\",\"还\",\"还是\",\"还有\",\"还要\",\"这\",\"这一来\",\"这个\",\"这么\",\"这么些\",\"这么样\",\"这么点儿\",\"这些\",\"这会儿\",\"这儿\",\"这就是说\",\"这时\",\"这样\",\"这次\",\"这般\",\"这边\",\"这里\",\"进而\",\"连\",\"连同\",\"逐步\",\"通过\",\"遵循\",\"遵照\",\"那\",\"那个\",\"那么\",\"那么些\",\"那么样\",\"那些\",\"那会儿\",\"那儿\",\"那时\",\"那样\",\"那般\",\"那边\",\"那里\",\"都\",\"鄙人\",\"鉴于\",\"针对\",\"阿\",\"除\",\"除了\",\"除外\",\"除开\",\"除此之外\",\"除非\",\"随\",\"随后\",\"随时\",\"随着\",\"难道说\",\"零\",\"非\",\"非但\",\"非徒\",\"非特\",\"非独\",\"靠\",\"顺\",\"顺着\",\"首先\",\"￥\"])\n",
    "\n",
    "def remove_stopwords(sentence, lang, nlp_models, chinese_stopwords):\n",
    "    if lang == \"zh\":\n",
    "        tokens = jieba.cut(sentence)\n",
    "        filtered_tokens = [token for token in tokens if token not in chinese_stopwords]\n",
    "        return \" \".join(filtered_tokens)\n",
    "    elif lang in nlp_models:\n",
    "        nlp = nlp_models[lang]\n",
    "        doc = nlp(sentence)\n",
    "        return \" \".join([token.text for token in doc if not token.is_stop])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {lang}\")\n",
    "\n",
    "\n",
    "df['processed_language1'] = df.apply(\n",
    "    lambda row: remove_stopwords(row['processed_language1'], row['lang1'], nlp_models, chinese_stopwords), axis=1)\n",
    "\n",
    "df['processed_language2'] = df.apply(\n",
    "    lambda row: remove_stopwords(row['processed_language2'], row['lang2'], nlp_models, chinese_stopwords), axis=1)\n",
    "\n",
    "df[['processed_language1', 'processed_language2']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('stopword_removal_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>score</th>\n",
       "      <th>lang1</th>\n",
       "      <th>lang2</th>\n",
       "      <th>processed_language1</th>\n"